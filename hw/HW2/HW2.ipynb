{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_hw2_studentcopy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykwpAFaecXiS"
      },
      "source": [
        "## Please write your name and NetID below:\n",
        "\n",
        "Name:\n",
        "\n",
        "NetID:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_yTd4AgOcYy"
      },
      "source": [
        "# CPSC 477/577: Natural Language Processing\n",
        "## Homework 2: Sentiment analysis (due  **11:59pm on March 29**)\n",
        "*TFs: Aditya Chander and Michael Linden*\n",
        "\n",
        "In this homework, you'll build several classifiers that can predict a user's rating of a movie solely from the words of their review. This is called **sentiment analysis**. We'll start with some simple non-neural classifiers before moving to a neural network implementation. We'll also see how transformers perform on this task. Throughout the assignment, we'll explore a few different ways to encode the text data and assess an encoding's impact on model performance. \n",
        "\n",
        "The dataset we'll be using is the Rotten Tomatoes dataset. We've downloaded it for you, but if you want to find out where we got it from you can check out this link: https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxCQgOeADMH1"
      },
      "source": [
        "## Part A: dataset exploration, non-neural methods, investigating different encodings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkphXis8OXmH"
      },
      "source": [
        "# Import our libraries (this may take a minute or two)\n",
        "import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv). \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import spacy\n",
        "from scipy import stats\n",
        "import os # Good for navigating your computer's files \n",
        "import sys\n",
        "pd.options.mode.chained_assignment = None #suppress warnings\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "!python -m spacy download en_core_web_md\n",
        "import en_core_web_md\n",
        "text_to_nlp = en_core_web_md.load()\n",
        "\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torch.utils.data import random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "! pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVEibUAnD_kk"
      },
      "source": [
        "### Data preprocessing\n",
        "The dataset is in the file **train.tsv** (even though the file is called train.tsv, this is the entire dataset that we want you to use). We have written a line of code to load up the dataset from the directory where you stored the file into a pandas DataFrame (check out the documentation [here](https://pandas.pydata.org/docs/reference/frame.html) if you've never used pandas before). This variable is called `all_raw`.\n",
        "\n",
        "The dataframe `all_raw` currently has the columns `PhraseId`, `SentenceId`, `Phrase` and `Sentiment`. We're going to outline a few preprocessing steps that you need to take to get the data in the right form for classification. Name your processed dataframe variable `all_data`. \n",
        "\n",
        "**Step 1**: There are several reviews with duplicated `SentenceId` (subphrases of the full review). Write some code to keep only the first phrase for any given `SentenceId` in your dataframe. Then you can drop the `PhraseId` and `SentenceId` columns. Rename the column called `Phrase` to `text`.\n",
        "\n",
        "**Step 2**: The next preprocessing step is to convert the values in the `Sentiment` column to a binary encoding (Boolean values) that represents whether the review is good (rating of 3 or 4) or bad (rating of 0, 1 or 2). **Keep the original `Sentiment` column**, renaming it `rating`, and title your binary encoding column `is_good`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4OUohu9C8ma"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/Year2/Spring2021/CS477/train.tsv' # TODO: replace this with your own path to the dataset (should end in train.tsv)\n",
        "all_raw = pd.read_table(data_path)\n",
        "all_data = None # this will contain your final preprocessed dataset\n",
        "\n",
        "# ## TODO: write code for steps 1 and 2 above\n",
        "\n",
        "# Tests to ensure your columns are labelled correctly and contain the right content\n",
        "assert 'is_good' in all_data.columns\n",
        "assert 'text' in all_data.columns\n",
        "assert 'rating' in all_data.columns\n",
        "assert sum(all_data.text.apply(lambda x: len(x))) == 868869"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhWIYhTZIqKi"
      },
      "source": [
        "### Exploring the dataset\n",
        "Make up to 5 plots that show some relationships between the text of the review and the sentiment (you can use both the 0-4 rating in the `rating` column and the binary rating in the `is_good` column, though ultimately we'll only be using the binary rating). Some things you may wish to explore are whether the length of the review has any bearing on the sentiment, how many good and bad reviews there are, etc. In the text cell below, comment on what you've found and whether this matches your intuitions about movie reviews more generally.\n",
        "\n",
        "We've imported matplotlib.pyplot and seaborn for your convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "auQBq_jlD9kt"
      },
      "source": [
        "# TODO: make some plots that show some patterns in the dataset!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K1NVLrNKia3"
      },
      "source": [
        "**TODO: write your observations about the plots here.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYj4lu7NKwjD"
      },
      "source": [
        "### Bag of words encoding\n",
        "**Step 1**: Print out the number of unique tokens in your dataset. You should use the `word_tokenize` function from NLTK (imported)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hjp6rbO8D-T1"
      },
      "source": [
        "## TODO: count the number of unique tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x-kSEqHLtGa"
      },
      "source": [
        "**Step 2**: Convert the text of each review to a bag-of-words encoding. You should use a vocabulary size that is the minimum of 2001 and the number of tokens you calculated in the cell above.\n",
        "\n",
        "The number 2001 corresponds to the 2000 most common tokens and an extra one for any words that do not match those tokens (think of these like the `RARE_WORDS` from Homework 1). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b2LIKvrL6z4"
      },
      "source": [
        "# TODO: implement bag of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcKbEXRSMYQs"
      },
      "source": [
        "### Basic models: logistic regression\n",
        "**Step 1**: Use your bag of words encoding to train a logistic regression model that predicts whether a review is good or bad. Train the model on 80% of your dataset and print the accuracy score for both your training and testing data.\n",
        "\n",
        "We've imported the `LogisticRegression` model from scikit-learn function for your convenience. Use the scikit-learn `train_test_split` function (also imported) to split your dataset into training and testing; set the `random_state` to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yNpJ1-XNInd"
      },
      "source": [
        "# TODO: train your logistic regression model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neHvg0PXOL_k"
      },
      "source": [
        "**Step 2**: We're going to see which words were most important to the model. Print out the words that correspond to the 20 coefficients in the logistic regression model with the highest absolute value. (To get the coefficients for the model you trained, you can use `model.coef_`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoRw1piMRC4v"
      },
      "source": [
        "# TODO: print out top 20 features of logistic regression model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNWw_PoNPAr_"
      },
      "source": [
        "**Step 3**: Plot these 20 coefficients (with their original sign) in a bar plot. Label each bar with the word to which it corresponds. What do you notice about the relationship between the words and the bars? Add your observation in the text cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZbHNufTRuGX"
      },
      "source": [
        "# TODO: plot the top 20 coefficients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ObImTmHPW5A"
      },
      "source": [
        "**TODO: write your observations here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNY9PC1DPdk7"
      },
      "source": [
        "### Other non-neural models\n",
        "\n",
        "Run the classification again using three different models: **K-nearest Neighbors (with 5 neighbors)**, **Gaussian Naive Bayes** and **Support Vector Classifier**. All of these have been imported from scikit-learn. Which model performed best? Can you speculate why this was the case? You may provide feature importance plots/figures where appropriate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Q9DZuNRxmp"
      },
      "source": [
        "# TODO: do the same thing with other models and see whether they perform any better\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDlzXcPlRdXm"
      },
      "source": [
        "**TODO: which model was best and why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViqLoXemRNQn"
      },
      "source": [
        "### Reducing the size of our review representations\n",
        "Our reviews are currently encoded in a form with thousands of features. This is potentially slowing down the runtime of our models. However, we can use dimensionality reduction techniques such as **Principal Component Analysis (PCA)** to capture the majority of the variance that we observe in our encodings but with far fewer dimensions needed.\n",
        "\n",
        "Use PCA to perform dimensionality reduction on the bag-of-words encodings. Then train a logistic regression model using these reduced vectors. Try this on different numbers of principal components; we have provided these in the code. We have imported the PCA implementation from scikit-learn for your convenience. (Hint: the `fit_transform` function might be useful.)\n",
        "\n",
        "Plot the test performance of your different encodings in a bar plot. How does the performance of the reduced vectors compare with the original?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om6eU3pWSd2h"
      },
      "source": [
        "n_components = [10, 50, 100, 300, 800]\n",
        "\n",
        "# TODO: perform PCA on bag of words using the numbers of components above and explore how this affects logistic regression model performance\n",
        "for n in n_components:\n",
        "    pca = PCA(n_components=n)\n",
        "    # YOUR CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmSrPRzXTJ6y"
      },
      "source": [
        "**TODO: how do the dimensionally-reduced vectors affect performance?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQoYZTRjRcXy"
      },
      "source": [
        "### Other encodings\n",
        "We're going to explore different ways to represent the dataset that may improve performance on the basic models compared to the bag-of-words representation. \n",
        "#### Bag of bigrams\n",
        "We're going to extend the bag-of-words model to a \"bag-of-bigrams\" model, where we count the number of occurrences of each bigram that appears in the dataset. Implementing this is very similar to implementing bag of words, but we might expect improved performance as this model accounts for valenced phrases such as \"not good\". \n",
        "\n",
        "As such, implement the bag-of-bigrams model on the dataset (with a vocab size of the minimum of 5000 and the number of unique bigrams) and train a logistic regression model as before. Feel free to reuse your code from the bag-of-words model implementation as you see fit. \n",
        "\n",
        "Is the test performance with the bigram encoding any better? Explain why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIjYJUPRPkxg"
      },
      "source": [
        "# TODO: implement bag of bigrams and train on logistic regression model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpXuGK9BP0Yk"
      },
      "source": [
        "**TODO: explain the performance of your bag-of-bigrams model vs. bag-of-words model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pirBfdZ0UTDm"
      },
      "source": [
        "#### Word embeddings \n",
        "Another type of text representation is **word embeddings**. These are large vectors that aim to capture some kind of semantic information about the words. As such, you might expect to find similar words close to each other in word embedding vector space. \n",
        "\n",
        "We're going to be using the spacy word embeddings in this assignment. The line below loads up the spacy embeddings, which are 300-dimensional vectors, into a column in the `all_data` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eau2BoJoVmSy"
      },
      "source": [
        "# Spacy embeddings\n",
        "all_data['spacy_embedding'] = all_data['text'].apply(lambda x: text_to_nlp(x).vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU4vMscVQobm"
      },
      "source": [
        "As before, train a logistic regression model on the spacy embeddings and see how the test performance compares to the bag-of-words and bag-of-bigrams encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCsiR7RcQ11T"
      },
      "source": [
        "# TODO: train logistic regression on spacy word embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i48DTrgGQ43v"
      },
      "source": [
        "**TODO: how does the word embedding vector performance compare to bag of words/bag of bigrams?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHVNFRgdgu_-"
      },
      "source": [
        "## Part B: RNN implementation (with embedding layer + pretrained embeddings)\n",
        "Now, we're going to look at some neural methods for sentiment classification. Specifically, we'll explore how **Recurrent Neural Networks (with and without LSTM cells)** and **Transformers** perform on the same task.\n",
        "\n",
        "Please run the following cells to set everything up. **DO NOT MODIFY THEM**, but do take a look to see how they work.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pLtsgBVfXnu"
      },
      "source": [
        "# Settings\n",
        "SEED = 12138\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Torchtext will let us to load the text and labels separately.\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVI_L_cVfckG"
      },
      "source": [
        "# class to convert pandas dataframe to the torchtext dataset\n",
        "class DataFrameDataset(data.Dataset):\n",
        "    \"\"\"Class for using pandas DataFrames as a datasource\"\"\"\n",
        "    def __init__(self, examples, fields, filter_pred=None):\n",
        "        \"\"\"\n",
        "        Create a dataset from a pandas dataframe of examples and Fields\n",
        "        Arguments:\n",
        "            examples pd.DataFrame: DataFrame of examples\n",
        "            fields {str: Field}: The Fields to use in this tuple. The\n",
        "                string is a field name, and the Field is the associated field.\n",
        "            filter_pred (callable or None): use only exanples for which\n",
        "                filter_pred(example) is true, or use all examples if None.\n",
        "                Default is None\n",
        "        \"\"\"\n",
        "        self.examples = examples.apply(SeriesExample.fromSeries, args=(fields,), axis=1).tolist()\n",
        "        if filter_pred is not None:\n",
        "            self.examples = filter(filter_pred, self.examples)\n",
        "        self.fields = dict(fields)\n",
        "        # Unpack field tuples\n",
        "        for n, f in list(self.fields.items()):\n",
        "            if isinstance(n, tuple):\n",
        "                self.fields.update(zip(n, f))\n",
        "                del self.fields[n]\n",
        "\n",
        "class SeriesExample(data.Example):\n",
        "    \"\"\"Class to convert a pandas Series to an Example\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def fromSeries(cls, data, fields):\n",
        "        return cls.fromdict(data.to_dict(), fields)\n",
        "\n",
        "    @classmethod\n",
        "    def fromdict(cls, data, fields):\n",
        "        ex = cls()\n",
        "        \n",
        "        for key, field in fields.items():\n",
        "            if key not in data:\n",
        "                raise ValueError(\"Specified key {} was not found in \"\n",
        "                \"the input data\".format(key))\n",
        "            if field is not None:\n",
        "                setattr(ex, key, field.preprocess(data[key]))\n",
        "            else:\n",
        "                setattr(ex, key, data[key])\n",
        "        return ex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkOdcoM-gn_Q"
      },
      "source": [
        "# Format reviews in a pytorch-specific way\n",
        "fields = {'is_good' : LABEL, 'text' : TEXT}\n",
        "all_ds = DataFrameDataset(all_data, fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbA3BJKXhbgZ"
      },
      "source": [
        "# display first entry; this should be the text of the review followed by a binary sentiment classification\n",
        "print(vars(all_ds.examples[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oVrJEVLiHDz"
      },
      "source": [
        "# cast to actual torchtext dataset class\n",
        "all_ds = data.Dataset(all_ds,fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmvGDCJejDjm"
      },
      "source": [
        "# make train + valid + test sets\n",
        "train_ds, test_ds = random_split(all_ds,[int(0.8*len(all_ds)), len(all_ds)-int(0.8*len(all_ds))])\n",
        "train_ds, valid_ds = random_split(train_ds, [int(0.8*len(train_ds)), len(train_ds)-int(0.8*len(train_ds))])\n",
        "train_ds, valid_ds, test_ds = data.Dataset(train_ds,fields), data.Dataset(valid_ds,fields), data.Dataset(test_ds,fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B2rJI1bke6e"
      },
      "source": [
        "# See how many examples there are in each subset\n",
        "print(f'Number of training examples: {len(train_ds)}')\n",
        "print(f'Number of validation examples: {len(valid_ds)}')\n",
        "print(f'Number of testing examples: {len(test_ds)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsW-PjJZlgey"
      },
      "source": [
        "# Build preliminary encodings to pass into the model\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_ds, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNSQ2dCymagk"
      },
      "source": [
        "# Define iterator for minibatches (helps to train RNN in chunks)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# If there is a GPU available, we will set to use it; otherwise we will use cpu.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_ds, valid_ds, test_ds), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    sort_key = lambda x: len(x.text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnl8bwVTVHfR"
      },
      "source": [
        "### Building the network\n",
        "Now that you've set everything up, you're ready to build the network architecture.\n",
        "\n",
        "**Step 1**: The first thing you’ll want to do is fill out the code in the initialization of the RNN class. You’ll need to define three layers: self.embedding, self.rnn, and self.fc. Use the built-in functions in torch.nn to accomplish this (remember that a fully-connected layer is also a linear layer!) and pay attention to what each dimensions each layer should have for its input and output.\n",
        "\n",
        "**Step 2**: The next step (still in the RNN class) is to implement the forward pass. Make use of the layers you defined above to create embedded, hidden, and output vectors for a given input x.\n",
        "\n",
        "Hint to start our model:\n",
        "The RNN model should have the following structure:\n",
        "1. start by an embedding layer; shape:  (input_dim, embedding_dim)\n",
        "2. then we put the RNN layer; shape: (embedding_dim, hidden_dim)\n",
        "3. last, we add a liner layer; shape: (hidden_dim, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6IcfH17nEZt"
      },
      "source": [
        "## TODO: define the RNN class\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        ## TODO starts\n",
        "        ## TODO ends\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        ## TODO starts\n",
        "        ## TODO ends\n",
        "\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iqPoDNSV3KB"
      },
      "source": [
        "Now we're going to define a few model hyperparameters and initialize our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-RmdvKRoHAF"
      },
      "source": [
        "# define some hyperparameters\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# apply our RNN model here\n",
        "rnn = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "# set up learning rate + optimization algorithm\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.BCEWithLogitsLoss() # need the \"with logits\" as we need the range to be between 0 and 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjgNNKzRoWbm"
      },
      "source": [
        "rnn = rnn.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzYPL4U7WF7g"
      },
      "source": [
        "### Calculating model accuracy\n",
        "The model will give us outputs in the range (-Inf,Inf). Your job now is to take a list of these outputs, convert them to predictions of the sentiment label, and calculate the accuracy of the RNN classifier. To this end, fill in the code cell below to implement the `binary_accuracy` function.\n",
        "\n",
        "Hint: you will need to use an activation function called the **sigmoid** function (`torch.sigmoid`) to get your outputs in the appropriate range, as they need to be converted to probabilities. Then you can predict the class based on whether the probability is below or above 0.5, and compare it to the ground truth label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DojzZMC-oaG9"
      },
      "source": [
        "## TODO: return the accuracy given the RNN outputs (outputs) and true values (y); accuracy should be a float number\n",
        "def binary_accuracy(outputs, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ39tX3YXjoi"
      },
      "source": [
        "### Training function\n",
        "\n",
        "The next function is the train function. Most of the code is handled for you – you only need to get a set of predictions for the current batch and then calculate the current loss and accuracy. For the latter two calculations, make sure to use the criterion and binary_accuracy functions you are given. For calculating the batch predictions, extract the text of the current batch and run it through the model, which is passed in as a parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMzXdFSJob1N"
      },
      "source": [
        "## TODO: finish the training function\n",
        "## iterator contains batches of the training data; \n",
        "## hint: use batch.text and batch.label to get access to the training data and labels\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        # reset the optimiser\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: make predictions\n",
        "\n",
        "        # TODO: calculate loss and accuracy\n",
        "\n",
        "        # backprop\n",
        "        loss.backward()\n",
        "        # update params\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xs_6apsYph_"
      },
      "source": [
        "### Evaluation function\n",
        "\n",
        "This step is to copy and paste what you did in the training function into the evaluate function. This time, there’s no additional optimization after the predictions, loss, and accuracy are calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15lc74Xeod4G"
      },
      "source": [
        "## TODO: finish the evaluation function\n",
        "## iterator contains batches of the training data; \n",
        "## hint: this function is very similar to the training function\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "            \n",
        "            ## TODO starts\n",
        "\n",
        "            ## TODO ends\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBe3xAOSYzYW"
      },
      "source": [
        "### Start training\n",
        "It may take a few minutes in total. The validation set accuracy is around 54%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHmtBs41ofYB"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "# let's train 5 epochs\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    train_loss, train_acc = train(rnn, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(rnn, valid_iterator, criterion)\n",
        "      \n",
        "    # we keep track of the best model, and save it\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(rnn.state_dict(), 'best_model.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNIha3BAY7S-"
      },
      "source": [
        "### Restore the best model and evaluate\n",
        "The test accuracy should be around 55%.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hmbd5M4oksG"
      },
      "source": [
        "rnn.load_state_dict(torch.load('best_model.pt'))\n",
        "test_loss, test_acc = evaluate(rnn, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZN0pQuhaI4n"
      },
      "source": [
        "Compare this performance to the simple model performance from Part A.\n",
        "\n",
        "**TODO: write your answer below**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBrKRqEMrAVn"
      },
      "source": [
        "### LSTM\n",
        "This step of this assignment is to modify your RNN into a bidirectional LSTM network. We should expect that this kind of model performs better than our previous ones.\n",
        "\n",
        "1. You’ll be making changes to your model in the `RNN` Class. In the `init` class, for the rnn layer, use the `nn.LSTM` function and make sure you pass in the bidirectional argument. Also note that the fully connected layer now has to map from two hidden layer passes (forward and backward).\n",
        "2. In the forward pass, not much changes from before, besides the addition of the cell. Also note that you’ll have to concatenate the final forward hidden layer and the final backward hidden layer. If any of this is unclear, look up example of how `nn.LSTM` works for clarification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ3s96Q1p-iQ"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    # TODO: IMPLEMENT THIS FUNCTION\n",
        "    # Initialize the three layers in the RNN, self.embedding, self.rnn, and self.fc\n",
        "    # Each one has a corresponding function in nn\n",
        "    # embedding maps from input_dim->embedding_dim\n",
        "    # rnn maps from embedding_dim->hidden_dim\n",
        "    # fc maps from hidden_dim*2->output_dim\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, bidirectional):\n",
        "        super().__init__()\n",
        "        \n",
        "        ## TODO starts\n",
        "        ## TODO ends\n",
        "        \n",
        "    \n",
        "    # TODO: IMPLEMENT THIS FUNCTION\n",
        "    # x has dimensions [sentence length, batch size]\n",
        "    # embedded has dimensions [sentence length, batch size, embedding_dim]\n",
        "    # output has dimensions [sentence length, batch size, hidden_dim*2] (since bidirectional)\n",
        "    # hidden has dimensions [2, batch size, hidden_dim]\n",
        "        # cell has dimensions [2, batch_size, hidden_dim]\n",
        "    # Need to concatenate the final forward and backward hidden layers\n",
        "    def forward(self, x):\n",
        "        \n",
        "        ## TODO starts\n",
        "        ## TODO ends\n",
        "        \n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsP8X7BIrGnQ"
      },
      "source": [
        "# apply our RNN model here\n",
        "BIDIRECTIONAL = True\n",
        "lstm = LSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, BIDIRECTIONAL)\n",
        "## setup device\n",
        "lstm = lstm.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnnPg8iNrNX3"
      },
      "source": [
        "# train again!\n",
        "best_valid_loss = float('inf')\n",
        "# let's train 5 epochs\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    train_loss, train_acc = train(lstm, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(lstm, valid_iterator, criterion)\n",
        "      \n",
        "    # we keep track of the best model, and save it\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(lstm.state_dict(), 'best_model_LSTM.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmdEY4nkaSgI"
      },
      "source": [
        "Do you think LSTM is working better than RNN? Why or why not? How do you compare with LSTM and RNN (model complexity, etc)?\n",
        "\n",
        "**TODO: write your answer to this question**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFLWqwgMr7yM"
      },
      "source": [
        "## Part C: Transformers\n",
        "In this part, we will see how to apply pre-trained BERT model to improve text classification. You won't have to code anything for yourself here; we provide the code to serve as a learning resource.\n",
        "\n",
        "Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. Google is leveraging BERT to better understand user searches. (From WIKI)\n",
        "\n",
        "\n",
        "Read more: http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "BERT paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
        "\n",
        "Understanding BERT: https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad\n",
        "\n",
        "As before, run the cells below. Don't modify them but do take a look at them to see what's going on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJTIx2PUrPFq"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "# tokenize a sentence: you will see the tokenizer \"cleans\" the sentence as well.\n",
        "tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n",
        "print(tokens)\n",
        "# There will be a warning, but just leave it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HLj7q60sF4Z"
      },
      "source": [
        "# Convert text to ids (positions in a dictionary)\n",
        "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(indexes)\n",
        "\n",
        "# Special tokens\n",
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)\n",
        "\n",
        "# Special token ids\n",
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgmQfEYGsOhC"
      },
      "source": [
        "# Tokenize a sentence and trim it to the max allowed sentence length\n",
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaQp_pC7sQMg"
      },
      "source": [
        "# Redefine TEXT with the new tokens\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJabRIoYsZFW"
      },
      "source": [
        "# Redefine the training, validation and test data\n",
        "fields = {'is_good' : LABEL, 'text' : TEXT}\n",
        "all_ds_t = DataFrameDataset(all_data, fields)\n",
        "train_ds_t, test_ds_t = random_split(all_ds_t,[int(0.8*len(all_ds_t)), len(all_ds_t)-int(0.8*len(all_ds_t))])\n",
        "train_ds_t, valid_ds_t = random_split(train_ds_t, [int(0.8*len(train_ds_t)), len(train_ds_t)-int(0.8*len(train_ds_t))])\n",
        "train_ds_t, valid_ds_t, test_ds_t = data.Dataset(train_ds_t,fields), data.Dataset(valid_ds_t,fields), data.Dataset(test_ds_t,fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmq8cUTCsdoG"
      },
      "source": [
        "# Rebuild vocabulary \n",
        "LABEL.build_vocab(train_ds_t)\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_ds_t, valid_ds_t, test_ds_t), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device,\n",
        "    sort_key = lambda x: len(x.text))\n",
        "\n",
        "# It will download the pre-trained model\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_GN_A8Qvx8a"
      },
      "source": [
        "# Modify the RNN class to incorporate BERT representations\n",
        "class MyBERTwithRNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjXEtkn1wIqi"
      },
      "source": [
        "# create model instance\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "\n",
        "bert_model = MyBERTwithRNN(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwlH2u8zwNp5"
      },
      "source": [
        "# Count the number of parameters in the model. The value should shock you!\n",
        "def count_parameters(model):\n",
        "    param_number = 0\n",
        "    param_number = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return param_number\n",
        "\n",
        "print(f'The model has {count_parameters(bert_model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esGlGhCbwYVe"
      },
      "source": [
        "# Let's fix the bert embeddings\n",
        "for name, param in bert_model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4t_ZmpHSwdo9"
      },
      "source": [
        "# Tether the model to the GPU\n",
        "bert_model = bert_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXUfsLOswhS9"
      },
      "source": [
        "# a helper function to see how much time needed\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G06mHxTQw3cN"
      },
      "source": [
        "# Start training.\n",
        "# Note that it will take ~17 minutes for one epoch.\n",
        "# The output will be: Epoch: 01 | Epoch Time: 17m 36s...\n",
        "# Validate accuracy is higher than 85% in the first epoch, higher than 90% in the second epoch.\n",
        "\n",
        "N_EPOCHS = 2\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(bert_model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(bert_model, valid_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(bert_model.state_dict(), 'best_model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBX6K4rQcHys"
      },
      "source": [
        "**Question**: how do you compare the bert model with Part B? (Hint: consider training time, accuracy, model complexity etc.)\n",
        "\n",
        "**TODO: write your answer here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVcmEio9cjEz"
      },
      "source": [
        "# Submission instructions\n",
        "1. Print this page as a **PDF** and name it `[your_NetID]_nlp_hw2.pdf`.\n",
        "2. Rename your notebook `[your_NetID]_nlp_hw2.ipynb`.\n",
        "3. Submit both on Canvas by **11:59pm on March 29**."
      ]
    }
  ]
}