{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intimate-given",
   "metadata": {},
   "source": [
    "# Homework 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may utilize other packages if you are more familiar with them.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-composition",
   "metadata": {},
   "source": [
    "## Word2vec in multiple languages\n",
    "For homework 4, we will be coding a word2vec model using an english and a small russian parallel text corpus. The goal is to understand how a very simple word embedding functions and how one might begin using them to investigate machine translation. \n",
    "\n",
    "\n",
    "### Load data\n",
    "For this homework, we will be working with a subset of the multi-lingual text corpus from Helsinki-NLP/Tatoeba-Challenge https://github.com/Helsinki-NLP/Tatoeba-Challenge.\n",
    "The subset is a partial copy of the bilingual Russian-English, English-Finnish, and Finnish-Russian parts of the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-costume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finnish and Russian parallel corpora\n",
    "finrus = pd.read_csv('finrus_hw4.csv', index_col=0)\n",
    "# English and Russian parallel corpora\n",
    "engrus = pd.read_csv('engrus_hw4.csv', index_col=0)\n",
    "# English and Finnish parallel corpora\n",
    "engfin = pd.read_csv('engfin_hw4.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-letter",
   "metadata": {},
   "source": [
    "### 0) First Look\n",
    "Take a look at some of the data. Print out a few sentence pairs, show a histogram of sentence lengths for each of the languages, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "### type code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-hearing",
   "metadata": {},
   "source": [
    "### 1) Preprocessing\n",
    "We will be building a skip-gram word2vec embedding using this corpus. Our first step is to preprocess the text. \n",
    "The end results of preprocessing can be slightly different, but here we want to at least do these 4 steps.\n",
    "\n",
    "1) tokenize\n",
    "\n",
    "2) lowercase\n",
    "\n",
    "3) remove punctuation\n",
    "\n",
    "4) remove stop words (this step will be done in part 3 of the homework, do not include in your 'preprocess_sentence' function)\n",
    "\n",
    "Make a single function to preprocess all of our English, Finnish, and Russian text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    ### type code here ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    return preprocessed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-humanity",
   "metadata": {},
   "source": [
    "Now preprocess all of our russian and english sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_eng2fin = [preprocess_sentence(sentence, english_stopwords) for sentence in engfin['english']]\n",
    "pp_eng2rus = [preprocess_sentence(sentence, russian_stopwords) for sentence in engrus['english']]\n",
    "pp_fin2eng = [preprocess_sentence(sentence, russian_stopwords) for sentence in engfin['finnish']]\n",
    "pp_fin2rus = [preprocess_sentence(sentence, russian_stopwords) for sentence in finrus['finnish']]\n",
    "pp_rus2eng = [preprocess_sentence(sentence, russian_stopwords) for sentence in engrus['russian']]\n",
    "pp_rus2fin = [preprocess_sentence(sentence, russian_stopwords) for sentence in finrus['russian']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-reunion",
   "metadata": {},
   "source": [
    "### 2) One-hot-encode\n",
    "The next step is to one-hot-encode our text corpora.\n",
    "To do this, first begin by creating a vocabulary for each of our languages.\n",
    "There can be many ways to do this, but the outcome should have the format of a dictionary where the key is the word and the value is an int unique to that language.\n",
    "Also in this function, produce a separate dictionary, for token counts consisting of a dictionary with {key, count}. This will be used to remove stopwords and later for negative sampling.\n",
    "\n",
    "Then, write a function using the vocabularies to one-hot-encode single words.\n",
    "\n",
    "Hint: allow your function to take in 2 sets of preprocessed text so that it can create a vocabulary for all of the text from a given language. Example: english vocabulary should include words from eng-rus and eng-fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocabulary(preprocessed_corpus):\n",
    "    ### type code here ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return vocabulary, token_counts\n",
    "\n",
    "vocabulary_english, token_counts_english = create_vocabulary(pp_eng2fin, pp_eng2rus)\n",
    "print('english vocabulary length:',len(vocabulary_english))\n",
    "\n",
    "vocabulary_finnish, token_counts_finnish = create_vocabulary(pp_fin2eng, pp_fin2rus)\n",
    "print('english vocabulary length:',len(vocabulary_finnish))\n",
    "\n",
    "vocabulary_russian, token_counts_russian = create_vocabulary(pp_rus2eng, pp_rus2fin)\n",
    "print('russian vocabulary length:',len(vocabulary_russian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencode(word, vocabulary):\n",
    "    ### type code here ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-yield",
   "metadata": {},
   "source": [
    "### 3) Zipf's law\n",
    "Provide 3 plots, one for each of our corpora, to indicate whether they follow Zipf's law.\n",
    "\n",
    "Then, using your plots, make a judgement call on where to threshold each of your vocabularies where high-frequency words will be considered stopwords. Then, filter out all of the stopwords from the preprocessed sentences: pp_eng2fin, pp_eng2rus, pp_fin2eng, pp_fin2rus, pp_rus2eng, and pp_rus2fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word frequencies\n",
    "### type code here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out stop words\n",
    "### type code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-brazilian",
   "metadata": {},
   "source": [
    "### 4) Generate training data\n",
    "Our training data will consist of pairs of word and contexts (target, [context1, context2, ...]). \n",
    "The contexts for a given word are generated using the skip-gram model.\n",
    "Here, generate these training data where x is a list of the target words.\n",
    "\n",
    "The window size refers to the distance we look to generate the skip gram.\n",
    "For the sentence \"Apricots are the most delicious fruit.\" and a window size of 2, the skip-grams for the target word \"delicious\" would be [(delicious, the), (delicious, most), (delicious, fruit)]. In this example stop words are not removed, this is not the case in the homework.\n",
    "\n",
    "Hint: a single target word can have multiple context words within a skipgram model.\n",
    "\n",
    "Hint: for each language, make sure to pass in all sentences for that language. Example, there are 5000 english sentences in the engfin corpus and 5000 english sentences in the russian corpus, thus it should include 10000 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "\n",
    "def generate_training_data(corpus1, corpus2, vocabulary, window_size):\n",
    "    ### type code here ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    return\n",
    "\n",
    "x_english, y_english = generate_training_data(pp_eng2fin, pp_eng2rus, vocabulary_english, window_size)\n",
    "x_finnish, y_finnish = generate_training_data(pp_fin2eng, pp_fin2rus, vocabulary_finnish, window_size)\n",
    "x_russian, y_russian = generate_training_data(pp_rus2eng, pp_rus2fin, vocabulary_russian, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-province",
   "metadata": {},
   "source": [
    "### 5) Negative sampling\n",
    "In order to train this model efficiently, we will be using negative sampling. Here, the model will be trained to discern positive, true pairs of (target, context) from negative, false pairs of (target, context). This binary classification makes the model much faster to train.\n",
    "\n",
    "Here, we use a hyper parameter m, where for every positive pair of words we will include m negative pairs of words. \n",
    "\n",
    "Generate a set of true pairs consisting of one target word and one context word. Then, for each true pair, generate a corresponding set of m false pairs. These negative words are sampled from the original corpus, so should be sampled not uniformly from the vocabulary but from the distribution of tokens. \n",
    "\n",
    "These pairs need corresponding labels stored in a separate list with 0 for false pair and 1 for true pair.\n",
    "\n",
    "Then, create a function that pulls batches from this data for training. \n",
    "\n",
    "Hint: shuffling your data will assist in learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 2\n",
    "\n",
    "def generate_negative_samples(x, y, m, vocabulary):\n",
    "    ### type code here ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "pairs_english, labels_english = generate_negative_samples(x_english, y_english, m, vocabulary_english)\n",
    "pairs_finnish, labels_finnish = generate_negative_samples(x_finnish, y_finnish, m, vocabulary_finnish)\n",
    "pairs_russian, labels_russian = generate_negative_samples(x_russian, y_russian, m, vocabulary_russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(pairs, labels, batch_size):\n",
    "    ### type code here ###\n",
    "        \n",
    "        \n",
    "        \n",
    "    return batch_pairs, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-resistance",
   "metadata": {},
   "source": [
    "### 6) Word2vec\n",
    "Now, use pytorch and/or numpy to implement your own word2vec model. Here is the original paper for the model: https://arxiv.org/abs/1301.3781\n",
    "\n",
    "Word2vec is a simple word embedding that incorporates 2 fully connected layers, one to embed words into a latent space and one to decode that embedding. Word2vec comes in a few different flavors, here we want to implement the version with negative sampling. \n",
    "\n",
    "Hint: the objective for negative sampling is to classify pairs of words as true pairs or false, negatively sampled, pairs. If your objective is a softmax, you're implementing the wrong type of word2vec.\n",
    "\n",
    "Below are some parameters I recommend, but you may tune them to assist your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embed_size = 100\n",
    "LR = 0.001\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "### type code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-twist",
   "metadata": {},
   "source": [
    "### 7) Results\n",
    "Show that you embedding worked. \n",
    "Vectorize each word in our corpus using your 3 word2vec models. \n",
    "Use PCA (feel free to use sklearn) to visualize each of our 3 word embeddings. Indicate some way of showing that the training of the model was successful. Besides the PCA plots, do at least 2 other things.\n",
    "\n",
    "Suggestions:\n",
    "1) Chose a few words from the corpus and print them out over the PCA plots to indicate how similar they are.\n",
    "\n",
    "2) Use cosine similarity to compare a few chosen pairs of words from the corpora.\n",
    "\n",
    "3) Try retraining the models with a subset of the data and compare its ability to embed words. \n",
    "\n",
    "4) Perform a clustering method on the words and investigate the quality of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "### type code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-precipitation",
   "metadata": {},
   "source": [
    "### 8) Short Answer\n",
    "In your own words (2-3 sentences), explain what the model learns. Make sure your explanation applies to all 3 of the word2vec models in this homework. \n",
    "\n",
    "Write your answer below this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-medicaid",
   "metadata": {},
   "source": [
    "### 9) Connect our corpora\n",
    "Develop a method to connect our corpora. Grading for this part of the homework will be lenient, but take this chance to be creative and do your best to show that you've connected the corpora. \n",
    "\n",
    "Suggestions:\n",
    "1) Retrain a word2vec model that simultaneously embeds both english and russian words\n",
    "\n",
    "2) Create a mapping between the two embeddings generated\n",
    "\n",
    "3) Identify some idioms in one language within our embedded corpora and assess how they manifest in the embedding of another language.  \n",
    "\n",
    "Feel free to try some different things out here. The end result should be a figure with a short caption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "### type code here ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
