{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# contex-free parsing: pushdown automata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-free grammars (CFGs) and pushdown automata (PDAs) are equivalent in their computational power, meaning that they can recognize and generate the same class of languages: context-free languages.\n",
    "\n",
    "\n",
    "From CFG to PDA: Given a context-free grammar, we can construct a pushdown automaton that simulates the derivation process of the grammar. \n",
    "\n",
    "The PDA uses its **stack** to store and manipulate the non-terminal symbols and productions during the derivation process. \n",
    "\n",
    "It guesses the correct production rules non-deterministically to apply and pushes non-terminal symbols onto the stack. \n",
    "\n",
    "The PDA then matches the input **non-terminal symbols** with the **terminal symbols** from the production rules and pops them off the stack. \n",
    "\n",
    "If input is successfully processed and stack is empty, the PDA accepts the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: a pushdown automaton (PDA) that recognizes the language $L = \\{x^n y^n\\}$, which consists of strings with n 'x' characters followed by n 'y' characters. \n",
    "\n",
    "The PDA uses a stack to keep track of the number of 'x' characters it has encountered and then matches them with the 'y' characters. \n",
    "\n",
    "production rules: we could define a CFG for $L = \\{x^n y^n\\}$\n",
    "\n",
    "- S → xSy\n",
    "\n",
    "- S → ε\n",
    "\n",
    "Here, 'S' is a non-terminal symbol '*'. It represents the start symbol of the grammar and is used to generate strings in the language L by applying the production rules. \n",
    "\n",
    "The symbol 'ε' represents an empty string, which indicates that no more production rules need to be applied.\n",
    "\n",
    "In this grammar, 'x' and 'y' are terminal symbols, while 'S' is a non-terminal symbol.\n",
    "\n",
    "Here's a step-by-step explanation of the process:\n",
    "\n",
    "| Step | Action                                      | Stack    | Remaining Input |\n",
    "|------|---------------------------------------------|----------|----------------|\n",
    "| 1    | Start with empty stack and input \"xxxyyy\"  | []       | xxxyyy         |\n",
    "| 2    | Read 'x', push '*', move to next character | [*]      | xxyyy          |\n",
    "| 3    | Read 'x', push '*', move to next character | [*, *]   | xyyy           |\n",
    "| 4    | Read 'x', push '*', move to next character | [*, *, *]| yyy            |\n",
    "| 5    | Read 'y', pop '*', move to next character  | [*, *]   | yy             |\n",
    "| 6    | Read 'y', pop '*', move to next character  | [*]      | y              |\n",
    "| 7    | Read 'y', pop '*', move to end of input    | []       | \"\"             |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom up: explores options that won’t lead to a full parse \n",
    "\n",
    "- shift-reduce (srparser in nltk)\n",
    "\n",
    "- CKY (Cocke-Kasami-Younger): DP\n",
    "\n",
    "Top down: explores options that don’t match the full sentence \n",
    "\n",
    "- recursive descent (rdparser in nltk)\n",
    "\n",
    "- Earley parser: DP\n",
    "\n",
    "Dynamic programming (DP): caches of intermediate results (memoization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method            | Description                                                                                                                                                                                                                                                                           | search method|Pros                                           | Cons                                                                               |\n",
    "|-------------------|---|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| Shift-Reduce      | match the RHS of a production until it can build an S. <br>shift operation: push words onto a stack. <br>reduce-n operation: tpop and replace matched words with LHS of production. <br>Stop criteria: when input is processed and S is popped from the stack. | BFS bottom-up| Simple, intuitive                               | left recursion, may generate locally feasible subtrees that are not viable globally   |\n",
    "| CKY               | chart parser that requires a normalized (binarized) grammar - Chomsky Normal Form (CNF).|DP     bottom-up                                                                       | Efficient due to DP $O(n^3)$| weak equivalence only, syntactic ambiguity |\n",
    "| Recursive Descent | starts with the start symbol and repeatedly expanding non-terminals. process input left-to-right|DFS   top-down                                                              | Straightforward and easy to implement         | left recursion, may require backtracking, less efficient     |\n",
    "| Earley Parser     | build a parse forest                                                           | DP top-down|Handles left-recursive and ambiguous grammars | More complicated             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chomsky Normal Form (CNF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chomsky Normal Form (CNF) is a specific representation of context-free grammars (CFGs) where all production rules are restricted to one of two forms:\n",
    "\n",
    "- X → YZ: A non-terminal symbol X produces two non-terminal symbols Y and Z.\n",
    "\n",
    "- X → w: A non-terminal symbol X produces a single terminal symbol w.\n",
    "\n",
    "advantage of CNF: simplifies parsing algorithms, e.g. CKY algorithm\n",
    "\n",
    "All CFG can be converted to CNF, and the resulting CNF grammar generates the same language as the original CFG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Transformation Type</th>\n",
    "    <th>Description</th>\n",
    "    <th>Original Rules</th>\n",
    "    <th>Transformed Rules</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Hybrid Rules</td>\n",
    "    <td>Split rules with mixed terminal and non-terminal symbols on the RHS</td>\n",
    "    <td>INF-VP → to VP</td>\n",
    "    <td>INF-VP → TO VP<br>TO → to</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>n-ary Rules</td>\n",
    "    <td>Transform rules with more than two symbols on the RHS into binary rules</td>\n",
    "    <td>S → Aux NP VP</td>\n",
    "    <td>S → R1 VP<br>R1 → Aux NP</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Unary Rules</td>\n",
    "    <td>Remove rules with a single non-terminal symbol on the RHS</td>\n",
    "    <td>S → VP<br>VP → Verb<br>VP → Verb NP<br>VP → Verb PP</td>\n",
    "    <td>S → book<br>S → buy<br>S → R2PP<br>S → VerbPP</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Epsilon Rules</td>\n",
    "    <td>Remove rules that produce the empty string ε</td>\n",
    "    <td>S → Verb NP PP &#124; Verb NP<br>NP → book &#124; ε<br>PP → on table &#124; ε<br>Verb → read</td>\n",
    "    <td>S → Verb NP PP &#124; Verb NP &#124; Verb PP &#124; Verb<br>NP → book<br>PP → on table<br>Verb → read</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Parsing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "a sentence can have multiple possible parses with different probability, need a probabilistic ranking method to choose the most likely parse as final parse for a sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "- Find most likely parse tree $t$ among all parse trees $T(s)$ that maximizes the probability $p(t)$ given all $n$ production rules\n",
    "\n",
    "    $$t = \\underset{t \\in T(s)}{\\arg\\max}\\ p(t) = \\underset{t \\in T(s)}{\\arg\\max}\\prod_{i=1}^{n} p(\\alpha_i \\to \\beta_i)$$\n",
    "\n",
    "    possible to do rerank based on specific task (e.g., speech recognition, translation) \n",
    "    \n",
    "    probability of a production rule $(\\alpha_i \\to \\beta_i)$ is estimated using MLE given a training corpus (a set of parsed sentences)\n",
    "\n",
    "    $$\n",
    "    p(\\beta_i | \\alpha_i) = \\frac{\\text{Count}(\\alpha_i \\to \\beta_i)}{\\text{Count}(\\alpha_i)}\n",
    "    $$\n",
    "\n",
    "    Here, $\\text{Count}(\\alpha_i \\to \\beta_i)$ represents the number of times the production rule $(\\alpha_i \\to \\beta_i)$ occurs in the training corpus\n",
    "\n",
    "    $\\text{Count}(\\alpha_i)$ represents the total number of times the non-terminal symbol $\\alpha_i$ appears on the left-hand side of a production rule in the training corpus.\n",
    "\n",
    "- compute probability of a sentence $P(s)$:\n",
    "\n",
    "    If $p(t)$ is the probability of a parse tree $t$, the probability of the sentence $s$ is sum of the probabilities of all possible parse trees $T(s)$ for that sentence:\n",
    "\n",
    "$$\n",
    "P(s) = \\sum_{t \\in T(s)} p(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicalized Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### motivation\n",
    "\n",
    "Limitations of Probabilistic CFG: Probabilities don't depend on specific words, Cannot disambiguate sentences based on semantics\n",
    "\n",
    "e.g., the 2 verb phrase below have different parses because different words after 'with'\n",
    "\n",
    "1. \"eat pizza with **pepperoni**\": \"with pepperoni\" describes the pizza\n",
    "\n",
    "2.  \"eat pizza with **fork**\": \"with fork\" describes eat."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Example Parse tree</h5>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Sentence 1: \"eat pizza with pepperoni\"</th>\n",
    "    <th>Sentence 2: \"eat pizza with a fork\"</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <pre>\n",
    "VP (Head: eat)\n",
    "  V (Head): \"eat\"\n",
    "  NP (Head: pizza):\n",
    "    N (Head): \"pizza\"\n",
    "    PP (Head: with) (adjunct):\n",
    "      P (Head): \"with\"\n",
    "      NP (Head: pepperoni):\n",
    "        N (Head): <span style=\"color:red;\">\"pepperoni\"</span>\n",
    "    </td>\n",
    "    <td>\n",
    "      <pre>\n",
    "VP (Head: eat)\n",
    "  V (Head): \"eat\"\n",
    "  NP (Head: pizza):\n",
    "    N (Head): \"pizza\"\n",
    "  PP (Head: with) (adjunct):\n",
    "    P (Head): \"with\"\n",
    "    NP (Head: fork):\n",
    "      N (Head): <span style=\"color:red;\">\"fork\"</span>\n",
    "      </pre>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limitation\n",
    "\n",
    "- high Complexity $O(N^5g^3V^3)$\n",
    "\n",
    "  - $N$ = sentence length\n",
    "  - $g$ = number of non-terminal symbols\n",
    "  - $V$ = vocabulary size\n",
    "\n",
    "  solution: beam search to reduce complexity\n",
    "\n",
    "- Sparse data problem\n",
    "\n",
    "  training data is not sufficient to cover all the possible rules and structures that may be encountered during parsing, lead to lower parsing accuracy for previously unseen structures.\n",
    "  \n",
    "  e.g., Collins, 40k sentences and 12,409 rules in training, there is still a considerable percentage (15%) of test sentences that contain rules not seen in the training data. \n",
    "  \n",
    "  reason: lexicalized parsing models are more complex, capturing more detailed relationships between words, and thus require more data to accurately learn the underlying structure. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
