{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov assumption"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Markov assumption** (limited context): probability of the next word $w_{n+1}$ depends only on the previous words (n=1,2,3), instead of the entire history.\n",
    "\n",
    "- let $\\pi_n: V^n \\rightarrow C$ be a mapping from word seq of length $n$ to a finite set $C$ (context), then language model becomes\n",
    "\n",
    "    $$\n",
    "    p(w_{n+1}|w_1,...,w_n)=p(w_{n+1}|\\pi_n(w_1,...,w_n))\n",
    "    $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unigram, bigram, trigram model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unigram/bigram/trigram model, number of parameters grows as $O(|V|), O(|V|^2), O(|V|^3)$, respectively.\n",
    "    \n",
    "- unigrams: words are independent with each other, no context\n",
    "\n",
    "    $$\n",
    "    \\pi(w_1,...,w_n)=\\varnothing \\\\[1em]\n",
    "    P(seq) =\\prod_{i=1}^n p(w_i)\n",
    "    $$\n",
    "\n",
    "    Number of parameters is $O(|V|)$, as each word in the vocabulary has its own probability\n",
    "\n",
    "- bigrams: context is previous word $w_n$\n",
    "\n",
    "    $$\n",
    "    \\pi(w_1,...,w_n)=w_n\\\\[1em]\n",
    "    P(seq) =\\prod_{i=1}^n p(w_i|w_{i-1}, w_{i-2})\n",
    "    $$\n",
    "\n",
    "    Number of parameters is $O(|V|^2)$, as there is a probability for each pair of words (first word and its successor) in the vocabulary.\n",
    "\n",
    "   \n",
    "- trigrams: context is last 2 words $w_{n-1}, w_n$\n",
    "\n",
    "    $$\n",
    "    \\pi(w_1,...,w_n)=(w_{n-1}, w_n)\n",
    "    \\\\[1em]\n",
    "    P(seq) =\\prod_{i=1}^n p(w_i|w_{i-1}, w_{i-2}, w_{i-3})\n",
    "    $$\n",
    "\n",
    "    Number of parameters is $O(|V|^3)$, as there is a probability for each combination of 3 consecutive words in the vocabulary.\n",
    "\n",
    "- longer n-grams suffer from sparseness, many n-grams may not appear in the training data.\n",
    "\n",
    "- for LDA, num of params is $O(K \\cdot |V|)$ where $K$ is num of topics in a corpus\n",
    "\n",
    "    because each topic has a probability distribution over the words in the vocabulary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate probabilities for an n-gram model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method               | Short Description                                         | Formula                                                                                     | Pros                                                       | Cons                                        |\n",
    "|----------------------|-----------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------|---------------------------------------------|\n",
    "| Naive MLE            | Maximum Likelihood Estimation                             | $P(w_i \\| w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$                                      | Simple, easy to compute                                     | Zero probabilities for unseen n-grams       |\n",
    "| Additive Smoothing   | Laplace or Lidstone smoothing                             | $P(w_i \\| w_{i-1}) = \\frac{C(w_{i-1}, w_i) + \\alpha}{C(w_{i-1}) + \\alpha \\|V\\|}$                 | Handles unseen n-grams, easy to compute                    | Requires tuning of smoothing constant       |\n",
    "| Back-off & Interpolation | Combining different order n-grams probabilities      | $P(w_i \\| w_{i-1}, w_{i-2}) = \\lambda_1 P(w_i \\| w_{i-1}, w_{i-2}) + \\lambda_2 P(w_i \\| w_{i-1}) + \\lambda_3 P(w_i)$ | Better generalization, handles unseen n-grams | Requires tuning of interpolation weights    |\n",
    "| Bayesian Smoothing   | Smoothing with Dirichlet priors                           | $P(w_i \\| w_{i-1}) = \\frac{C(w_{i-1}, w_i) + \\alpha P_{\\text{base}}(w_i \\| w_{i-1})}{C(w_{i-1}) + \\alpha}$ | Principled Bayesian approach, handles unseen n-grams     | Requires tuning of concentration parameter  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximum likelihood estimation is the most simple way to estimate the probabilities.\n",
    "\n",
    "For a trigram model, the probability of a word given its preceding word is calculated as count of trigram divided by count of preceding 2 words:\n",
    "\n",
    "$$\n",
    "\\hat p(w_3 | w_1, w_2) =\\frac{p(w_1,w_2,w_3)}{p(w_1,w_2)}= \\frac{\\frac{\\text{Count}(w_1,w_2,w_3)}{ \\text{Count}trigram}}{\\frac{\\text{Count}(w_1,w_2)}{\\text{Count} bigram}} \\approx\\frac{\\text{Count}(w_1,w_2,w_3)}{\\text{Count}(w_1,w_2)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Smoothing\n",
    "\n",
    "- problem: MLE assign zero probability to for n-grams not observed in the training data. \n",
    "\n",
    "- solution: add small constant to count to smooth probabilities to avoid zero.\n",
    "\n",
    "    $$\n",
    "    \\hat p(w_3 | w_1, w_2) = \\frac{\\text{Count}(w_1,w_2,w_3)+k}{\\text{Count}(w_1,w_2)+k|V|}\n",
    "    $$\n",
    "\n",
    "    $0<k\\leq 1$ is the smoothing constant added to the count of each n-gram.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backoff and interpolation\n",
    "\n",
    "- problem: both Laplace smoothing (add-one) and Lidstone (add-k) smoothing don't work bc assigns too much probability to unseen trigram \n",
    "\n",
    "- solution: a mixture model combining the probabilities of different order n-grams to estimate the probability of the next word.\n",
    "\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    p(w_3 | w_1, w_2) \n",
    "    &=\\text{trigram model + bigram model + unigram model} \\\\[1em]\n",
    "    &= \\lambda _1 \\hat p(w_3 | w_1, w_2)  + \\lambda _2 \\hat p(w_3 | w_2) +\\lambda _3 \\hat p(w_3 ) \\\\[1em]\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $\\lambda_1, \\lambda_2, \\lambda_3$ are the interpolation weights that sum to 1.\n",
    "\n",
    "    - backoff: if a higher-order n-gram has a non-zero count, its probability is used; otherwise, the model \"backs off\" to a lower-order n-gram. \n",
    "\n",
    "    - interpolation: a weighted combination of probabilities from different order n-grams is used.\n",
    "     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian smoothing with Dirichlet prior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a Dirichlet distribution is used as prior over n-gram probability distribution. \n",
    "\n",
    "For a trigram model, we can estimate the probability of a word given its preceding word by incorporating the Dirichlet prior $p_{\\text{base}}(w_3 | w_1, w_2)$, usually derived from a lower-order n-gram model (e.g., bigram).:\n",
    "\n",
    "$$\n",
    "\\hat p(w_3 | w_1, w_2) = \\frac{\\text{Count}(w_1,w_2,w_3)+\\alpha p_{\\text{base}}(w_3 | w_1, w_2)}{\\text{Count}(w_1,w_2)+\\alpha}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ is the concentration parameter of Dirichlet prior, controls the strength of this prior belief\n",
    "\n",
    "- When $\\alpha$ is large, the prior belief has more influence on the estimated probability, making the model more conservative. \n",
    "\n",
    "- When $\\alpha$ is small, the observed data has more influence, and the model becomes more adaptive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class-based N-gram model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class-based n-gram model is a variation of the standard n-gram language model \n",
    "\n",
    "In a class-based n-gram model, **words are first clustered into classes** based on semantic or syntactic similarities, and then probabilities are estimated for the **class sequences** instead of individual words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a word $w_{n+1}$ given its context is factorized into the probability of the class $c_{n+1}$ given the context $c_1, ..., c_n$ and the probability of the word $w_{n+1}$ given the class $c_{n+1}$:\n",
    "\n",
    "$$\n",
    "p(w_{n+1} | w_1, ..., w_n) = p(c_{n+1}|c_1, ..., c_n) p(w_{n+1}|c_{n+1})\n",
    "$$\n",
    "\n",
    "where $c_i$ is class for word $w_i$, $n$ is context length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros\n",
    "\n",
    "- Reduces sparsity: Clustering words into classes leads to more accurate probability estimates and robustness.\n",
    "\n",
    "- unseen word: Estimating probabilities based on classes allows handling unseen word combinations effectively.\n",
    "\n",
    "- Smaller model size: Using classes instead of individual words requires fewer parameters and results in a smaller model.\n",
    "\n",
    "- Improved performance: In some cases, class-based models can outperform standard n-gram models, especially with limited training data.\n",
    "\n",
    "Cons\n",
    "\n",
    "- Class assignment: Optimally clustering words into classes can be difficult, requiring extra resources or manual input.\n",
    "\n",
    "- Loss of specificity: Clustering words may decrease performance in tasks requiring fine-grained distinctions between words.\n",
    "\n",
    "- Complexity: introduce additional complexity, making them harder to understand and implement."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
