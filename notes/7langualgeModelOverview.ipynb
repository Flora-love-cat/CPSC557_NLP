{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Type</th>\n",
    "    <th colspan='2'>Subtype</th>\n",
    "    <th>Model</th>\n",
    "    <th>Description</th>\n",
    "    <th>Pros</th>\n",
    "    <th>Cons</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Deterministic</td>\n",
    "    <td colspan='2'>Rule-based</td>\n",
    "    <td>Formal Grammar</td>\n",
    "    <td>Used for parsing or grammar checking</td>\n",
    "    <td>Highly interpretable, easy to modify rules</td>\n",
    "    <td>Requires manual rule creation, not adaptive to new data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan='2'>Template-based</td>\n",
    "    <td>Regular Expression, Slot-filling, Frame semantics</td>\n",
    "    <td>Pattern matching and semantic analysis based on templates</td>\n",
    "    <td>Simple, fast, effective for specific tasks</td>\n",
    "    <td>Not adaptive, limited expressiveness</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"14\">Probabilistic</td>\n",
    "    <td rowspan=\"8\">Statistical</td>\n",
    "    <td rowspan=\"5\">Frequentist</td>\n",
    "    <td>N-gram model</td>\n",
    "    <td>predict words based on previous n words</td>\n",
    "    <td>Simple, interpretable, scalable</td>\n",
    "    <td>Requires large data, suffers from data sparsity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Hidden Markov Model (HMM)</td>\n",
    "    <td>Mmemoryless, weighted finite hidden state transducer follows a Markov Chain</td>\n",
    "    <td>Handles temporal dependencies, interpretable</td>\n",
    "    <td>Assumes Markov property, limited modeling capacity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>MaxEnt</td>\n",
    "    <td>logistic regression with softmax for classification. objective is maximize entropy</td>\n",
    "    <td>Flexible, can handle overlapping features</td>\n",
    "    <td>Requires large data, can be computationally expensive</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Conditional Random Field (CRF)</td>\n",
    "    <td>Discriminative model for structured prediction</td>\n",
    "    <td>Discriminative, can handle overlapping features</td>\n",
    "    <td>Computationally expensive, requires large data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Caching</td>\n",
    "    <td>assign high probability to recently occurred word sequences</td>\n",
    "    <td>Simple, can adapt to recent data</td>\n",
    "    <td>Depends on instance similarity, limited capacity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">Bayesian</td>\n",
    "    <td>Noisy Channel model</td>\n",
    "    <td>information theory. recover input given noisy output by MLE or Bayesian inference</td>\n",
    "    <td>Handles uncertainty, incorporates prior knowledge</td>\n",
    "    <td>Computationally expensive, requires prior assumptions</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Latent Dirichlet Allocation (LDA)</td>\n",
    "    <td>Dirichlet prior for Topic modeling</td>\n",
    "    <td>Handles uncertainty, incorporates prior knowledge</td>\n",
    "    <td>Computationally expensive, requires prior assumptions</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Naive Bayes</td>\n",
    "    <td>classification and generative model. objective is MAP(Maximum a posteriori)</td>\n",
    "    <td>Simple, handles uncertainty, incorporates prior knowledge</td>\n",
    "    <td>Naive from independent feature assumption, can be affected by irrelevant features</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"6\">Neural Networks</td>\n",
    "    <td rowspan=\"3\">Recurrent Neural Network (RNN)</td>\n",
    "    <td>SRN (Simple Recurrent Network)</td>\n",
    "    <td>hidden states are determined by both current input and previous hidden state. Unidirectional with one hidden layer.</td>\n",
    "    <td>Handles short-term dependencies, simple architecture</td>\n",
    "    <td>Vanishing and exploding gradient problem, limited long-range dependency</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>LSTM (Long Short-Term Memory)</td>\n",
    "    <td>capture long-term dependency by gating mechanism like skip connection through time. 3 gates (input, forget, output)</td>\n",
    "    <td>Handles long-range dependencies, alleviates vanishing gradient problem</td>\n",
    "    <td>Computationally expensive, more complex architecture</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>GRU (Gated Recurrent Unit)</td>\n",
    "    <td>2 gates (update and reset)</td>\n",
    "    <td>Handles long-range dependencies, simpler than LSTM, faster training</td>\n",
    "    <td>can't count effectively (a^nb^n) as LSTM</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Transformer</td>\n",
    "    <td>GPT</td>\n",
    "    <td>Pretrained decoder-only Transformer</td>\n",
    "    <td>SOTA, versatile, scalable, parallelizable</td>\n",
    "    <td>Resource-intensive, requires large data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BERT</td>\n",
    "    <td>Pretrained encoder-only Transformer for contextualized word embedding</td>\n",
    "    <td>Versatile, scalable, parallelizable</td>\n",
    "    <td>Resource-intensive, requires large data</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan='2'>Recursive Neural Network</td>\n",
    "    <td>Input is tree-structured data (Parsed tree)</td>\n",
    "    <td>Handles hierarchical structures, suitable for parsing, sentiment analysis</td>\n",
    "    <td>Dependent on accurate tree structures, limited to specific tasks</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probablistic language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A language model assigns a probability to occurence of a sequence of words. the sum of probabilities over all possible sequences is 1.\n",
    "\n",
    "  $$\n",
    "  P(seq) = p(w_1,...,w_n)\n",
    "  $$\n",
    "\n",
    "\n",
    "- this probability is calculated by chain rule: conditional probability of a word occur given its preceding words (history) in the sequence.\n",
    "\n",
    "  $$\n",
    "  P(seq) =\\prod_{i=1}^n p(w_i|w_1, ..., w_{i-1})=p(w_1)p(w_2|w_1)...p(w_n|w_1,...,w_{n-1})\n",
    "  $$\n",
    "\n",
    "\n",
    "- As sequence length $n$ increases, the number of histories (possible unique sequences of words) grow exponentially as \n",
    "\n",
    "  $$|V|^{n-1}$$\n",
    "\n",
    "- number of parameters in the model for the last conditional probability is \n",
    "\n",
    "  $$(|V|-1)|V|^{n-1}$$\n",
    "\n",
    "  where $|V|$ is vocabulary size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## problem of MLP as language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram language model: a word only depend on previous $N-1$ words (context)\n",
    "\n",
    "$$P(w_{t} | w_{t-1}, w_{t-2}, ..., w_{1}) \\approx P(w_{t} | w_{t-1}, w_{t-2}, ..., w_{t-N+1})$$\n",
    "\n",
    "\n",
    "To estimate the conditional probability $P(w_n | w_{t-1}, w_{t-2}, ..., w_{t-N+1})$, we can use Bayes rule based on the counts of the n-grams in the training data. Specifically, we can estimate the probability as:\n",
    "\n",
    "$$P(w_{t} | w_{t-1}, w_{t-2}, ..., w_{t-N+1}) \n",
    "= \\frac{P(w_{t-N+1}, w_{t-N+2}, ..., w_{t})}{P(w_{t-N+1}, w_{t-N+2}, ..., w_{t-1})}\\\\[1em]\n",
    "\\approx \\frac{\\text{count}(w_{t-N+1}, w_{t-N+2}, ..., w_{t})}{\\text{count}(w_{t-N+1}, w_{t-N+2}, ..., w_{t-1})}$$\n",
    "\n",
    "where numerator is the number of times the n-gram $w_{t-N+1}, w_{t-N+2}, ..., w_{t}$ appears in the training data, \n",
    "\n",
    "and denominator is the number of times the (N-1)-gram $w_{t-N+1}, w_{t-N+2}, ..., w_{t-1}$ appears in the training data.\n",
    "\n",
    "e.g. for 4-gram N = 4, probability of word 'store' occurred given context 'walked to the' is\n",
    "\n",
    "$$P(\\text{store} | \\text{walked to the}) \\approx \\frac{\\text{count}(\\text{walked to the store})}{\\text{count}(\\text{walked to the})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems of N-gram model\n",
    "\n",
    "- Sparsity of input: \n",
    "\n",
    "    an input sequence is represented as a feature vector of ngram vocabulary size, whose entry is the number of occurrence of a unique n-gram.\n",
    "    \n",
    "    as N increases, the ngram vocabulary size grows exponentially, leading to sparse representations because most N-grams in the corpus are unique or rare. \n",
    "\n",
    "\n",
    "- large model size $O(|V|^N)$: \n",
    "\n",
    "    as N increase, input dimension increase exponentially.\n",
    "\n",
    "    increasing the model's size and memory requirements, lead to slower training and inference times and require more computational resources. \n",
    "\n",
    "\n",
    "- Limited context: context window size is small ($N-1$). can't hdandle longer-range dependencies. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# development cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create labelled dataset: expensive human labor\n",
    "\n",
    "- train model: computation (PFLOPs)\n",
    "\n",
    "- inference: user computation time (ms/example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# key to robust language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- large data size: A large amount of text data is essential to train the NLP system to understand various language structures, patterns, and nuances. Data diversity helps the system to generalize and adapt to different situations and domains.\n",
    "\n",
    "- Linguistic intuition: Incorporating knowledge of linguistic principles and structures helps the NLP system to better understand and process language. Understanding syntax, semantics, and pragmatics helps the system to make sense of complex language phenomena.\n",
    "\n",
    "- Appropriate representation: Choosing the right representation of data, such as word embeddings, syntax trees, or more advanced techniques like transformers, is crucial for the system's effectiveness in capturing language patterns and relationships.\n",
    "\n",
    "- Robust algorithms: Implementing efficient and robust algorithms for different NLP tasks, such as parsing, tokenization, sentiment analysis, or machine translation, is vital for the system's overall performance and accuracy.\n",
    "\n",
    "- World knowledge: Incorporating general knowledge about the world, including common sense knowledge, facts, and relationships, allows the NLP system to make inferences, resolve ambiguities, and better understand the context of language.\n",
    "\n",
    "- **Grounding**: connecting language to real-world knowledge or perceptual experiences. For example, grounding a word or phrase in an image, video, or a sensory experience. It often involves tasks like visual question-answering or image captioning, where models need to understand both linguistic and visual information to generate meaningful results."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, May 19 2021, 11:01:55) \n[Clang 10.0.0 ]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "15a162aef56c374f20e075fbc4add75b55bd097c903a763d3b0c4cccda1c0caf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
