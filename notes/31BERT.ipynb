{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT (Bidirectional Embedding Representations from Transformers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is versatile and SOTA on various tasks\n",
    "\n",
    "**contextualized embedding**: BERT produce word embeddings or sentence embeddings which capture bidirectional context info in a sentence. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two models were released:\n",
    "\n",
    "- BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million params.\n",
    "\n",
    "- BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params.\n",
    "\n",
    "Training data\n",
    "\n",
    "- BooksCorpus (800 million words)\n",
    "\n",
    "- English Wikipedia (2,500 million words)\n",
    "\n",
    "\n",
    "pretraining: 64 TPU chips for 4 days.\n",
    "\n",
    "**fine-tuning**: adding a task-specific output layer on top of encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## contextual embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contextual embeddings/BERT embeddings of input sequence $w_1, ..., w_n$ are the outputs of the last encoder block $h_1, ..., h_n$. \n",
    "\n",
    "$h_1$ is contextual embedding of [CLS] token, represent embedding for the whole input sequence\n",
    "\n",
    "- next sentence prediction: $h_1$ is input for FeedFward NN\n",
    "\n",
    "- fine-tuning on classification: $h_1$ is input for FeedFward NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a simple example of using the pre-trained BERT model from the Hugging Face's `transformers` library in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load BERT tokenizer and pre-trained base model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input text\n",
    "text = \"Here is an example of using BERT in PyTorch.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Run the text through the pre-trained BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# sentence representation : last hidden state of [CLS] token\n",
    "# It captures the aggregated contextual information from the entire input sequence.\n",
    "# It provides a fixed-size vector (batch_size, hidden_size) that can be used as input to a classifier for downstream tasks.\n",
    "sentence_representation = outputs.last_hidden_state[:, 0, :]  # (batch_size, sequence_length, hidden_size)\n",
    "# [CLS] token is always the first token in the input sequence\n",
    "\n",
    "print(sentence_representation)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretraining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Sentence Prediction (NSP)**: Predicting if a given sentence B follows sentence A in the original text. \n",
    "\n",
    "- a type of sentence-pair classification.\n",
    "\n",
    "- learn relationships between multiple sentences.\n",
    "\n",
    "- input: 2-sentence pairs. \n",
    "\n",
    "- output: binary (Yes or No)\n",
    "\n",
    "- Later work has argued this “next sentence prediction” is not necessary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Masked language modelling (MLM)**: \n",
    "\n",
    "Predict a random 15% of tokens.\n",
    "\n",
    "Loss is only evaluated for [CLS] and masked-out words. \n",
    "\n",
    "- 80%: Replace input word with [MASK]\n",
    "\n",
    "    The primary goal is to predict the masked tokens, so the majority of the masked positions will have the [MASK] token.\n",
    "\n",
    "- 10%: Replace input word with a random token \n",
    "\n",
    "    introduces noise and makes the model more robust. prevents the model from solely relying on the presence of the [MASK] token to make predictions, forcing it to build a better understanding of the context.\n",
    "\n",
    "- 10%: Leave input word unchanged, but \n",
    "still predict it.\n",
    "\n",
    "    addresses **a mismatch between pre-training and fine-tuning**. \n",
    "    \n",
    "    During fine-tuning, there are no [MASK] tokens in the input, so it is essential to train the model to make predictions even without explicit masking. \n",
    "    \n",
    "    helps the model build strong representations for **all words** in the sentence, not just the masked ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hryang06.github.io/assets/images/post/bert/bert-mlm-ex.PNG\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## architecture: Encoder-only Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- multiple stacks of encoder block (base 12, large 24)\n",
    "\n",
    "- each Encoder block has 2 sub-layers: a multi-head self-attention, a position-wise FC feedforward network.\n",
    "\n",
    "\n",
    "- **Input Representation**: WordPiece tokenization and combines it with positional encoding and segment encoding.\n",
    "\n",
    "  - Token Embeddings: $E \\in \\mathbb{R}^{m \\times d_{\\text{model}}}$\n",
    "\n",
    "  - Positional Embeddings: $P \\in \\mathbb{R}^{m \\times d_{\\text{model}}}$\n",
    "\n",
    "  - Segment Embeddings: $S \\in \\mathbb{R}^{m \\times d_{\\text{model}}}$\n",
    "\n",
    "  $m$: input sequence length (context size)\n",
    "\n",
    "  first token is `[CLS]` (classification), sentences are separated by `[SEP]` (separation)\n",
    "  \n",
    "- **Output**: token-level or sequence-level representations depending on the task\n",
    "\n",
    "  Output of each encoder layer along each token's path is an embedding for that token, Which output to use depends on the task\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment embeddings are used for next sentence prediction pre-training task\n",
    "\n",
    "For each token in the input sequence, a segment embedding is added depending on the sequence it belongs to.\n",
    "\n",
    "help model distinguish between the two input sequences "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "various tasks\n",
    "\n",
    "- QP: Quora Question Pairs (detect paraphrase \n",
    "questions)\n",
    "\n",
    "- QNLI: natural language inference over question \n",
    "answering data\n",
    "\n",
    "- SST-2: sentiment analysis\n",
    "\n",
    "- CoLA: corpus of linguistic acceptability (detect \n",
    "whether sentences are grammatical.)\n",
    "\n",
    "- STS-B: semantic textual similarity\n",
    "\n",
    "- MRPC: microsoft paraphrase corpus\n",
    "\n",
    "- RTE: a small natural language inference corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT variants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SpanBERT**: span masking\n",
    "\n",
    "RoBERTa: \n",
    "\n",
    "- change: more training data + dynamic masking for every epoch + more epoch + no next sentence prediction. \n",
    "\n",
    "- conclusion: more compute, more data can improve pretraining even when not changing the underlying Transformer encoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART: BERT + Autoregressive decoder = denoising autoencoder\n",
    "\n",
    "[ExBert - Huggingface Exploring Transformers]((https://huggingface.co/exbert/?model=gpt2&modelKind=bidirectional&sentence=The%20girl%20ran%20to%20a%20local%20pub%20to%20escape%20the%20din%20of%20her%20city.&layer=2&heads=..0,1,2,3,4,5,6,7,8,9,10,11&threshold=0.01&tokenInd=null&tokenSide=null&maskInds=..2&hideClsSep=true))\n",
    "\n",
    "DistilBERT, ALBERT: Smaller versions of BERT\n",
    "\n",
    "CamemBERT, FlauBERT: BERT in French\n",
    "\n",
    "PhoBERT, herBERT: BERT in Vietnamese and Polish, resp.\n",
    "\n",
    "mBERT: BERT in 104 languages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpanBERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Span Masking Objective:} \\quad L_{\\text{span\\_MLM}} = -E_{(i,j) \\sim \\text{Spans}}[\\log P(w_i^j|w_{-i^j})]\\\\\n",
    "\\textbf{Span Boundary Objective:} \\quad L_{\\text{SBO}} = -E_{(i,j,k,l) \\sim \\text{Spans}}[\\log P(w_i, w_j|w_k^l)]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpanBERT (Joshi & Chen et al., 2020): Improving Pre-training by Representing and Predicting Spans\n",
    "\n",
    "**omit next sentence prediction** task in BERT\n",
    "\n",
    "**Span Boundary Objective (SBO)**: \n",
    "\n",
    "- span maksing: mask contiguous spans of tokens as a sequence of [MASK] tokens of same length instead of individual tokens and then predict the entire span given its context.\n",
    "\n",
    "- Given observed left token and right token at masked span boundary, predict all the tokens in the span.\n",
    "\n",
    "    $$\n",
    "    y_i = f(h_{s-1}, h_{e+1}, pos_{i-s+1})\n",
    "    $$\n",
    "\n",
    "    $y_i$: predicted ith token in sentence\n",
    "\n",
    "    $f$: 2-layer FFNN with GeLU\n",
    "\n",
    "    $h_{s-1}, h_{e+1}$: hidden state of left and right boundary token\n",
    "\n",
    "    $pos_{i-s+1}$: position embedding of target token marking relative positions of the masked tokens with respect to the left boundary token at start-1 position\n",
    "\n",
    "- harder pretraining task useful for phrase-level tasks (e.g., named entity recognition, coreference resolution)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://hyunyoung2.github.io/img/Image/NaturalLanguageProcessing/NLPLabs/Paper_Investigation/Language_Model/2020-12-23-SpanBERT/SpanBERT.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
