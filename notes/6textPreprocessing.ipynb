{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cleaning\n",
    "    - Non-text\n",
    "        - Punctuation\n",
    "        - JavaScript\n",
    "    - Encoding\n",
    "\n",
    "- Segmentation\n",
    "\n",
    "    - Sentence segmentation (sentence boundary recognition): Document -> Sentence\n",
    "\n",
    "    - Tokenization\n",
    "        - Character level\n",
    "        - Subword level: Byte Pair Encoding or WordPiece\n",
    "        - Word level\n",
    "\n",
    "- Normalization\n",
    "\n",
    "    - Spell checking and correction\n",
    "\n",
    "    - Lowercase\n",
    "    \n",
    "    - Standardization\n",
    "\n",
    "- Stopword removal\n",
    "\n",
    "- Morphological processing\n",
    "\n",
    "    - Stemming (词干提取): Remove derivation morphology: happiness -> happy\n",
    "    \n",
    "    - Lemmatization (词形还原): Remove inflection morphology: better -> good\n",
    "\n",
    "- build vocabulary from training set. \n",
    "\n",
    "    - all novel words at test set are replaced by [UNK] token.\n",
    "\n",
    "- Part-of-speech tagging\n",
    "\n",
    "- Entity recognition\n",
    "    - Special characters (emoticons and emojis)\n",
    "    - Named entity\n",
    "    - URLs, emails\n",
    "    - Numbers and dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence boundary recognition, also known as sentence segmentation, is the task of identifying the boundaries between sentences in a given text. \n",
    "\n",
    "It is an essential preprocessing step in many NLP tasks, such as parsing, machine translation, and information extraction. \n",
    "\n",
    "Sentence boundary recognition can be treated as a classification problem, where each potential boundary (usually a punctuation mark) is classified as either a sentence boundary or not.\n",
    "\n",
    "To perform sentence boundary recognition, various features can be used, including:\n",
    "\n",
    "- Punctuation: Periods `.`, question marks `?`, and exclamation marks `!`.\n",
    "\n",
    "- Formatting: Line breaks, paragraph breaks.\n",
    "\n",
    "- Fonts: Changes in font style or size.\n",
    "\n",
    "- Spacing: spaces before and after punctuation marks. For example, if there is no space after a period, it is less likely to be a sentence boundary.\n",
    "\n",
    "- Capitalization: The first word of a sentence is typically capitalized in many languages.\n",
    "\n",
    "- Case: Similar to capitalization, the case of characters in a word can provide clues about sentence boundaries.\n",
    "\n",
    "- Use of abbreviations: Certain abbreviations, like \"Dr.\" or \"a.m.\", may be followed by a period but not necessarily indicate the end of a sentence.\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# subword tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| subword tokenization algorithm                | BPE                        | WordPiece                      |\n",
    "|------------------------|----------------------------|--------------------------------|\n",
    "| Vocab Initialization         | all raw characters   | all raw characters and some common subwords      |\n",
    "| Token selection        | Most frequent byte pairs | maximum likelihood of a token given its surrounding context.  |\n",
    "| Tokenization process   | Greedy longest match      | Greedy based on likelihood     |\n",
    "| Pros                   | Simple, language-agnostic | Better token selection in context |\n",
    "| Cons                   | Suboptimal token selection | language-dependent, Requires predefined vocabulary |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## motivation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unit of vocabulary in subword tokenization is subword (parts of words, bytes, characters)\n",
    "\n",
    "    byte: not refer to traditional definition of a storage unit -  byte = 8 bit. it means a string of characters the BPE algorithm operate on.\n",
    "\n",
    "- finite vocabulary assumption:  a fixed vocab of tens of thousands of words, built from the training set,  all novel words in test set are replaced as [UNK] token.\n",
    "\n",
    "- issue of finite vocab assumption: \n",
    "\n",
    "    - many languages has complex morphology, representing Tense, mood, definiteness, negation, information about the object, etc.\n",
    "    \n",
    "    - simply replace novel words of different morphology in test set as unknown tokens and have same embeddings will loss many information.\n",
    "\n",
    "    - morphology processing (stemming and lemmatization) can only address part of issue:\n",
    "    \n",
    "        - loss of information: words at base or root form result in ambiguity without context. morphological features carry important syntactic or semantic information are lost.\n",
    "\n",
    "        - languge-dependent: Stemming and lemmatization algorithms are typically language-dependent, requiring separate rule sets or resources for each language. This makes them less flexible and scalable than subword encoding techniques like BPE, which can be applied across languages.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
