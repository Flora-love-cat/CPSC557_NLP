{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Simple classification method based on Bayes rule\n",
    "\n",
    "- Relies on bag-of-words representation of documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### objective: MAP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Document classification task, given\n",
    "\n",
    "- Date: $d$ is a document with bag of words as features $w_1, ..., w_k$.\n",
    "\n",
    "- Label: $c$ is a class (e.g., spam or not spam)\n",
    "\n",
    "model joint probability $P(d, c)$ by maximizing posterior probability $P(c|d)$\n",
    "\n",
    "$$\n",
    "P(d, c) = P(c)\\prod_{i=1}^kP(w_i | c)\n",
    "$$\n",
    "\n",
    "MAP (Maximum a posteriori = most likely class)\n",
    "\n",
    "\\begin{align}\n",
    "c_{MAP} \n",
    "&=\\underset{c \\in C}{\\mathrm{argmax}}\\ P(c|d)\\\\[1em]\n",
    "\\text{(Bayes Rule)}&=\\underset{c \\in C}{\\mathrm{argmax}}\\ \\frac{P(d | c)P(c)}{P(d)}\\\\[1em]\n",
    "\\text{(P(d) is constant)}&\\approx \\underset{c \\in C}{\\mathrm{argmax}}\\ P(d | c)P(c)\\\\[1em]\n",
    "&= \\underset{c \\in C}{\\mathrm{argmax}}\\ P(w_1, ..., w_k | c)P(c)\\\\[1em]\n",
    "\\text{Naive assumption}&= \\underset{c \\in C}{\\mathrm{argmax}}\\ P(c) \\prod_{i=1}^kP(w_i | c)\\\\[1em]\n",
    "&=\\underset{c \\in C}{\\mathrm{argmax}} P(d, c)\\\\[1em]\n",
    "\\text{take log}&= \\underset{c \\in C}{\\mathrm{argmax}}\\ \\log P(c) + \\underbrace{\\sum_{i=1}^k \\log P(w_i | c)}_{\\text{linear model}}\n",
    "\\end{align} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Naive Assumptions: features are independent give class. \n",
    "\n",
    "- arise from Bag of Words assumption: Position of words doesn't matter\n",
    "\n",
    "- that's why Naive Bayes called **Naive**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: training set $D$ and labels $C$\n",
    "\n",
    "For each class $c \\in C$:\n",
    "\n",
    "1. **estimate prior** $P(c)$ by MLE\n",
    "\n",
    "   $$\\log P(c) = \\log\\frac{N_c}{N_{\\text{doc}}}$$\n",
    "\n",
    "   where $N_{\\text{doc}}$ is the number of documents in $D$\n",
    "\n",
    "   $N_c$ is the number of documents from $D$ in class $c$.\n",
    "\n",
    "2. Create a vocabulary $V$ of words from training set\n",
    "\n",
    "\n",
    "3. Create a mega-document by concatenating all documents $d \\in D$ with class $c$.\n",
    "\n",
    "\n",
    "4. For each word $w$ in vocabulary $V$\n",
    "\n",
    "   1. count number of occurrences of $w$ in the mega-document: \n",
    "   \n",
    "      $$\\text{count}(w,c)$$\n",
    "\n",
    "   2. **estimate likelihood** $P(w|c)$ by MLE\n",
    "   \n",
    "      $$\n",
    "      \\log P(w|c) = \\log\\frac{\\text{count}(w,c) + \\alpha}{\\sum_{w' \\in V} \\text{count}(w',c) +\\alpha |V|}\n",
    "      $$ \n",
    "\n",
    "      Laplacian add-$\\alpha$ smoothing to avoid zero probabilities due to no training documents with a word $w$ classified in class $c$\n",
    "\n",
    "return log prior $\\log P(c)$, log likelihood $\\log P(w|c)$, vocabulary $V$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: a test document, logprior, loglikelihood, labels $C$, vocabulary $V$\n",
    "\n",
    "For each class $c \\in C$:\n",
    "\n",
    "1. Initialize log MAP to be log prior\n",
    "\n",
    "   $$\\log MAP = \\log P(c)$$\n",
    "\n",
    "2. For each word position $w_i$ in the test document:\n",
    "\n",
    "   If word in vocabulary, add log likelihood to log MAP.\n",
    "\n",
    "   $$\n",
    "   \\log MAP = \\log P(c) + \\log P(w_i|c) = \\log P(c)P(w_i|c)=...\\\\[1em]\n",
    "   =\\log P(c)\\prod_{i\\in \\text{position}}P(w_i|c)\n",
    "   $$\n",
    "\n",
    "Return class with the maximum posterior probability \n",
    "\n",
    "$$c_{MAP}=\\underset{c \\in C}{\\arg\\max} P(c)\\prod_{i=1}P(w_i|c)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### advantages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- computationally efficient and requires minimal memory: suitable for large-scale applications.\n",
    "\n",
    "- Robust to noise and irrelevant features: cancel each other out without affecting the results significantly.\n",
    "\n",
    "- Very good in domains with many equally important features: whereas decision trees may suffer from fragmentation, especially when the data is limited.\n",
    "\n",
    "- Optimal if the independence assumptions hold\n",
    "\n",
    "- A good baseline for text classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes generative model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian approaches can naturally handle missing features by Simply ignore them and compute the likelihood based only on observed features.\n",
    "\n",
    "There is no need to fill-in or explicitly model missing values.\n",
    "\n",
    "\n",
    "e.g., three coin tosses $E = \\{H, ?, T\\} \\to P(E) = P(\\{H, H, T\\}) + P(\\{H, T, T\\})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "&P(x_1, x_2, \\dots, x_{j-1}, ?, x_{j+1}, \\dots, x_d | y)\\\\[1em]\n",
    "&= \\sum_{Z_j} P(x_1, x_2, \\dots, x_{j-1}, z_j, x_{j+1}, \\dots, x_d | y)\\\\[1em]\n",
    "\\text{Naive assumption}&= \\sum_{z_j} \\left[P(z_j | y) \\prod_{k \\neq j} P(x_k | y)\\right]\\\\[1em]\n",
    "&=  \\left[\\prod_{k \\neq j} P(x_k | y)\\right]\\sum_{z_j} P(z_j | y)\\\\[1em]\n",
    "\\text{ignore missing values}&=  \\prod_{k \\neq j} P(x_k | y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
