{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-Speech tagging is a task of assigning appropriate part-of-speech labels (or tags) to each word in a given text.\n",
    "\n",
    "can be solved by Viterbi algorithm with HMM model, classification (MLP, LSTM, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM is an extension of Naive Bayes to sequences.\n",
    "\n",
    "computes the posterior probability distribution $P(T|W)$ over **all possible** tag sequences given the words.\n",
    "\n",
    "Uses Bayes' theorem to relate the likelihood, prior, and posterior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "T &= \\arg\\max_T P(T|W)\\\\[1em]\n",
    "\\text{(Bayes theorem)}&= \\arg\\max_T\\frac{P(T)P(W|T)}{P(W)}\\\\[1em]\n",
    "\\text{(ignore P(W))}&=\\arg\\max_T P(T)P(W|T)\\\\[1em]\n",
    "\\text{(simplify)}&= \\prod P(t_i|t_1, ..., t_{i-1})P(w_i|t_1, ..., t_i)\\\\[1em]\n",
    "&= \\prod P(t_i|t_{i-1})P(w_i|t_i)\\\\[1em]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Simplifications:\n",
    "1. likelihood $P(W|T) = \\prod P(w_i|t_i)$\n",
    "2. prior $P(T) = \\prod P(t_i|t_{i-1})$ (Bigram approximation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM: Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMM task: find state sequence $\\mathcal{Q}$ given observation sequence $O$ and HMM model $\\mu$\n",
    "\n",
    "$$\n",
    "Q=\\arg\\max_{Q}P(Q|O, \\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging: find **most likely** tag sequence $T=\\{t_1, ..., t_n\\}$ given a sequence of words $W=\\{w_1, ..., w_n\\}$\n",
    "\n",
    "$$\n",
    "T=\\arg\\max_{T}P(T|W)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solution\n",
    "\n",
    "- brute force: Using HMM model $\\mu$, compute probability $P(T|W)$ for all possible tag sequences $T$. \n",
    "\n",
    "  cons: computationally infeasible due to large number of combinations.\n",
    "\n",
    "- Greedy Search: Choose the best tag for each word independently, without considering the overall sequence of tags. \n",
    "\n",
    "  cons: suboptimal solutions since dependencies between tags are not considered.\n",
    "\n",
    "- Beam Search: A more efficient approach that uses partial hypotheses.\n",
    "\n",
    "  At each state, only the top $k$ best hypotheses are retained. \n",
    "  \n",
    "  cons: miss optimal solution due to early pruning \n",
    "\n",
    "- **Viterbi algorithm**: best solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi algorithm is an efficient algorithm for POS tagging using an HMM\n",
    "\n",
    "features:\n",
    "\n",
    "* dynamic programming: solve the problem efficiently.\n",
    "\n",
    "* memoization: store intermediate results and avoid redundant computations.\n",
    "\n",
    "* backpointers: trace back the optimal path.\n",
    "\n",
    "Algorithm\n",
    "\n",
    "1. Initialization $1 \\leq j \\leq N$:\n",
    "\n",
    "    $$\n",
    "    v_1(j) = \\pi_j b_j(o_1)\\\\[1em]\n",
    "    b_1(j)=0\n",
    "    $$\n",
    "\n",
    "2. Recursion $1 \\leq j \\leq N, 1 < t \\leq T$:\n",
    "   \n",
    "   $$\n",
    "   v_t(j) = \\max_{1 \\leq i \\leq N} v_{t-1}(i) a_{ij} b_j(o_t) \\\\[1em]\n",
    "   b_t(j) = \\arg\\max_{1 \\leq i \\leq N} v_{t-1}(i) a_{ij}b_j(o_t)\n",
    "   $$\n",
    "\n",
    "3. Termination:\n",
    "\n",
    "   Best score: $P^* = \\max_{1 \\leq i \\leq N} v_T(i)$\n",
    "\n",
    "   Start of backtrace: $q_T^* = \\arg\\max_{1 \\leq i \\leq N} v_T(i)$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
