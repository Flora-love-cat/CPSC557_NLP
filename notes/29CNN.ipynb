{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classic models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence Classification (Kim 2014): word embeddings followed by convolutional and max-pooling layers and a fully connected output layer.\n",
    "\n",
    "- Dynamic Convolutional Neural Network (2014 Kalchbrenner): dynamic k-max pooling operation to handle variable-length input sentences and capture n-gram features\n",
    "\n",
    "- Paraphrase Identification (Yin & Sch√ºtze 2015)\n",
    "\n",
    "- Summarization-based Video Caption (Li et al. 2015)\n",
    "\n",
    "- Question Answering (Dong et al. 2015)\n",
    "\n",
    "- Matching Natural Language Sentences (Hu et al. 2015)\n",
    "\n",
    "- Learning Semantic Representations (Sheng et al. 2015)\n",
    "\n",
    "- Sentiment Analysis of Short Texts (dos Santos & Gatti 2014)\n",
    "\n",
    "- Relation Extraction (Nguyen & Grishman 2015)\n",
    "\n",
    "- Entity Disambiguation (Sun et al.2015)\n",
    "\n",
    "- Modeling Interestingness with Deep Neural Networks (Gao 2015)\n",
    "\n",
    "- Character-Aware word embedding (2016 Kim)\n",
    "\n",
    "- Text Classification (2016 Conneau)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adjustment of CNN for NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for NLP tasks, CNNs need some adjustments to handle sequential data effectively\n",
    "\n",
    "- wor embedding matrix for Input representation: each row of the matrix corresponds to the embedding of a token in the input sequence. can be Static or/and non-static channels\n",
    "\n",
    "- 1D convolutions: process input along sequence dimension. Filters slide across the sequence capturing local patterns (n-grams).\n",
    "\n",
    "- padding, truncation, or dynamic pooling: handle various-length inputs and ensure consistent output sizes.\n",
    "\n",
    "- Stacking: capture the hierarchical structure of language. Same idea as with RNN and LSTM\n",
    "\n",
    "- Dilated convolution: handle long-range dependency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### static vs. non-static channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "static and non-static channels refer to the way word embeddings are treated during training.\n",
    "\n",
    "In CNN models for NLP, both static and non-static channels can be used simultaneously. \n",
    "\n",
    "leverage the benefits of both approaches, with static channel providing general semantic information and non-static channel capturing task-specific nuances.\n",
    "\n",
    "- Static channel: \n",
    "\n",
    "    word embeddings are fixed throughout the training without adjusting them to the specific task at hand. \n",
    "    \n",
    "    e.g., pre-trained word vectors like Word2Vec or GloVe.\n",
    "\n",
    "    pros: prevent overfitting, especially when there is a limited amount of training data. \n",
    "\n",
    "    cons: fixed parameters can't be fine-tuned for specific task\n",
    "\n",
    "\n",
    "- Non-static channel: \n",
    "\n",
    "    word embeddings are initialized with pre-trained word vectors but are updated during the training process. \n",
    "\n",
    "    pros: improved performance, as the embeddings can be adapted to capture nuances relevant to the target task.\n",
    "    \n",
    "    cons: overfitting, especially if the available training data is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 align='center'>CNN model architecture with a single static channel for an example utterance.<h5/>\n",
    "<img src='https://www.researchgate.net/profile/Hyunjung-Lee-3/publication/315468897/figure/fig1/AS:501882825461760@1496669598515/CNN-model-architecture-with-single-channel-for-an-example-utterance.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 align='center'>CNN model architecture with 2 channels (static and non-static) for an example utterance.<h5/>\n",
    "<img src='https://d3i71xaburhd42.cloudfront.net/15637f68230040fe0792fa505573049aa64045ee/5-Figure3-1.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
