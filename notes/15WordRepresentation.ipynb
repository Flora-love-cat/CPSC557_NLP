{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word representation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Method\t|Pros\t|Cons      |\n",
    "|-------|-------|----------|\n",
    "|Image\t|Visual representation|High-dimensional data, may not capture abstract concepts|\n",
    "|Denotational Semantics\t|Formal representation, based on logical relations\t|Requires complex, formal knowledge representation|\n",
    "|Dictionary Definition\t|explicit meaning, captures semantic relationships\t|Verbose. incomplete synonym. outdated. subjective. manually created. manually created. hard to build for different language or domains. hard to compute word similarity|\n",
    "|One-hot Encoding vector\t|Simple, easy to implement\t|High-dimensional ($\\|\\mathcal{V}\\|$). sparse. lacks semantic relationships. orthogonal vectors can't interpretate|\n",
    "|Distributed Embedding\t|Captures semantic relationships, lower-dimensional, dense\t| Requires large training data, may not capture rare words|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embedding algorithm: a function $f: \\mathcal{V} \\rightarrow R^n$ mapping from vocabulary to a **contiouns low dimensional dense** vector space (n = 50-300) that capture semantic and syntactic relationships between words\n",
    "\n",
    "each word embedding $f(w)$ is hidden feature vector extracted from data that represent word meanings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th colspan='2'>Type</th>\n",
    "    <th>Model</th>\n",
    "    <th>Short Description</th>\n",
    "    <th>Pros</th>\n",
    "    <th>Cons</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td colspan='2'>Count-based</td>\n",
    "    <td>LSA</td>\n",
    "    <td>Co-occurrence Matrix factorization by SVD</td>\n",
    "    <td>Simple, captures semantic relationships</td>\n",
    "    <td>High memory usage, can't capture polysemy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"5\">Prediction-based</td>\n",
    "    <td rowspan=\"3\">Distributed</td>\n",
    "    <td >GloVe</td>\n",
    "    <td >combines count-based and prediction-based </td>\n",
    "    <td >captures semantic relationships, low memory usage</td>\n",
    "    <td >Less efficient with rare words</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Word2Vec</td>\n",
    "    <td>Predicts words in a context, Skip-gram or CBOW </td>\n",
    "    <td>captures semantic and syntactic relationships, low memory usage</td>\n",
    "    <td>can't capture polysemy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>FastText</td>\n",
    "    <td>Word2Vec Extension, subword embedding</td>\n",
    "    <td>can learn embeddings of rare words and OOV words</td>\n",
    "    <td>Higher memory usage compared to Word2Vec</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Contextualized</td>\n",
    "    <td>ELMo</td>\n",
    "    <td>bidirectional LSTM</td>\n",
    "    <td>Context-aware, captures polysemy</td>\n",
    "    <td>Higher memory usage and computational complexity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BERT</td>\n",
    "    <td>Encoder-only Transformers</td>\n",
    "    <td>Context-aware, powerful, supports transfer learning</td>\n",
    "    <td>High memory usage and computational complexity</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other types: Syntactic, Mutilingual, Multisense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Syntactic</td>\n",
    "    <td>Dependency-based</td>\n",
    "    <td>dependency parsing</td>\n",
    "    <td>Captures syntactic relationships</td>\n",
    "    <td>Requires accurate dependency parsing, doesn't capture polysemy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>POS-based</td>\n",
    "    <td>part-of-speech tags</td>\n",
    "    <td>Simple, captures syntactic relationships</td>\n",
    "    <td>Requires accurate POS tagging, doesn't capture polysemy</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"3\">Multilingual</td>\n",
    "    <td>MUSE</td>\n",
    "    <td>Unsupervised alignment of monolingual embeddings in a shared space</td>\n",
    "<td>Supports multiple languages, allows for cross-lingual tasks</td>\n",
    "<td>Quality depends on source embeddings</td>\n",
    "\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Aligning FastText</td>\n",
    "    <td>Aligns FastText embeddings in a shared multilingual space</td>\n",
    "    <td>Supports multiple languages, allows for cross-lingual tasks</td>\n",
    "    <td>Quality depends on source embeddings</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>XLM</td>\n",
    "    <td>Cross-lingual Language Model, based on BERT </td>\n",
    "    <td>Powerful, supports multiple languages, allows for transfer learning</td>\n",
    "    <td>High memory usage and computational complexity</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Sense</td>\n",
    "    <td>Sense2Vec</td>\n",
    "    <td>Adaptation of Word2Vec, generating embeddings for word senses</td>\n",
    "    <td>Disambiguates word senses, low memory usage</td>\n",
    "    <td>Requires accurate sense annotation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DeConf</td>\n",
    "    <td>Deconfuses Word2Vec embeddings using sense-annotated corpus</td>\n",
    "    <td>Disambiguates word senses, low memory usage</td>\n",
    "    <td>Requires accurate sense annotation</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional semantics hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distributed word embeddings are based on Distributional Semantics Hypothesis \n",
    "\n",
    "**distributional hypothesis** (Harris, 1954): \"a word is characterized by the company it keeps\"\n",
    "\n",
    "interpretation: \n",
    "    \n",
    "- linguistic items with similar distributions have similar meanings.\n",
    "\n",
    "- **words that occur in similar contexts have similar meanings**\n",
    "\n",
    "    here, contexts are also words that surround target words\n",
    "\n",
    "- synonyms (words of similar meaning) should have similar embeddings\n",
    "\n",
    "- Different senses of a same word should have different embeddings\n",
    "\n",
    "- Relations of words should be preserved: cat-kitten should be similar to dog-puppy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "various context features\n",
    "\n",
    "- The word **before/after** the target word\n",
    "\n",
    "- Any word **within n words** of the target word\n",
    "\n",
    "- Any word **within a specific syntactic relationship** with the target word (e.g., the head of the dependency or the subject of the sentence)\n",
    "\n",
    "- Any word within the **same sentence/document**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "effectiveness of word embedding algorithm in capturing semantic and syntactic relationships in language can be evaluated in a combination of intrinsic and extrinsic methods.\n",
    "\n",
    "- Intrinsic Evaluation: nearest neighbors, information retrieval, Word Similarity\n",
    "\n",
    "- Extrinsic Evaluation: performance of downstream NLP tasks taking word embeddings as input features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity\n",
    "\n",
    "Comparing 2-word cosine similarity between word embeddings with human-annotated similarity scores in benchmark datasets like WordSim-353, SimLex-999, or MEN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Analogy\n",
    "\n",
    "solve syntactic analogy task \"A is to A' as B is to B'\" (e.g., \"man\" is to \"woman\" as \"king\" is to \"queen\")ï¼Œ predict B' from vocabulary.\n",
    "\n",
    "- A popular benchmark is Google Word Analogy dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- equation:\n",
    "\n",
    "    $$\n",
    "    \\phi(king)-\\phi(man)\\approx \\phi(?)-\\phi(woman)\n",
    "    $$\n",
    "\n",
    "- objective: minimize the $l_2$ norm\n",
    "\n",
    "    $$\n",
    "    \\hat w = \\arg \\min_w \\left \\| \\phi(king)-\\phi(man)+ \\phi(woman)-\\phi(w) \\right \\|^2\n",
    "    \\\\[1em]\n",
    "    \\Rightarrow \\hat w = queen\n",
    "    $$\n",
    "\n",
    "- alternative objective: maximize 2 similarities and one difference\n",
    "\n",
    "    $$\n",
    "    \\arg \\max_{b'} \\cos(b', b-a+a') = \\cos(b', b)-\\cos(b', a)+\\cos(b', a')\\\\[1em]\n",
    "    \\arg \\max_{b'}=\\frac{(b-a+a')^T b'}{||b-a+a'||}\n",
    "    $$\n",
    "\n",
    "- metric: accuracy\n",
    "\n",
    "    true B' is optimal result of Levy and Goldberg's similarity multiplication method\n",
    "    \n",
    "    $$\n",
    "    B' = \\arg \\max_{B'} \\frac{\\cos(B', A')\\cos(B',B)}{\\cos(B',A)+\\epsilon}\n",
    "    $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words are not distributed evenly across texts.\n",
    "\n",
    "- Stop Words\n",
    "\n",
    "    250-300 most common words in English account for >50% of a given text.\n",
    "\n",
    "    e.g., \"the\" and \"of\" represent 10% of tokens, while \"and\", \"to\", \"a\", and \"in\" make up another 10%. \n",
    "\n",
    "- token/type ratio\n",
    "\n",
    "    In the first chapter of Moby Dick, there are 859 unique words (types) and 2256 word occurrences (tokens). \n",
    "    \n",
    "    The top 65 types cover 1132 tokens, which is >50% of the total tokens.\n",
    "\n",
    "\n",
    "- Pareto Principle (80/20 Rule)\n",
    "\n",
    "    uneven distributions in various domains: letters of the alphabet (ETAOIN SHRDLU), city sizes, wealth, etc.\n",
    "\n",
    "    80% of the wealth goes to 20% of the people or it takes 80% of the effort to build the easier 20% of the system.\n",
    "\n",
    "\n",
    "- Power-law Distribution\n",
    "\n",
    "    \\begin{align}\n",
    "    &p(x) =cx^{-\\alpha}\\\\[1em]\n",
    "    &\\text{log-log scale\\ } \\ln p(x)=c-\\alpha \\ln x\n",
    "    \\end{align}\n",
    "\n",
    "    The probability of observing an item of size $x$ is $x$ power to ${-\\alpha}$.\n",
    "\n",
    "    item can be everything, e.g., word frequency, citations, web hits, books sold, telephone calls received, earthquake magnitude.\n",
    "    \n",
    "    $c$ is normalization constant ensuring probabilities over all items sum to 1.\n",
    "    \n",
    "    $\\alpha$ is scaling exponent or power law exponent.\n",
    "\n",
    "    Power-law distributions are high skew (asymmetry) in linear scale and linear in log-log scale: many items with a small frequency of occurrence and a few items with a very large frequency.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zipf's Law in Natural Language**\n",
    "\n",
    "$$Rank \\times Frequency \\approx 0.1 \\times len(text)$$\n",
    "\n",
    "- frequency of a word is inversely proportional to its rank in the frequency table.\n",
    "\n",
    "- nth most frequent word has a frequency approximately proportional to 1/n.\n",
    "\n",
    "- 2nd frequent word has 1/2 frequency of 1st frequent word\n",
    "\n",
    "- 3rd frequent word has 1/3 frequency of 1st frequent word\n",
    "\n",
    "- Although Zipf's Law is not accurate at the tails of the distribution, it is accurate enough for practical purposes in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "15a162aef56c374f20e075fbc4add75b55bd097c903a763d3b0c4cccda1c0caf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
