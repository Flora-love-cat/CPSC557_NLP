{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW0 Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from collections import Counter, OrderedDict\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load 3 Corpus `punkt`, `wordnet_ic` and `wordnet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/wenxinxu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet_ic')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Warm up with Linux and Python Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment, you will learn a few handy Linux commands and play with a Python script.\n",
    "\n",
    "First import some modules(pre-written code). Here nltk is most popular module used in nlp. We did it right above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a variable (in this case, it’s a string) called `greeting` 'Hello, world!'. \n",
    "\n",
    "Then use the nltk module’s `word_tokenize` method to make a list of all of the tokens—words or punctuation—in our `greeting` string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = 'Hello, world!'\n",
    "token_list = word_tokenize(greeting)\n",
    "#TODO: add your own code to convert tokens into lower case\n",
    "token_list = [token.lower() for token in token_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python lists are kind of like arrays that automatically grow or shrink as needed. Like an array, you can iterate through it or index into it. We then use the for loop to print each token on its own line.\n",
    "If you are unfamiliar with the following string formatting, check out this [tutorial](https://realpython.com/python-f-strings/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token list is: ['hello', ',', 'world', '!']\n",
      "The tokens in the greeting are\n",
      "hello\n",
      ",\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "print(f\"The token list is: {token_list}\")\n",
    "print(f\"The tokens in the greeting are\")\n",
    "for token in token_list:\n",
    "    print(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the number of occurrence of each word in a word list. Also, we can see if there are some repetitions of the word 'squirrel'. Let’s find out! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'hello': 1, ',': 1, 'world': 1, '!': 1}), 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: add your own code\n",
    "from collections import Counter \n",
    "\n",
    "occur = Counter(token_list)\n",
    "\n",
    "occur, occur['squirrel']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should go back to the pure code part to fill out the code in `A.py`, and play with some Linux command lines. \n",
    "\n",
    "But wait! You maybe have learned some handy Linux commands, just for review, you can find plenty of Linux cheat sheets like [this](https://files.fosswire.com/2007/08/fwunixref.pdf). \n",
    "\n",
    "If you’re looking for more detail, [nixCraft](http://www.cyberciti.biz/) is often helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, modify line 4 of `A.py` so that the command `echo \"This is a different greeting\" | python3 A.py` produces the following output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is a different greeting\n",
    "\n",
    "The tokens in the greeting are \n",
    "This\n",
    "is\n",
    "a\n",
    "different \n",
    "greeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: you can even excute command lines in this notebook. A simple tutorial is [here](http://www.math.uiuc.edu/~gfrancis/illimath/windows/aszgard_mini/doc/IPython81Manual/node12.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a different greeting\n",
      "\n",
      "The tokens in the greeting are\n",
      "This\n",
      "is\n",
      "a\n",
      "different\n",
      "greeting\n",
      "There were 0 instances of the word 'squirrel' and 0 instances of the word 'girl.'\n"
     ]
    }
   ],
   "source": [
    "! echo \"This is a different greeting\" | python3 A.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: If you’re having trouble of printing the exactly same lines, take a look at this [StackExchange Q&A](http://stackoverflow.com/questions/11109859/pipe-output-from-shell-command-to-a-python-script).\n",
    "\n",
    "What just happened? You used a pipe ( | ) to use the output of the first command as input to the second. Your first command was an `echo`, which just outputs the same thing you type in.\n",
    "\n",
    "What if you already have a file with the text you want to use as input to your script? \n",
    "\n",
    "For example, suppose your GSI had already typed up the Squirrel Girl1 theme song for you, and you didn’t want to retype the whole thing. \n",
    "\n",
    "One thing you can do is use `cat <filename>`, which outputs the text of the file `cat squirrel_girl.txt` should print the Squirrel Girl lyrics to the screen. \n",
    "\n",
    "You can also run `A.py` on these lyrics by piping the output of your `cat` command into your `python` command, like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squirrel Girl, Squirrel Girl!\n",
      "She's a human and also squirrel!\n",
      "Can she climb up a tree?\n",
      "Yes she can, easily.\n",
      "That's whyyyy\n",
      "Her name is Squirrel Girl!\n",
      "\n",
      "Is she tough? Listen bud:\n",
      "She's got partially squirrel blood.\n",
      "Who's her friend? Don't you know:\n",
      "That's the squirrel, Tippy Toe.\n",
      "Surprise!\n",
      "She likes to talk to squirrels!\n",
      "\n",
      "At the top of the trees,\n",
      "is where she spends her time.\n",
      "Like a huuuuman squirrel \n",
      "She enjoys fighting crime!!\n",
      "\n",
      "Squirrel Girl, Squirrel Girl!\n",
      "Powers of both squirrel and girl!\n",
      "Finds some nuts, eats some nuts!\n",
      "Kicks bad guuuuuys' evil butts!\n",
      "To her, life is a great big acorn!\n",
      "Where there's a city crime-torn,\n",
      "You'll find the Squirrel Girl!!!!\n",
      "\n",
      "The tokens in the greeting are\n",
      "Squirrel\n",
      "Girl\n",
      ",\n",
      "Squirrel\n",
      "Girl\n",
      "!\n",
      "She\n",
      "'s\n",
      "a\n",
      "human\n",
      "and\n",
      "also\n",
      "squirrel\n",
      "!\n",
      "Can\n",
      "she\n",
      "climb\n",
      "up\n",
      "a\n",
      "tree\n",
      "?\n",
      "Yes\n",
      "she\n",
      "can\n",
      ",\n",
      "easily\n",
      ".\n",
      "That\n",
      "'s\n",
      "whyyyy\n",
      "Her\n",
      "name\n",
      "is\n",
      "Squirrel\n",
      "Girl\n",
      "!\n",
      "Is\n",
      "she\n",
      "tough\n",
      "?\n",
      "Listen\n",
      "bud\n",
      ":\n",
      "She\n",
      "'s\n",
      "got\n",
      "partially\n",
      "squirrel\n",
      "blood\n",
      ".\n",
      "Who\n",
      "'s\n",
      "her\n",
      "friend\n",
      "?\n",
      "Do\n",
      "n't\n",
      "you\n",
      "know\n",
      ":\n",
      "That\n",
      "'s\n",
      "the\n",
      "squirrel\n",
      ",\n",
      "Tippy\n",
      "Toe\n",
      ".\n",
      "Surprise\n",
      "!\n",
      "She\n",
      "likes\n",
      "to\n",
      "talk\n",
      "to\n",
      "squirrels\n",
      "!\n",
      "At\n",
      "the\n",
      "top\n",
      "of\n",
      "the\n",
      "trees\n",
      ",\n",
      "is\n",
      "where\n",
      "she\n",
      "spends\n",
      "her\n",
      "time\n",
      ".\n",
      "Like\n",
      "a\n",
      "huuuuman\n",
      "squirrel\n",
      "She\n",
      "enjoys\n",
      "fighting\n",
      "crime\n",
      "!\n",
      "!\n",
      "Squirrel\n",
      "Girl\n",
      ",\n",
      "Squirrel\n",
      "Girl\n",
      "!\n",
      "Powers\n",
      "of\n",
      "both\n",
      "squirrel\n",
      "and\n",
      "girl\n",
      "!\n",
      "Finds\n",
      "some\n",
      "nuts\n",
      ",\n",
      "eats\n",
      "some\n",
      "nuts\n",
      "!\n",
      "Kicks\n",
      "bad\n",
      "guuuuuys\n",
      "'\n",
      "evil\n",
      "butts\n",
      "!\n",
      "To\n",
      "her\n",
      ",\n",
      "life\n",
      "is\n",
      "a\n",
      "great\n",
      "big\n",
      "acorn\n",
      "!\n",
      "Where\n",
      "there\n",
      "'s\n",
      "a\n",
      "city\n",
      "crime-torn\n",
      ",\n",
      "You\n",
      "'ll\n",
      "find\n",
      "the\n",
      "Squirrel\n",
      "Girl\n",
      "!\n",
      "!\n",
      "!\n",
      "!\n",
      "There were 11 instances of the word 'squirrel' and 7 instances of the word 'girl.'\n"
     ]
    }
   ],
   "source": [
    "! cat squirrel_girl.txt | python3 A.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of repeated words in this song, aren’t there? \n",
    "\n",
    "I wonder if there are more repetitions of the word squirrel or girl. \n",
    "\n",
    "Let’s find out! Add the following line to the end of A.py:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"There were {squirrel} instances of the word 'squirrel' and {girl} instances of the word 'girl.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, modify the $for$ loop to keep a count of the number of times “squirrel” appears and the number of times “girl” appears. Make the count case insensitive.\n",
    "\n",
    "Hints:\n",
    "* Make sure you’re initializing both variables (`squirrel` and `girl`) to 0 before the loop!\n",
    "* The old standby `++` that you may know from other languages won’t work in Python, but you can still use `+= 1`\n",
    "* Lower the words, see [here](https://docs.python.org/2/library/stdtypes.html#str.lower)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half your grade for this assignment will be based on whether `A.py` is producing the right output, so you’d probably like to confirm that the output is correct. We’ve given you the correct output in `correct_outputA.txt`. You can check that your output matches ours by saving yours to a file, then using the `diff` command. \n",
    "\n",
    "Run the following lines in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat squirrel_girl.txt | python3 A.py > outputA.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! diff outputA.txt correct_outputA.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `>` operator in the first line writes the text that would normally be printed to stdout into the file `outputA.txt` instead. \n",
    "\n",
    "If everything’s working, the second line should return with no output; this means there were no differences between the two files. \n",
    "\n",
    "If you accidentally replaced the first line of the song with “This line shouldn’t be here.” and then ran `diff`, you would see"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1c1\n",
    "< This line shouldn't be here.\n",
    "---\n",
    "> Squirrel Girl, Squirrel Girl!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would show you exactly where your output was wrong, which should help you pinpoint the problem in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More general Python and Numpy tutorial can be found at [Stanford CS231N](http://cs231n.github.io/python-numpy-tutorial/) if you aren't familiar with Python."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word similarity can play an important role in information retrieval. For instance, suppose a user asked for information about birds, and your system had to decide which of the following passages to return:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Stoats (Mustela erminea) were introduced into New Zealand to control rabbits and hares, but are now a major threat to the native bird population. The natural range of the stoat is limited to parts of the Northern Hemisphere. Immediately prior to human settlement, New Zealand did not have any land-based mammals apart from bats, but Polynesian and European settlers introduced a wide variety of animals._\n",
    "(From [resource](https://en.wikipedia.org/wiki/Stoats_in_New_Zealand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Eagles tend to be large birds with long, broad wings and massive feet. Booted eagles have legs and feet feathered to the toes and build very large stick nests. Ospreys, a single species found worldwide that specializes in catching fish and builds large stick nests. Kites have long wings and relatively weak legs. They spend much of their time soaring. They will take live vertebrate prey, but mostly feed on insects or even carrion._\n",
    "(From [resource](https://en.wikipedia.org/wiki/Bird_of_prey))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the second passage is more relevant, even though “bird” appears once in the first passage and “birds” appears once in the second. As humans, we know that words like “eagles” and “ospreys” are similar to “bird,” while words like “stoats” and “rabbits” are not very similar to “bird.”"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the assignment, you will compare three word similarity measures to determine which agrees the most with human ratings of similarity. \n",
    "\n",
    "The three methods are:\n",
    "- [Lin similarity](https://www.cs.swarthmore.edu/~richardw/cs65-f08/litreview/phyo.pdf)\n",
    "- [Resnik similarity](https://arxiv.org/pdf/cmp-lg/9511007.pdf)\n",
    "- [vector-based similarity](http://www.aclweb.org/anthology/D14-1162)\n",
    "\n",
    "Lin similarity and Resnik similarity are both based on [WordNet](https://wordnet.princeton.edu/), a hand-built taxonomy of English words. \n",
    "\n",
    "The vector-based method represents words with [GloVe](http://nlp.stanford.edu/projects/glove/) or [Word2vec](https://code.google.com/archive/p/word2vec/) vectors that were automatically learned by a system that looked at a corpus of billions of words and measures how similar the words are by how close together their vectors are.\n",
    "\n",
    "You won’t have to implement these algorithms from scratch, or even understand how they work, for this assignment. \n",
    "\n",
    "The [NLTK](http://www.nltk.org) library already implements Resnick and Lin similarity, \n",
    "\n",
    "and the [Gensim](https://radimrehurek.com/gensim/) library works with vector-based models such as GloVe and Word2Vec. \n",
    "\n",
    "We have provided the skeleton code in `B.py`, but you will need to fill in several functions to make it work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B1: Parse the input file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implement `parseFile`. This function will be given the name of a file that contains a word pair and a similarity score (annotated by humans) on each line. \n",
    "\n",
    "To see an example of the format of the file, look at `input.txt`. \n",
    "\n",
    "Your function should read in the file and return a dictionary of `(word1, word2): score` entries. \n",
    "\n",
    "Make sure that you represent your scores as floats.\n",
    "\n",
    "Hint: You can check out more on [file I/O in Python](https://docs.python.org/2/tutorial/inputoutput.html#reading-and-writing-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFile(filename):\n",
    "    '''\n",
    "    \n",
    "    ---Parameters---\n",
    "    \n",
    "    filename (str): a filename where each line is in the format \"<word1>  <word2>  <human_score>\"\n",
    "\n",
    "    ---Returns---\n",
    "    \n",
    "    similarities (dict): a dictionary of {(word1, word2): human_score), ...}\n",
    "    \n",
    "    '''\n",
    "    similarities = OrderedDict()\n",
    "    #TODO: add your own code\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            token_list = word_tokenize(line)\n",
    "            similarities[(token_list[0], token_list[1])] = float(token_list[2])\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file of human similarity scores below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_sims = parseFile(\"input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human similarity scores are between 0 and 10 (exactly the same). Let's try to see the human similarity between \"tiget\" and \"cat\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.35"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_sims[('tiger', 'cat')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B2: Use WordNet to measure similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, implement `linSimilarities` and `resSimilarities` using NLTK. \n",
    "\n",
    "These functions are given a list of word pairs and a WordNet IC corpus (an information content file from the wordnet_ic corpus), and each should return a dictionary of `(word1, word2): score` entries. \n",
    "\n",
    "For this exercise, represent each word in the pair as the first noun in its WordNet synset. \n",
    "\n",
    "If no noun is found, represent it as the first verb in its WordNet synset.\n",
    "\n",
    "Note that the NLTK Lin scores are between 0 and 1; these will need to be scaled by a factor of 10 to be comparable to the human similarity scores.\n",
    "\n",
    "Hint: You can check out more on [NLTK’s similarity functions](http://www.nltk.org/howto/wordnet.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implement `linSimilarities`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linSimilarities(word_pairs, ic):\n",
    "    '''\n",
    "    Calculate Lin similarities between word pairs\n",
    "    \n",
    "    ---Parameters---\n",
    "    \n",
    "    word_pairs (a list of tuples): a list of tuples [(word1, word2), ...]\n",
    "    ic: an information content file from the wordnet_ic corpus\n",
    "\n",
    "    ---Returns---\n",
    "    \n",
    "    similarities (dict): a dictionary of Lin similarity scores {(word1, word2): lin_similarity_score, ...}\n",
    "    \n",
    "    '''\n",
    "    similarities = {}\n",
    "    # Fill in your code here\n",
    "    for pair in word_pairs:\n",
    "        synset1 = wn.synsets(pair[0], pos = wn.NOUN)\n",
    "        synset2 = wn.synsets(pair[1], pos = wn.NOUN)\n",
    "        if synset1 and synset2: # non-empty NOUN synsets\n",
    "            similarities[(pair[0], pair[1])] = 10 * synset1[0].lin_similarity(synset2[0], ic)\n",
    "            continue\n",
    "        # at leat one word of the pair has no noun synset\n",
    "        synset1 = wn.synsets(pair[0], pos = wn.VERB)\n",
    "        synset2 = wn.synsets(pair[1], pos = wn.VERB)\n",
    "        if synset1 and synset2: # non-empty VERB synsets\n",
    "            similarities[(pair[0], pair[1])] = 10 * synset1[0].lin_similarity(synset2[0], ic)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try `linSimilarities` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wenxinxu/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linsim is: dict_values([8.768009843733973, 5.312808366793076])\n"
     ]
    }
   ],
   "source": [
    "#get brown_ic from wordnet_ic corpus\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "sor = linSimilarities([('dog','cat'), ('dog', 'elephant')], brown_ic)\n",
    "print(f'linsim is: {sor.values()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, complete `resSimilarities`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resSimilarities(word_pairs, ic):\n",
    "    '''\n",
    "    Calculate Resnik similarities between word pairs\n",
    "    \n",
    "    ---Parameters---\n",
    "    \n",
    "    word_pairs (a list of tuples): a list of tuples [(word1, word2), ...]\n",
    "    ic: an information content file from the wordnet_ic corpus\n",
    "\n",
    "    ---Returns---\n",
    "    \n",
    "    similarities (dict): a dictionary of Lin similarity scores {(word1, word2): res_similarity_score, ...}\n",
    "    \n",
    "    '''\n",
    "    similarities = {}\n",
    "    # Fill in your code here\n",
    "    for pair in word_pairs:\n",
    "        synset1 = wn.synsets(pair[0], pos = wn.NOUN)\n",
    "        synset2 = wn.synsets(pair[1], pos = wn.NOUN)\n",
    "        if synset1 and synset2: # non-empty NOUN synsets\n",
    "            similarities[(pair[0], pair[1])] = synset1[0].res_similarity(synset2[0], ic)\n",
    "            continue\n",
    "        # at leat one word of the pair has no noun synset\n",
    "        synset1 = wn.synsets(pair[0], pos = wn.VERB)\n",
    "        synset2 = wn.synsets(pair[1], pos = wn.VERB)\n",
    "        if synset1 and synset2: # non-empty VERB synsets\n",
    "            similarities[(pair[0], pair[1])] = synset1[0].res_similarity(synset2[0], ic)\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B3: Use the vector-based similarity measure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector-based similarity measure represents words with vectors (word embeddings) so that similar words are close together in the space. \n",
    "\n",
    "Word embeddings become the most popular way to represent words especially when deep learning models are introduced to solve many NLP tasks (these models usually use word embeddings of words as inputs). \n",
    "\n",
    "We're going to implement a neural network using word embeddings in this class later.\n",
    "\n",
    "For this assignment, you only need to know how to use two pre-trained word embedding models [GloVe](http://nlp.stanford.edu/projects/glove/) or [Word2vec](https://code.google.com/archive/p/word2vec/). \n",
    "\n",
    "After you load these models, you can think them as dictionaries of `{word: [-0.04380177, 0.81042248, ..., -0.98958516]}` (see more real examples later). \n",
    "\n",
    "Once we have vectors for words, we can use Gensim to get the cosine similarity of corresponding words.\n",
    "\n",
    "[Mikolov et al. (2013a)](https://www.aclweb.org/anthology/N/N13/N13-1090.pdf) has the following visualization of a word embedding space:\n",
    "\n",
    "<img src=\"https://towardsml.files.wordpress.com/2018/06/capture2.png\" style='width: 50%' />\n",
    "\n",
    "You can see  not only these words of humans are close together, but also their relations are captured. \n",
    "\n",
    "For example, if you take the vector of \"woman\" minus the vector of \"man,\" the resulting vector is the same as `Vector(\"aunt\") − Vector(\"uncle\")`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Additional notes:___\n",
    "\n",
    "If you want to build word embedding models by youself. \n",
    "\n",
    "First, you should download [Word2vec](https://code.google.com/archive/p/word2vec/) or [GloVe](http://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "In NLP research, we would like to try different word embeddings, and select the one that gives us the best performance. \n",
    "\n",
    "Embeddings can differ in the dimension of their vectors as well as in the amount of data the vectors are trained on. \n",
    "\n",
    "The word embedding trained on larger datasets usually performs better. You can try out some other embeddings (e.g. glove.840B.300d) for curiosity. \n",
    "\n",
    "We provide the code to read these models (to read Glove into a dict, use `utils.glove2dict` from [here](https://github.com/cgpotts/cs224u/blob/master/utils.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the directory to Glove word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/wenxinxu/Desktop/LING780/homework/HW2/data/glove_embeddings.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will load a pre-trained vector-based model into memory. Notice it's going to take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load glove model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(filename, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try real word embedding examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23088  ,  0.28283  ,  0.6318   , -0.59411  , -0.58599  ,\n",
       "        0.63255  ,  0.24402  , -0.14108  ,  0.060815 , -0.7898   ,\n",
       "       -0.29102  ,  0.14287  ,  0.72274  ,  0.20428  ,  0.1407   ,\n",
       "        0.98757  ,  0.52533  ,  0.097456 ,  0.8822   ,  0.51221  ,\n",
       "        0.40204  ,  0.21169  , -0.013109 , -0.71616  ,  0.55387  ,\n",
       "        1.1452   , -0.88044  , -0.50216  , -0.22814  ,  0.023885 ,\n",
       "        0.1072   ,  0.083739 ,  0.55015  ,  0.58479  ,  0.75816  ,\n",
       "        0.45706  , -0.28001  ,  0.25225  ,  0.68965  , -0.60972  ,\n",
       "        0.19578  ,  0.044209 , -0.31136  , -0.68826  , -0.22721  ,\n",
       "        0.46185  , -0.77162  ,  0.10208  ,  0.55636  ,  0.067417 ,\n",
       "       -0.57207  ,  0.23735  ,  0.4717   ,  0.82765  , -0.29263  ,\n",
       "       -1.3422   , -0.099277 ,  0.28139  ,  0.41604  ,  0.10583  ,\n",
       "        0.62203  ,  0.89496  , -0.23446  ,  0.51349  ,  0.99379  ,\n",
       "        1.1846   , -0.16364  ,  0.20653  ,  0.73854  ,  0.24059  ,\n",
       "       -0.96473  ,  0.13481  , -0.0072484,  0.33016  , -0.12365  ,\n",
       "        0.27191  , -0.40951  ,  0.021909 , -0.6069   ,  0.40755  ,\n",
       "        0.19566  , -0.41802  ,  0.18636  , -0.032652 , -0.78571  ,\n",
       "       -0.13847  ,  0.044007 , -0.084423 ,  0.04911  ,  0.24104  ,\n",
       "        0.45273  , -0.18682  ,  0.46182  ,  0.089068 , -0.18185  ,\n",
       "       -0.01523  , -0.7368   , -0.14532  ,  0.15104  , -0.71493  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA visualization of Word2Vec\n",
    "\n",
    "`PCA` is a dimension reduction technique useful for visualizing high dimensional data in low dimensions (usually 2). The following code creates a scatterplot of some word2vec embeddings in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_matrix(random_words, model, dim=100):\n",
    "    word_matrix = np.random.randn(len(random_words), dim)\n",
    "    i = 0\n",
    "    for word in random_words:\n",
    "        word_matrix[i] = model[word]\n",
    "        i += 1\n",
    "    return word_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "random_words = [\"man\", \"woman\", \"uncle\", \"aunt\", \"king\", \"queen\"]\n",
    "return_matrix_ = return_matrix(random_words, model)\n",
    "pca_ = PCA(n_components=2)\n",
    "viz_data = pca_.fit_transform(return_matrix_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5)) \n",
    "for i in range(len(viz_data)):\n",
    "    plt.scatter(viz_data[i, 0], viz_data[i, 1])\n",
    "    plt.annotate(random_words[i],\n",
    "                 xy=(viz_data[i, 0], viz_data[i, 1]),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement `vecSimilarities`. \n",
    "\n",
    "The usual list of word pairs and the pre-trained model will be passed in to your function. Like the other functions, it should return a dictionary of `(word1, word2): score` entries. \n",
    "\n",
    "You can use `Gensim` to get the cosine similarity between the vector for word1 and the vector for word2; \n",
    "\n",
    "you’ll need to multiply by ten like you did with the WordNet functions.\n",
    "\n",
    "Hint: See more on [the model](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecSimilarities(word_pairs, model):\n",
    "    '''\n",
    "    Calculate cosine similarities between word pairs using word embeddings\n",
    "    \n",
    "    ---Parameters---\n",
    "    \n",
    "    word_pairs (a list of tuples): a list of tuples [(word1, word2), ...]\n",
    "    model (gensim model): a word2vec model (word embedding model)\n",
    "\n",
    "    ---Returns---\n",
    "    \n",
    "    similarities (dict): a dictionary of Lin similarity scores {(word1, word2): vec_similarity_score, ...}\n",
    "    \n",
    "    '''\n",
    "    similarities = {}\n",
    "    # Fill in your code here\n",
    "    for pair in word_pairs:\n",
    "        similarities[(pair[0], pair[1])] = 10 * model.similarity(pair[0].lower(), pair[1].lower())\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B4: Save your output and check your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get three similarity scores for each word pair in `human_sims` so that we can compare these scores with human judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_sims = linSimilarities(human_sims.keys(), brown_ic)\n",
    "res_sims = resSimilarities(human_sims.keys(), brown_ic)\n",
    "vec_sims = vecSimilarities(human_sims.keys(), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use the same technique you saw in Part A to save the output of part B to `output_B.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(lin_sims, res_sims, vec_sims):\n",
    "    '''\n",
    "    save the output\n",
    "    \n",
    "    ---Parameters---\n",
    "    \n",
    "    lin_sims, res_sims, vec_sims (dict): similarity scores for word pairs {(word1, word2): similarity_score}\n",
    "    \n",
    "    '''\n",
    "    lin_score = 0\n",
    "    res_score = 0\n",
    "    vec_score = 0\n",
    "\n",
    "    print(f\"{'word1':15} {'word2':15} {'human':10} {'Lin':20} {'Resnik':20} {'Word2Vec':20}\")\n",
    "\n",
    "    for key, human in human_sims.items():\n",
    "        try:\n",
    "            lin = lin_sims[key]\n",
    "        except:\n",
    "            lin = 0\n",
    "        lin_score += (lin - human) ** 2\n",
    "        try:\n",
    "            res = res_sims[key]\n",
    "        except:\n",
    "            res = 0\n",
    "        res_score += (res - human) ** 2\n",
    "        try:\n",
    "            vec = vec_sims[key]\n",
    "        except:\n",
    "            vec = 0\n",
    "        vec_score += (vec - human) ** 2\n",
    "        print(f\"{key[0]:15} {key[1]:15} {human:10} {lin:20} {res:20} {vec:20}\")\n",
    "\n",
    "    num_examples = len(human_sims)\n",
    "    print(\"\\nMean Squared Errors\")\n",
    "    print(f\"Lin method error: {lin_score/num_examples:.2f}\") \n",
    "    print(f\"Resnick method error: {res_score/num_examples:.2f}\")\n",
    "    print(f\"Vector-based method error: {vec_score/num_examples:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1           word2           human      Lin                  Resnik               Word2Vec            \n",
      "tiger           cat                   7.35   2.0875196749630205   2.2241504712318556    5.415419340133667\n",
      "tiger           tiger                 10.0                 10.0    12.26837533572617                 10.0\n",
      "book            paper                 7.46   1.2143570284702045   0.8017591149538994    6.225871443748474\n",
      "computer        keyboard              7.62    3.737908423475237    4.076481475394825    5.418189764022827\n",
      "computer        internet              7.58    3.301904295102813   3.2576790258866897      7.2867751121521\n",
      "plane           car                   5.77    7.194339072268571    5.921764790699729    5.961414575576782\n",
      "train           car                   6.31    6.786666052669672    5.768922519672357      6.8227219581604\n",
      "telephone       communication          7.5                 -0.0                 -0.0    6.329402923583984\n",
      "television      radio                 6.77    9.036510587018252    8.924336367903962    8.153581023216248\n",
      "media           radio                 7.42    8.322760066971366    7.020766639170194    6.588770747184753\n",
      "drug            abuse                 6.85                 -0.0                 -0.0    5.858407020568848\n",
      "bread           butter                6.19    7.114204901462941    6.879811091330354    7.279619574546814\n",
      "cucumber        potato                5.92 1.6035182299077988e-299   0.8017591149538994    6.297711730003357\n",
      "doctor          nurse                  7.0    8.351534747090344     7.93035864204873    7.521507740020752\n",
      "professor       doctor                6.62    7.036526610448273    6.394069019505728   5.3018271923065186\n",
      "student         professor             6.81   2.6208607023317687    2.333545243735693    5.851861834526062\n",
      "smart           student               4.62                 -0.0                 -0.0   3.5440054535865784\n",
      "smart           stupid                5.81                 -0.0                 -0.0    5.957468748092651\n",
      "company         stock                 7.08      0.6964952747566   0.5962292078977726    6.375110745429993\n",
      "stock           market                8.08   0.6563718869196877   0.5962292078977726    7.991610169410706\n",
      "stock           phone                 1.62                 -0.0                 -0.0   3.7602296471595764\n",
      "stock           cd                    1.31    2.783129418196296    3.036577932314865   1.9055187702178955\n",
      "stock           jaguar                0.92                 -0.0                 -0.0   1.6278056800365448\n",
      "stock           egg                   1.81                 -0.0                 -0.0   1.7744457721710205\n",
      "fertility       egg                   6.69                 -0.0                 -0.0   3.8555097579956055\n",
      "stock           live                  3.73                  0.0                    0   2.6148995757102966\n",
      "stock           life                  0.92   0.6903709313332212   0.5962292078977726    3.554474413394928\n",
      "book            library               7.46   2.6582077887402433    2.305848731451232    5.616436004638672\n",
      "bank            money                 8.12                 -0.0                 -0.0    5.718163251876831\n",
      "wood            forest                7.73   0.6812573196394262   0.5962292078977726    5.236123204231262\n",
      "money           cash                  9.08    7.888839126424346    6.254931881899411    8.484836220741272\n",
      "professor       cucumber              0.31 4.448300942463711e-299   2.2241504712318556 -0.06589939817786217\n",
      "king            cabbage               0.23   0.8665047534890641   0.8017591149538994  0.38354914635419846\n",
      "king            queen                 8.58   2.5872135992145147   2.2241504712318556    7.507690787315369\n",
      "king            rook                  5.92   1.6418595658458828   1.5318337432196856   2.1684736013412476\n",
      "bishop          rabbi                 6.69    6.655650900427844    6.961759166363437     4.40165787935257\n",
      "football        soccer                9.03    9.057530055264609   11.064402531400233    8.732221126556396\n",
      "football        basketball            6.81    7.536025025710654    8.716206927154134    8.555637001991272\n",
      "football        tennis                6.63    7.699955045932811    8.716206927154134    5.885030627250671\n",
      "tennis          racket                7.56    1.699674361940393    2.036327717785946    4.217623472213745\n",
      "law             lawyer                8.38                 -0.0                 -0.0    5.974500775337219\n",
      "movie           star                  7.38   1.8932685175348656   1.5318337432196856    6.531304717063904\n",
      "movie           popcorn               6.19 3.063667486439371e-299   1.5318337432196856   3.7365633249282837\n",
      "movie           critic                6.73   1.8027991154249121   1.5318337432196856    4.072280824184418\n",
      "movie           theater               7.92    2.933164649344553    2.305848731451232    6.693013310432434\n",
      "physics         proton                8.12                 -0.0                 -0.0   2.3452210426330566\n",
      "physics         chemistry             7.35     8.17907088214157    8.716206927154134    8.498000502586365\n",
      "space           chemistry             4.88   0.5953492550918607   0.5962292078977726   3.3954480290412903\n",
      "alcohol         chemistry             5.54   0.6223542714689646   0.5962292078977726   2.0205311477184296\n",
      "vodka           gin                   8.46 1.7712256235754857e-298    8.856128117877429    6.326446533203125\n",
      "vodka           brandy                8.13 1.7712256235754857e-298    8.856128117877429    6.743062138557434\n",
      "drink           car                   3.04                 -0.0                 -0.0    3.372037708759308\n",
      "drink           ear                   1.31                 -0.0                 -0.0   1.3571266829967499\n",
      "drink           mouth                 5.96                 -0.0                 -0.0   3.7458914518356323\n",
      "drink           eat                   6.87    5.786779236153949    4.721425374936297    6.849385499954224\n",
      "baby            mother                7.85    6.315913189894093    5.710492791428491      6.9692063331604\n",
      "drink           mother                2.65                 -0.0                 -0.0   3.3608391880989075\n",
      "car             automobile            8.94                 10.0    7.591401417609093    6.831942200660706\n",
      "gem             jewel                 8.96   2.1259210832519866    2.305848731451232    6.415449380874634\n",
      "journey         voyage                9.29   6.9691765730277115    6.180338778993439    7.682994604110718\n",
      "boy             lad                   8.83    6.147396351443222    5.332140585422163   3.9526766538619995\n",
      "coast           shore                  9.1    9.632173804623257    9.415743905812851    7.000271677970886\n",
      "asylum          madhouse              8.87    3.445877309961412    3.979366155201983  -1.0434501618146896\n",
      "magician        wizard                9.02   2.1590925682506263    2.333545243735693    6.618205308914185\n",
      "midday          noon                  9.29                 10.0   11.064402531400233    7.853313088417053\n",
      "furnace         stove                 8.79   2.2813808925013808    2.305848731451232    6.326393485069275\n",
      "food            fruit                 7.52   1.2795371229682395   0.8017591149538994     5.73560357093811\n",
      "bird            cock                   7.1   0.9387862411688587   0.8017591149538994   2.7531012892723083\n",
      "bird            crane                 7.38   2.1857259151379678   2.2241504712318556   3.3719339966773987\n",
      "tool            implement             6.46    9.471542795803442     5.87738923441087    3.897005319595337\n",
      "brother         monk                  6.27   2.4862817480738677    2.333545243735693   4.2725783586502075\n",
      "crane           implement             2.69   1.6514632935799916   1.5318337432196856  0.24721698835492134\n",
      "lad             brother               4.46   2.5285029514817237    2.333545243735693    2.299250215291977\n",
      "journey         car                   5.85                 -0.0                 -0.0    3.486618399620056\n",
      "monk            oracle                 5.0    2.256522165279408    2.333545243735693   0.5987599864602089\n",
      "cemetery        woodland              2.08   1.2344084378220546   1.2900256809649917    4.753442704677582\n",
      "food            rooster               4.42   0.9193186093161774   0.8017591149538994 -0.06615799851715565\n",
      "coast           hill                  4.38    5.991131628821826    5.884681030765606   3.8247379660606384\n",
      "forest          graveyard             1.85                 -0.0                 -0.0    2.361788898706436\n",
      "shore           woodland              3.08    1.355829969630242   1.2900256809649917   2.2715842723846436\n",
      "monk            slave                 0.92   2.5431082019443068    2.333545243735693   2.2086457908153534\n",
      "coast           forest                3.15                 -0.0                 -0.0     4.10929948091507\n",
      "lad             wizard                0.92   2.5509146617598177    2.333545243735693   3.3802786469459534\n",
      "chord           smile                 0.54    0.555059125278703   0.5962292078977726   2.0381097495555878\n",
      "glass           magician              2.08   0.7447842876832838   0.8017591149538994   0.9220508486032486\n",
      "noon            string                0.54                 -0.0                 -0.0   0.3757568821310997\n",
      "rooster         voyage                0.62                 -0.0                 -0.0   0.6048741191625595\n",
      "money           dollar                8.42    3.073659611034023   2.6911028053586055    5.416751503944397\n",
      "money           currency              9.04     8.63502335339357    6.254931881899411    5.285155177116394\n",
      "money           wealth                8.27   0.6804581462865835   0.5962292078977726    6.151007413864136\n",
      "money           property              7.57   0.7982153089692023   0.5962292078977726     5.86850643157959\n",
      "money           possession            7.29   0.7050442796520504   0.5962292078977726    3.829999268054962\n",
      "money           bank                   8.5                 -0.0                 -0.0    5.718163251876831\n",
      "money           deposit               7.73                 -0.0                 -0.0    4.879058301448822\n",
      "money           withdrawal            6.88   0.5756165687165145   0.5962292078977726    2.957793176174164\n",
      "money           laundering            5.65   0.5837226203433131   0.5962292078977726    5.056719183921814\n",
      "money           operation             3.31   0.7255643852471938   0.5962292078977726    4.614012241363525\n",
      "tiger           jaguar                 8.0    1.794533346820838   2.2241504712318556   3.4266507625579834\n",
      "tiger           feline                 8.0   2.1251344156256518   2.2241504712318556   2.1453896164894104\n",
      "tiger           carnivore             7.08    2.204307095437546   2.2241504712318556   1.0046851634979248\n",
      "tiger           mammal                6.85   2.5168293847148906   2.2241504712318556    2.806573212146759\n",
      "tiger           animal                 7.0    2.584060028178696   2.2241504712318556   3.7160155177116394\n",
      "tiger           organism              4.77   3.0693758988015962   2.2241504712318556   1.0725899785757065\n",
      "tiger           fauna                 5.62                 -0.0                 -0.0    2.304101884365082\n",
      "tiger           zoo                   5.87    1.253988195883625   1.5318337432196856    3.868136405944824\n",
      "psychology      psychiatry            8.08    6.552293883817466    7.186970970741705    7.578726410865784\n",
      "psychology      anxiety                7.0   0.5960579771514708   0.5962292078977726   3.5818296670913696\n",
      "psychology      fear                  6.85   0.6578656336760624   0.5962292078977726    1.891551911830902\n",
      "psychology      depression            7.42   0.5938251406479789   0.5962292078977726    4.102334976196289\n",
      "psychology      clinic                6.58   0.5269273527818584   0.5962292078977726    3.445098102092743\n",
      "psychology      doctor                6.42                 -0.0                 -0.0    4.399580955505371\n",
      "psychology      Freud                 8.21                 -0.0                 -0.0    4.194843769073486\n",
      "psychology      mind                  7.69     3.04017384194818    2.644520854456544     4.44429874420166\n",
      "psychology      health                7.23   0.6004979886905243   0.5962292078977726    4.052343666553497\n",
      "psychology      science               6.71    8.474590505736941    7.186970970741705    7.279347777366638\n",
      "psychology      discipline            5.58   7.2756179109236925    5.588776149881785    3.876716196537018\n",
      "psychology      cognition             7.48    4.258908474909009    2.644520854456544    5.789973735809326\n",
      "planet          star                  8.45    8.742320798564323    6.931906203213756    4.435915648937225\n",
      "planet          constellation         8.06                 -0.0                 -0.0    4.576427936553955\n",
      "planet          moon                  8.08    7.808882364067532    6.931906203213756   6.3455528020858765\n",
      "planet          sun                   8.02    8.104382016691204    6.931906203213756    4.861040413379669\n",
      "planet          galaxy                8.11                 -0.0                 -0.0    5.532509684562683\n",
      "planet          space                 7.92                 -0.0                 -0.0    5.872235298156738\n",
      "planet          astronomer            7.94    1.873903398441255   1.5318337432196856   3.0200809240341187\n",
      "precedent       example               5.85    8.170984525340074    8.077038507516757   4.5670488476753235\n",
      "precedent       information           3.85   0.6877223684176621   0.5962292078977726    2.791127562522888\n",
      "precedent       cognition             2.81   3.6889484830512274    2.644520854456544   0.7975126802921295\n",
      "precedent       law                   6.65   0.5942965159929445   0.5962292078977726    5.554394721984863\n",
      "precedent       collection             2.5   0.7121036028371418   0.5962292078977726    1.631639301776886\n",
      "precedent       group                 1.77   0.8196545411162961   0.5962292078977726   1.5662479400634766\n",
      "precedent       antecedent            6.04                 -0.0                 -0.0   2.8422537446022034\n",
      "cup             coffee                6.58   0.7832228882008228   0.8017591149538994    3.357512354850769\n",
      "cup             tableware             6.85    8.402414484216962    7.490185985606436  0.14358223415911198\n",
      "cup             article                2.4   2.5705853404735612    2.305848731451232   1.1609416455030441\n",
      "cup             artifact              2.92   3.6472499561999214    2.305848731451232  -0.6592267751693726\n",
      "cup             object                3.69   2.2187326933154097   1.2900256809649917   0.8495847135782242\n",
      "cup             entity                2.15                 -0.0                 -0.0   0.5027098953723907\n",
      "cup             drink                 7.25                 -0.0                 -0.0    3.228711187839508\n",
      "cup             food                   5.0   1.0267172015564763   0.8017591149538994    2.934669554233551\n",
      "cup             substance             1.92   1.1273072734249998   0.8017591149538994   2.0780546963214874\n",
      "cup             liquid                 5.9   0.9554901937757259   0.8017591149538994   2.8184425830841064\n",
      "jaguar          cat                   7.42     8.03649819480706    8.663481537685325    2.921023964881897\n",
      "jaguar          car                   7.27    1.523372082982704   1.5318337432196856   4.9060311913490295\n",
      "energy          secretary             1.81    1.021874084143099   0.8017591149538994   3.9209863543510437\n",
      "secretary       senate                5.06                 -0.0                 -0.0    5.345171093940735\n",
      "energy          laboratory            5.09   0.9342112258117007   0.8017591149538994     4.30083155632019\n",
      "computer        laboratory            6.78   1.3377959985558243   1.2900256809649917    5.172495245933533\n",
      "weapon          secret                6.06                 -0.0                 -0.0    4.351357817649841\n",
      "FBI             fingerprint           6.94   0.5133839883064533   0.5962292078977726    4.090163707733154\n",
      "FBI             investigation         8.31   0.5436899923349531   0.5962292078977726    7.432117462158203\n",
      "investigation   effort                4.59   1.9661809151931997   1.5927548636075812    5.606021285057068\n",
      "Mars            water                 2.94   0.8169920991314521   0.8017591149538994   3.3306771516799927\n",
      "Mars            scientist             5.63    1.762883740283928   1.5318337432196856   3.4445106983184814\n",
      "news            report                8.16    3.273229561285036     2.61964384137646    7.190849184989929\n",
      "canyon          landscape             7.53   1.1382009652319542   1.2900256809649917    2.997792661190033\n",
      "image           surface               4.56                 -0.0                 -0.0    4.650995433330536\n",
      "discovery       space                 6.34   0.7530187782625699   0.5962292078977726    6.015124320983887\n",
      "water           seepage               6.56   0.5602256351572972   0.5962292078977726    4.512885808944702\n",
      "sign            recess                2.38   0.5267988610636135   0.5962292078977726   2.3203830420970917\n",
      "Wednesday       news                  2.22   0.6584187826273009   0.5962292078977726    6.888728141784668\n",
      "mile            kilometer             8.66    5.776900866754902    6.060679964128936     8.51086676120758\n",
      "computer        news                  4.47                 -0.0                 -0.0    4.043859541416168\n",
      "territory       surface               5.34   2.5998822955933463   1.2900256809649917    3.484088182449341\n",
      "atmosphere      landscape             3.69                 -0.0                 -0.0    4.379683136940002\n",
      "president       medal                  3.0                 -0.0                 -0.0    2.752062678337097\n",
      "war             troops                8.13   0.7105335913960591   0.5962292078977726    6.857718825340271\n",
      "record          number                6.31   0.8448848860677438   0.5962292078977726    5.872029066085815\n",
      "skin            eye                   6.22    4.902355662409748    4.402249455139793    6.734592318534851\n",
      "Japanese        American               6.5    6.928947328098976     6.10342444391276    5.854301452636719\n",
      "theater         history               3.91                 -0.0                 -0.0    3.983558714389801\n",
      "volunteer       motto                 2.56                 -0.0                 -0.0    3.290877044200897\n",
      "prejudice       recognition            3.0   0.5510899241582469   0.5962292078977726   2.5217238068580627\n",
      "decoration      valor                 5.63                 -0.0                 -0.0    4.925723969936371\n",
      "century         year                  7.59    4.841384158027682    3.800372388500703    4.688478708267212\n",
      "century         nation                3.16   0.7718770112725837   0.5962292078977726    4.707085490226746\n",
      "delay           racism                1.19 1.1924584157955451e-299   0.5962292078977726    1.917770802974701\n",
      "delay           news                  3.31   0.7200864183281619   0.5962292078977726   3.6830151081085205\n",
      "minister        party                 6.63                 -0.0                 -0.0    5.636986494064331\n",
      "peace           plan                  4.75   0.7604322709442058   0.5962292078977726   5.6073033809661865\n",
      "minority        peace                 3.69   0.5744988757171025   0.5962292078977726    3.564161956310272\n",
      "attempt         peace                 4.25    0.767538352473955   0.5962292078977726    4.545162916183472\n",
      "government      crisis                6.56   0.7435265673500501   0.5962292078977726    5.995239615440369\n",
      "deployment      departure             4.25   2.1155271370973843   2.3322377550841122    4.212912023067474\n",
      "deployment      withdrawal            5.88   1.7187073310385637   2.3322377550841122   6.8989890813827515\n",
      "energy          crisis                5.94                 -0.0                 -0.0    4.851409494876862\n",
      "announcement    news                  7.56    4.200520026828896   3.8478154212435802    6.127035617828369\n",
      "announcement    effort                2.75   0.7272141646065312   0.5962292078977726    5.028148293495178\n",
      "stroke          hospital              7.03                 -0.0                 -0.0    4.357018768787384\n",
      "disability      death                 5.47   0.6458212291145071   0.5962292078977726   3.1452009081840515\n",
      "victim          emergency             6.47                 -0.0                 -0.0   3.1829962134361267\n",
      "treatment       recovery              7.91   2.1564370436353024    2.036327717785946    4.170629978179932\n",
      "journal         association           4.97    0.698350146036528   0.5962292078977726     4.60332065820694\n",
      "doctor          personnel              5.0                 -0.0                 -0.0   3.0401092767715454\n",
      "doctor          liability             5.19                 -0.0                 -0.0    1.629389077425003\n",
      "liability       insurance             7.03     0.51993526566917   0.5962292078977726    6.688823699951172\n",
      "school          center                3.44                 -0.0                 -0.0    5.876184105873108\n",
      "reason          hypertension          2.31    0.575886026408655   0.5962292078977726   1.3257698714733124\n",
      "reason          criterion             5.91   0.8866038055590697   0.5962292078977726   2.7498450875282288\n",
      "\n",
      "Mean Squared Errors\n",
      "Lin method error: 20.87\n",
      "Resnick method error: 20.74\n",
      "Vector-based method error: 6.02\n"
     ]
    }
   ],
   "source": [
    "save_output(lin_sims, res_sims, vec_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided `correct_output_B.txt`, so you can diff your output against ours."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Comparing Numpy with Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [Pytorch](https://pytorch.org/) in most of our assignments in this class. We suppose that you are already familar with Numpy. \n",
    "\n",
    "In this section, we implement two same simple networks in both Numpy and Pytorch so that we can get some senses of differences between them. \n",
    "\n",
    "(Check out more details in [the Pytorch official tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#id17))\n",
    "\n",
    "At its core, PyTorch provides two main features:\n",
    "* An n-dimensional Tensor, similar to numpy but can run on GPUs\n",
    "* Automatic differentiation for building and training neural networks\n",
    "\n",
    "We will use a fully-connected ReLU network as our running example. The network will have a single hidden layer, and will be trained with gradient descent to fit random data by minimizing the Euclidean distance between the network output and the true output.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. \n",
    "\n",
    "Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. \n",
    "\n",
    "However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 40304320.61227014\n",
      "1 40682001.99983903\n",
      "2 41237867.99419424\n",
      "3 34633254.912385136\n",
      "4 22193918.51957117\n",
      "5 11261284.588764021\n",
      "6 5318666.864524838\n",
      "7 2796539.026639132\n",
      "8 1767432.5454492131\n",
      "9 1291894.4853344164\n",
      "10 1024197.9215817375\n",
      "11 844904.3879154057\n",
      "12 711210.7076259758\n",
      "13 605717.1463428626\n",
      "14 520233.143966499\n",
      "15 449752.55257332907\n",
      "16 391098.17322820507\n",
      "17 341887.3898103385\n",
      "18 300241.117432\n",
      "19 264739.06086471357\n",
      "20 234314.88671021943\n",
      "21 208108.4707194612\n",
      "22 185449.45268251107\n",
      "23 165773.76576464967\n",
      "24 148595.82190445697\n",
      "25 133546.4246515673\n",
      "26 120314.99414437514\n",
      "27 108637.35865919563\n",
      "28 98307.99037776611\n",
      "29 89140.38645864965\n",
      "30 80988.40418564514\n",
      "31 73721.11429153878\n",
      "32 67215.50417569275\n",
      "33 61384.01000138554\n",
      "34 56138.82519783947\n",
      "35 51417.29005355944\n",
      "36 47156.43305451682\n",
      "37 43301.628606787366\n",
      "38 39813.75611435356\n",
      "39 36649.37159340152\n",
      "40 33774.05879953873\n",
      "41 31158.13428106346\n",
      "42 28774.269745238176\n",
      "43 26599.88791512246\n",
      "44 24614.824519347425\n",
      "45 22797.277223107438\n",
      "46 21131.2510037753\n",
      "47 19602.006882669404\n",
      "48 18196.492142900213\n",
      "49 16903.53993994939\n",
      "50 15713.727502166515\n",
      "51 14617.272870300258\n",
      "52 13606.497917387707\n",
      "53 12673.023922544759\n",
      "54 11810.590170595155\n",
      "55 11012.826859594756\n",
      "56 10274.317056317881\n",
      "57 9590.57882319388\n",
      "58 8956.461765632266\n",
      "59 8368.19355261928\n",
      "60 7822.500007738011\n",
      "61 7315.21171182821\n",
      "62 6843.74842472132\n",
      "63 6405.411822067766\n",
      "64 5997.631140929568\n",
      "65 5617.95599375892\n",
      "66 5264.203573281387\n",
      "67 4934.434934959185\n",
      "68 4626.909425319711\n",
      "69 4340.242378676428\n",
      "70 4072.593529673657\n",
      "71 3822.835359970722\n",
      "72 3589.5400766459948\n",
      "73 3371.5471346385893\n",
      "74 3167.694230637886\n",
      "75 2977.012255142297\n",
      "76 2798.6321905669424\n",
      "77 2631.6374825025214\n",
      "78 2475.274130777313\n",
      "79 2328.8003785172386\n",
      "80 2191.517088014698\n",
      "81 2062.8625871377294\n",
      "82 1942.1870831183785\n",
      "83 1828.9932181827648\n",
      "84 1722.812430124939\n",
      "85 1623.1974309587104\n",
      "86 1529.6269070171636\n",
      "87 1441.7907598870183\n",
      "88 1359.2483158314415\n",
      "89 1281.711975038278\n",
      "90 1208.8399957196204\n",
      "91 1140.3409220671426\n",
      "92 1075.9207556250615\n",
      "93 1015.364390470131\n",
      "94 958.6009646161915\n",
      "95 905.1809256297081\n",
      "96 854.9146157170732\n",
      "97 807.5785349501886\n",
      "98 763.0101470046529\n",
      "99 721.027991173085\n",
      "100 681.4743856066405\n",
      "101 644.2021909757011\n",
      "102 609.0800505739896\n",
      "103 575.9682333248181\n",
      "104 544.7444977814882\n",
      "105 515.2914552351724\n",
      "106 487.5182807813916\n",
      "107 461.32291972309065\n",
      "108 436.5998839751178\n",
      "109 413.2633070923956\n",
      "110 391.24316485480654\n",
      "111 370.4489859182627\n",
      "112 350.80259501040956\n",
      "113 332.2489317078158\n",
      "114 314.7239449812488\n",
      "115 298.15785335419065\n",
      "116 282.5016630436005\n",
      "117 267.70564912025065\n",
      "118 253.71686122287218\n",
      "119 240.4916960227679\n",
      "120 227.98378203789002\n",
      "121 216.1558948750063\n",
      "122 204.9695543933073\n",
      "123 194.38145526283623\n",
      "124 184.36076110299655\n",
      "125 174.8762115987424\n",
      "126 165.9002127447957\n",
      "127 157.4054470950272\n",
      "128 149.36171670135226\n",
      "129 141.7435987017766\n",
      "130 134.52800667842945\n",
      "131 127.6950381619634\n",
      "132 121.22149588000985\n",
      "133 115.0884653659732\n",
      "134 109.27746591164727\n",
      "135 103.77174409280406\n",
      "136 98.55318307810815\n",
      "137 93.605192912934\n",
      "138 88.91474117278071\n",
      "139 84.46769571455495\n",
      "140 80.25037720843567\n",
      "141 76.25027768223032\n",
      "142 72.45559914315992\n",
      "143 68.85691307580288\n",
      "144 65.44329186226537\n",
      "145 62.20413725098534\n",
      "146 59.130398571945776\n",
      "147 56.213350753886026\n",
      "148 53.44508649800656\n",
      "149 50.81682694982056\n",
      "150 48.321739950773036\n",
      "151 45.953293852948\n",
      "152 43.70450395482224\n",
      "153 41.56891469111352\n",
      "154 39.54090999647417\n",
      "155 37.614417205865664\n",
      "156 35.78490524140979\n",
      "157 34.04665814795898\n",
      "158 32.3952090466568\n",
      "159 30.826016574281326\n",
      "160 29.335351647986236\n",
      "161 27.91869866711582\n",
      "162 26.572778405589034\n",
      "163 25.292953760622837\n",
      "164 24.07645234180783\n",
      "165 22.920123310213413\n",
      "166 21.820534108027253\n",
      "167 20.775048917327076\n",
      "168 19.78128242214924\n",
      "169 18.83623556760329\n",
      "170 17.937407021239753\n",
      "171 17.082682323108852\n",
      "172 16.269522082258145\n",
      "173 15.496190225215337\n",
      "174 14.76041712782209\n",
      "175 14.060314514618856\n",
      "176 13.394179274563587\n",
      "177 12.760552147531163\n",
      "178 12.157560197835645\n",
      "179 11.583765127197154\n",
      "180 11.037521179118414\n",
      "181 10.517729242704949\n",
      "182 10.02291662447447\n",
      "183 9.551840908338537\n",
      "184 9.103553778724809\n",
      "185 8.676802136669123\n",
      "186 8.270383583437294\n",
      "187 7.883352620726534\n",
      "188 7.514913567668518\n",
      "189 7.1640353288835925\n",
      "190 6.829934547745504\n",
      "191 6.51169637468308\n",
      "192 6.208535900580964\n",
      "193 5.919889976049866\n",
      "194 5.644951028193341\n",
      "195 5.382927220144636\n",
      "196 5.133368004290125\n",
      "197 4.895599303331757\n",
      "198 4.669021157712251\n",
      "199 4.453154604512539\n",
      "200 4.247448297108967\n",
      "201 4.051427399622973\n",
      "202 3.864639748866741\n",
      "203 3.6865939709106286\n",
      "204 3.5169024460883493\n",
      "205 3.355167864653334\n",
      "206 3.201035071938678\n",
      "207 3.054103589221433\n",
      "208 2.9140343531341646\n",
      "209 2.7805000264725304\n",
      "210 2.6531951835223433\n",
      "211 2.531809318207929\n",
      "212 2.4160653092671165\n",
      "213 2.305703962017816\n",
      "214 2.2004842951508525\n",
      "215 2.100176380170455\n",
      "216 2.0044740747285545\n",
      "217 1.9131964444968492\n",
      "218 1.8261637990050772\n",
      "219 1.7431758322570319\n",
      "220 1.6639965918508124\n",
      "221 1.588470542195603\n",
      "222 1.516416192809729\n",
      "223 1.4477000744883646\n",
      "224 1.3821597578506326\n",
      "225 1.3196169037980305\n",
      "226 1.2599551070746682\n",
      "227 1.2030213592904562\n",
      "228 1.1486959116352184\n",
      "229 1.0968660599416529\n",
      "230 1.0474059901704549\n",
      "231 1.0002122160225686\n",
      "232 0.9551795989393813\n",
      "233 0.9122043920688401\n",
      "234 0.8711877845398119\n",
      "235 0.8320374295181352\n",
      "236 0.7946705696979783\n",
      "237 0.7590152251094072\n",
      "238 0.7249769077644959\n",
      "239 0.6924839235563047\n",
      "240 0.6614688847427899\n",
      "241 0.63186113727732\n",
      "242 0.6036058673432507\n",
      "243 0.5766285847654007\n",
      "244 0.5508645771090388\n",
      "245 0.5262720839174586\n",
      "246 0.5027890864870175\n",
      "247 0.48036712479440447\n",
      "248 0.4589630573705207\n",
      "249 0.4385179187827191\n",
      "250 0.4190001269331933\n",
      "251 0.4003627982005084\n",
      "252 0.38256587522349556\n",
      "253 0.36556761294487883\n",
      "254 0.34933270106102765\n",
      "255 0.3338275811387319\n",
      "256 0.31902178448482127\n",
      "257 0.30487595059594863\n",
      "258 0.2913656498224263\n",
      "259 0.2784663418233834\n",
      "260 0.2661416708836388\n",
      "261 0.2543702748743998\n",
      "262 0.2431219360691337\n",
      "263 0.2323796490652892\n",
      "264 0.22212030247874837\n",
      "265 0.21231560424090867\n",
      "266 0.20294811194393012\n",
      "267 0.19399963087769684\n",
      "268 0.18545046399871026\n",
      "269 0.1772816079905991\n",
      "270 0.16947697093409803\n",
      "271 0.16201817221471157\n",
      "272 0.15489438356698232\n",
      "273 0.14808368417903134\n",
      "274 0.1415771647254117\n",
      "275 0.1353602187372288\n",
      "276 0.12941923382870124\n",
      "277 0.1237404771114007\n",
      "278 0.11831370968541241\n",
      "279 0.1131261255388108\n",
      "280 0.10817269903422208\n",
      "281 0.10343469515020387\n",
      "282 0.09890642188033488\n",
      "283 0.09457895977399486\n",
      "284 0.09044369934715613\n",
      "285 0.08649058816634678\n",
      "286 0.08271159522189586\n",
      "287 0.07909913444650934\n",
      "288 0.07564710267594493\n",
      "289 0.07234579588169676\n",
      "290 0.06919064544461985\n",
      "291 0.066174296225287\n",
      "292 0.06329093690094091\n",
      "293 0.06053387445880283\n",
      "294 0.057898020852144155\n",
      "295 0.05537826340772746\n",
      "296 0.05296994739419289\n",
      "297 0.05066613278904658\n",
      "298 0.04846320772490551\n",
      "299 0.04635775575632589\n",
      "300 0.0443450907418938\n",
      "301 0.04241964779723562\n",
      "302 0.04057862112472198\n",
      "303 0.038817994501518086\n",
      "304 0.03713512253108206\n",
      "305 0.03552560727027934\n",
      "306 0.03398612542403484\n",
      "307 0.032514226741145166\n",
      "308 0.031106584590513554\n",
      "309 0.02976049326134476\n",
      "310 0.028473421108606036\n",
      "311 0.027241871022779578\n",
      "312 0.02606435853454075\n",
      "313 0.02493783470717402\n",
      "314 0.02386035103771901\n",
      "315 0.022830347396253448\n",
      "316 0.021844943646268704\n",
      "317 0.020902099126156558\n",
      "318 0.020000537204289983\n",
      "319 0.01913815491440023\n",
      "320 0.018313419684204753\n",
      "321 0.017524184594908145\n",
      "322 0.016769174343842685\n",
      "323 0.01604734636228404\n",
      "324 0.015356525887467404\n",
      "325 0.01469573024224356\n",
      "326 0.01406372776426999\n",
      "327 0.0134590140311114\n",
      "328 0.012880401359952968\n",
      "329 0.012326914674744352\n",
      "330 0.011797402480430229\n",
      "331 0.011290892188339018\n",
      "332 0.010806151546574978\n",
      "333 0.010342315849542764\n",
      "334 0.009898786658299172\n",
      "335 0.009474216691893212\n",
      "336 0.009068014346134197\n",
      "337 0.008679333480023626\n",
      "338 0.00830745311646738\n",
      "339 0.007951698319273061\n",
      "340 0.007611153955800597\n",
      "341 0.00728530929136837\n",
      "342 0.006973627705310554\n",
      "343 0.006675232103779906\n",
      "344 0.006389761703236844\n",
      "345 0.006116581795466073\n",
      "346 0.005855141210155405\n",
      "347 0.005604966087727899\n",
      "348 0.005365529330967539\n",
      "349 0.005136398897321391\n",
      "350 0.004917141968530745\n",
      "351 0.004707236441654995\n",
      "352 0.004506421626548841\n",
      "353 0.004314233142463346\n",
      "354 0.0041302129099963615\n",
      "355 0.003954155150353411\n",
      "356 0.003785669412359081\n",
      "357 0.003624368655059425\n",
      "358 0.0034699943961925025\n",
      "359 0.003322205352313099\n",
      "360 0.0031807729446185432\n",
      "361 0.003045413052228217\n",
      "362 0.002915816942770282\n",
      "363 0.002791814227418526\n",
      "364 0.002673064137084325\n",
      "365 0.0025594117900249362\n",
      "366 0.002450647393772919\n",
      "367 0.002346499105033586\n",
      "368 0.0022468146243537004\n",
      "369 0.002151382143528628\n",
      "370 0.0020600368753274255\n",
      "371 0.001972606470329056\n",
      "372 0.0018888846095259668\n",
      "373 0.0018087336185578181\n",
      "374 0.0017320169635852598\n",
      "375 0.0016585559353209916\n",
      "376 0.0015882587910934962\n",
      "377 0.00152093649832354\n",
      "378 0.0014564700905046585\n",
      "379 0.0013947746082872646\n",
      "380 0.0013357098395655137\n",
      "381 0.0012791508896825776\n",
      "382 0.0012249935725148595\n",
      "383 0.0011731406976547337\n",
      "384 0.0011235128069977196\n",
      "385 0.001075979748023058\n",
      "386 0.0010304610903777882\n",
      "387 0.0009868968650183474\n",
      "388 0.0009451672561118998\n",
      "389 0.0009052203044903771\n",
      "390 0.0008669699624246005\n",
      "391 0.0008303373804505927\n",
      "392 0.000795267995369567\n",
      "393 0.0007616805622433468\n",
      "394 0.0007295318415353533\n",
      "395 0.0006987387037921647\n",
      "396 0.0006692437534165816\n",
      "397 0.0006410131718141026\n",
      "398 0.0006139745151561391\n",
      "399 0.0005880746179533012\n",
      "400 0.0005632804674730317\n",
      "401 0.0005395352480229925\n",
      "402 0.0005167962983812831\n",
      "403 0.0004950177722133669\n",
      "404 0.00047416375907067274\n",
      "405 0.0004542012206715041\n",
      "406 0.00043507049748897777\n",
      "407 0.0004167523456117115\n",
      "408 0.0003992115784054811\n",
      "409 0.00038240441264456254\n",
      "410 0.00036631639933515305\n",
      "411 0.0003509064431914727\n",
      "412 0.0003361457649099311\n",
      "413 0.0003220111158991714\n",
      "414 0.00030847081312638754\n",
      "415 0.00029550765315363976\n",
      "416 0.00028308601279743194\n",
      "417 0.00027118888489410483\n",
      "418 0.0002597972790893924\n",
      "419 0.0002488837638786534\n",
      "420 0.00023843153060116085\n",
      "421 0.00022842058529400495\n",
      "422 0.00021883025273314202\n",
      "423 0.0002096481113423412\n",
      "424 0.00020084989415452314\n",
      "425 0.00019242125596638447\n",
      "426 0.00018435068654985983\n",
      "427 0.00017661730320169027\n",
      "428 0.00016921272081710752\n",
      "429 0.00016211662361796108\n",
      "430 0.0001553221033390745\n",
      "431 0.0001488139508383606\n",
      "432 0.0001425765514312482\n",
      "433 0.0001366048747658132\n",
      "434 0.000130882020607251\n",
      "435 0.00012539894488922078\n",
      "436 0.00012014883274617406\n",
      "437 0.00011511841578915101\n",
      "438 0.0001102994442098885\n",
      "439 0.00010568212770174242\n",
      "440 0.00010126084226278357\n",
      "441 9.702484180349255e-05\n",
      "442 9.296464916852582e-05\n",
      "443 8.907683722740549e-05\n",
      "444 8.535181641686004e-05\n",
      "445 8.178242324880483e-05\n",
      "446 7.836395985343154e-05\n",
      "447 7.508806175590852e-05\n",
      "448 7.195026818744969e-05\n",
      "449 6.894422455062891e-05\n",
      "450 6.606393527057322e-05\n",
      "451 6.330475469487204e-05\n",
      "452 6.0659904326198516e-05\n",
      "453 5.8127458483336346e-05\n",
      "454 5.570051750262321e-05\n",
      "455 5.33747160218545e-05\n",
      "456 5.114804223442141e-05\n",
      "457 4.901331219513026e-05\n",
      "458 4.6968176766569476e-05\n",
      "459 4.500892641833364e-05\n",
      "460 4.313097440230956e-05\n",
      "461 4.1332783017163536e-05\n",
      "462 3.9608832754702656e-05\n",
      "463 3.7957242085498094e-05\n",
      "464 3.6374784479241984e-05\n",
      "465 3.485848964913507e-05\n",
      "466 3.340619665988941e-05\n",
      "467 3.201371837686435e-05\n",
      "468 3.0680086698036816e-05\n",
      "469 2.940202386011401e-05\n",
      "470 2.8177099415640645e-05\n",
      "471 2.7004010605040488e-05\n",
      "472 2.587931160911019e-05\n",
      "473 2.4801842651176072e-05\n",
      "474 2.3769490288298952e-05\n",
      "475 2.2780101074063228e-05\n",
      "476 2.1832136248981522e-05\n",
      "477 2.0923451377887154e-05\n",
      "478 2.0053224089939265e-05\n",
      "479 1.921888795166269e-05\n",
      "480 1.8419281828206843e-05\n",
      "481 1.765354477376735e-05\n",
      "482 1.6919527389557537e-05\n",
      "483 1.621611598121242e-05\n",
      "484 1.5541978537166365e-05\n",
      "485 1.4895858143975326e-05\n",
      "486 1.4276905716930563e-05\n",
      "487 1.3683489755044802e-05\n",
      "488 1.3114987339099276e-05\n",
      "489 1.257004104391036e-05\n",
      "490 1.2047951997122344e-05\n",
      "491 1.1547594960796148e-05\n",
      "492 1.1067904352518945e-05\n",
      "493 1.0608456143939451e-05\n",
      "494 1.0167982607065419e-05\n",
      "495 9.745874757508014e-06\n",
      "496 9.341382630926632e-06\n",
      "497 8.953540660800148e-06\n",
      "498 8.582107407198361e-06\n",
      "499 8.225994018089206e-06\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. \n",
    "\n",
    "For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning. \n",
    "\n",
    "However, we aren't using GPUs on the Zoo machines.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. \n",
    "\n",
    "A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. \n",
    "\n",
    "Behind the scenes, Tensors can keep track of a computational graph and gradients, but they’re also useful as a generic tool for scientific computing.\n",
    "\n",
    "See [this page](https://pytorch.org/get-started/locally/) if you are using a different system (e.g. OSX or conda). \n",
    "\n",
    "Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. \n",
    "\n",
    "To run a PyTorch Tensor on GPU, you simply need to cast it to a new datatype.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. \n",
    "\n",
    "Like the numpy example above we need to manually implement the forward and backward passes through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24656736.0\n",
      "1 18215596.0\n",
      "2 15386913.0\n",
      "3 13655857.0\n",
      "4 12087073.0\n",
      "5 10321528.0\n",
      "6 8415059.0\n",
      "7 6534388.0\n",
      "8 4890714.0\n",
      "9 3563092.5\n",
      "10 2568393.5\n",
      "11 1850454.375\n",
      "12 1347404.625\n",
      "13 997375.0625\n",
      "14 754263.75\n",
      "15 583579.6875\n",
      "16 461999.25\n",
      "17 373653.125\n",
      "18 308086.03125\n",
      "19 258221.046875\n",
      "20 219458.78125\n",
      "21 188672.828125\n",
      "22 163777.34375\n",
      "23 143301.34375\n",
      "24 126224.7734375\n",
      "25 111778.4765625\n",
      "26 99424.6796875\n",
      "27 88783.3203125\n",
      "28 79565.59375\n",
      "29 71523.1953125\n",
      "30 64455.47265625\n",
      "31 58216.109375\n",
      "32 52687.1171875\n",
      "33 47772.6875\n",
      "34 43387.27734375\n",
      "35 39462.6640625\n",
      "36 35943.23828125\n",
      "37 32780.37109375\n",
      "38 29933.357421875\n",
      "39 27363.826171875\n",
      "40 25042.15234375\n",
      "41 22942.154296875\n",
      "42 21038.5859375\n",
      "43 19311.080078125\n",
      "44 17743.193359375\n",
      "45 16317.5615234375\n",
      "46 15018.98828125\n",
      "47 13834.626953125\n",
      "48 12753.7080078125\n",
      "49 11767.9677734375\n",
      "50 10866.541015625\n",
      "51 10041.2529296875\n",
      "52 9285.3125\n",
      "53 8592.015625\n",
      "54 7955.63671875\n",
      "55 7371.55517578125\n",
      "56 6834.81689453125\n",
      "57 6340.94287109375\n",
      "58 5886.1435546875\n",
      "59 5467.16015625\n",
      "60 5080.96484375\n",
      "61 4724.4609375\n",
      "62 4395.52880859375\n",
      "63 4091.57421875\n",
      "64 3810.6845703125\n",
      "65 3550.72998046875\n",
      "66 3310.31201171875\n",
      "67 3087.601806640625\n",
      "68 2881.25390625\n",
      "69 2689.848876953125\n",
      "70 2512.24169921875\n",
      "71 2347.374267578125\n",
      "72 2194.318359375\n",
      "73 2052.108154296875\n",
      "74 1919.8272705078125\n",
      "75 1796.916748046875\n",
      "76 1682.404296875\n",
      "77 1575.845947265625\n",
      "78 1476.5472412109375\n",
      "79 1384.0040283203125\n",
      "80 1297.726806640625\n",
      "81 1217.254150390625\n",
      "82 1142.19873046875\n",
      "83 1072.10400390625\n",
      "84 1006.650634765625\n",
      "85 945.4902954101562\n",
      "86 888.337158203125\n",
      "87 834.9282836914062\n",
      "88 784.9830932617188\n",
      "89 738.3034057617188\n",
      "90 694.604736328125\n",
      "91 653.67529296875\n",
      "92 615.3345336914062\n",
      "93 579.4053344726562\n",
      "94 545.7149047851562\n",
      "95 514.119140625\n",
      "96 484.47796630859375\n",
      "97 456.65814208984375\n",
      "98 430.56707763671875\n",
      "99 406.07525634765625\n",
      "100 383.0690612792969\n",
      "101 361.4417419433594\n",
      "102 341.1153564453125\n",
      "103 322.00390625\n",
      "104 304.03448486328125\n",
      "105 287.1295166015625\n",
      "106 271.2213134765625\n",
      "107 256.25018310546875\n",
      "108 242.15414428710938\n",
      "109 228.8748779296875\n",
      "110 216.37271118164062\n",
      "111 204.58438110351562\n",
      "112 193.4744415283203\n",
      "113 183.00209045410156\n",
      "114 173.12692260742188\n",
      "115 163.81324768066406\n",
      "116 155.03143310546875\n",
      "117 146.74002075195312\n",
      "118 138.9169158935547\n",
      "119 131.52993774414062\n",
      "120 124.55671691894531\n",
      "121 117.9724349975586\n",
      "122 111.75221252441406\n",
      "123 105.87294006347656\n",
      "124 100.32062530517578\n",
      "125 95.07457733154297\n",
      "126 90.11446380615234\n",
      "127 85.42752075195312\n",
      "128 80.99148559570312\n",
      "129 76.79696655273438\n",
      "130 72.82945251464844\n",
      "131 69.07666015625\n",
      "132 65.52423095703125\n",
      "133 62.1609992980957\n",
      "134 58.978477478027344\n",
      "135 55.967124938964844\n",
      "136 53.11257553100586\n",
      "137 50.41123580932617\n",
      "138 47.85298538208008\n",
      "139 45.42985916137695\n",
      "140 43.133575439453125\n",
      "141 40.95691680908203\n",
      "142 38.8951530456543\n",
      "143 36.941017150878906\n",
      "144 35.08890914916992\n",
      "145 33.33320617675781\n",
      "146 31.667919158935547\n",
      "147 30.08876609802246\n",
      "148 28.590469360351562\n",
      "149 27.170330047607422\n",
      "150 25.8222599029541\n",
      "151 24.54278564453125\n",
      "152 23.32939338684082\n",
      "153 22.177936553955078\n",
      "154 21.084575653076172\n",
      "155 20.04666519165039\n",
      "156 19.06245231628418\n",
      "157 18.127056121826172\n",
      "158 17.23869514465332\n",
      "159 16.395578384399414\n",
      "160 15.595306396484375\n",
      "161 14.834793090820312\n",
      "162 14.112105369567871\n",
      "163 13.426457405090332\n",
      "164 12.774239540100098\n",
      "165 12.155014991760254\n",
      "166 11.56576919555664\n",
      "167 11.0067777633667\n",
      "168 10.474717140197754\n",
      "169 9.969365119934082\n",
      "170 9.488666534423828\n",
      "171 9.032201766967773\n",
      "172 8.598074913024902\n",
      "173 8.184978485107422\n",
      "174 7.7924394607543945\n",
      "175 7.419320106506348\n",
      "176 7.064165115356445\n",
      "177 6.726452827453613\n",
      "178 6.405546188354492\n",
      "179 6.099803924560547\n",
      "180 5.8094611167907715\n",
      "181 5.532795429229736\n",
      "182 5.269975185394287\n",
      "183 5.0196356773376465\n",
      "184 4.781554222106934\n",
      "185 4.554649829864502\n",
      "186 4.339118957519531\n",
      "187 4.133727073669434\n",
      "188 3.938551902770996\n",
      "189 3.752390146255493\n",
      "190 3.5754289627075195\n",
      "191 3.4069182872772217\n",
      "192 3.246748447418213\n",
      "193 3.0936105251312256\n",
      "194 2.948367118835449\n",
      "195 2.810016632080078\n",
      "196 2.6780924797058105\n",
      "197 2.552509069442749\n",
      "198 2.432870388031006\n",
      "199 2.319122314453125\n",
      "200 2.210691452026367\n",
      "201 2.107403516769409\n",
      "202 2.0090105533599854\n",
      "203 1.9153356552124023\n",
      "204 1.8260974884033203\n",
      "205 1.740975022315979\n",
      "206 1.659881353378296\n",
      "207 1.582589864730835\n",
      "208 1.509082317352295\n",
      "209 1.4390041828155518\n",
      "210 1.3720762729644775\n",
      "211 1.30852210521698\n",
      "212 1.2479393482208252\n",
      "213 1.1900023221969604\n",
      "214 1.134930968284607\n",
      "215 1.0824750661849976\n",
      "216 1.0323885679244995\n",
      "217 0.9846950173377991\n",
      "218 0.9392337203025818\n",
      "219 0.8958359956741333\n",
      "220 0.854507327079773\n",
      "221 0.8151715993881226\n",
      "222 0.7775081396102905\n",
      "223 0.741688072681427\n",
      "224 0.7075701951980591\n",
      "225 0.6750682592391968\n",
      "226 0.6440219283103943\n",
      "227 0.6144191026687622\n",
      "228 0.5862026214599609\n",
      "229 0.5592942237854004\n",
      "230 0.5336652398109436\n",
      "231 0.5091333389282227\n",
      "232 0.4857807755470276\n",
      "233 0.46360069513320923\n",
      "234 0.44230350852012634\n",
      "235 0.42207077145576477\n",
      "236 0.4027763903141022\n",
      "237 0.38437896966934204\n",
      "238 0.36683890223503113\n",
      "239 0.35003650188446045\n",
      "240 0.33409538865089417\n",
      "241 0.31885001063346863\n",
      "242 0.3043363094329834\n",
      "243 0.2904238998889923\n",
      "244 0.27720826864242554\n",
      "245 0.26453930139541626\n",
      "246 0.2525236904621124\n",
      "247 0.24102452397346497\n",
      "248 0.23008498549461365\n",
      "249 0.21959328651428223\n",
      "250 0.2096215933561325\n",
      "251 0.20007558166980743\n",
      "252 0.19097062945365906\n",
      "253 0.1823432296514511\n",
      "254 0.1740855723619461\n",
      "255 0.16616290807724\n",
      "256 0.15862444043159485\n",
      "257 0.15144206583499908\n",
      "258 0.1445508599281311\n",
      "259 0.13804717361927032\n",
      "260 0.13177792727947235\n",
      "261 0.12577584385871887\n",
      "262 0.12008947879076004\n",
      "263 0.11467472463846207\n",
      "264 0.10951688885688782\n",
      "265 0.10456326603889465\n",
      "266 0.0998004823923111\n",
      "267 0.09530653059482574\n",
      "268 0.09100636839866638\n",
      "269 0.08688852936029434\n",
      "270 0.08295724540948868\n",
      "271 0.07924234867095947\n",
      "272 0.07567691057920456\n",
      "273 0.07225626707077026\n",
      "274 0.06900832802057266\n",
      "275 0.06590209156274796\n",
      "276 0.06293146312236786\n",
      "277 0.060119375586509705\n",
      "278 0.0574180893599987\n",
      "279 0.054831281304359436\n",
      "280 0.05238280072808266\n",
      "281 0.05000856891274452\n",
      "282 0.047780077904462814\n",
      "283 0.04563014581799507\n",
      "284 0.04357897490262985\n",
      "285 0.04163263738155365\n",
      "286 0.03977704793214798\n",
      "287 0.037995677441358566\n",
      "288 0.03629019483923912\n",
      "289 0.03467678651213646\n",
      "290 0.03313974291086197\n",
      "291 0.03164979815483093\n",
      "292 0.03022429719567299\n",
      "293 0.028898242861032486\n",
      "294 0.027581527829170227\n",
      "295 0.02635202370584011\n",
      "296 0.025186676532030106\n",
      "297 0.024057652801275253\n",
      "298 0.022992443293333054\n",
      "299 0.021963683888316154\n",
      "300 0.020992109552025795\n",
      "301 0.02006768248975277\n",
      "302 0.01918727159500122\n",
      "303 0.01833932287991047\n",
      "304 0.017508935183286667\n",
      "305 0.01673782244324684\n",
      "306 0.0160052552819252\n",
      "307 0.015307172201573849\n",
      "308 0.014628740958869457\n",
      "309 0.013978561386466026\n",
      "310 0.013368673622608185\n",
      "311 0.012784846127033234\n",
      "312 0.012226269580423832\n",
      "313 0.011697997339069843\n",
      "314 0.011189517565071583\n",
      "315 0.01070381049066782\n",
      "316 0.010239332914352417\n",
      "317 0.009789521805942059\n",
      "318 0.009361814707517624\n",
      "319 0.008955960161983967\n",
      "320 0.008570670150220394\n",
      "321 0.008195087313652039\n",
      "322 0.00784570537507534\n",
      "323 0.007510276976972818\n",
      "324 0.007186456583440304\n",
      "325 0.0068789017386734486\n",
      "326 0.006583455018699169\n",
      "327 0.006303888279944658\n",
      "328 0.006036394275724888\n",
      "329 0.005780348554253578\n",
      "330 0.005532555282115936\n",
      "331 0.005301046185195446\n",
      "332 0.005077864043414593\n",
      "333 0.004864408168941736\n",
      "334 0.004660854581743479\n",
      "335 0.004466564394533634\n",
      "336 0.004282405599951744\n",
      "337 0.00409952737390995\n",
      "338 0.003930611535906792\n",
      "339 0.003772798692807555\n",
      "340 0.003621759358793497\n",
      "341 0.0034698653034865856\n",
      "342 0.0033298474736511707\n",
      "343 0.0031959637999534607\n",
      "344 0.003064662916585803\n",
      "345 0.002941180719062686\n",
      "346 0.0028227490838617086\n",
      "347 0.0027097950223833323\n",
      "348 0.002599612809717655\n",
      "349 0.0025016008876264095\n",
      "350 0.0024061002768576145\n",
      "351 0.0023120082914829254\n",
      "352 0.0022209046874195337\n",
      "353 0.002135466318577528\n",
      "354 0.0020523262210190296\n",
      "355 0.001975408988073468\n",
      "356 0.0018989989766851068\n",
      "357 0.0018273575697094202\n",
      "358 0.001757304067723453\n",
      "359 0.001693023252300918\n",
      "360 0.001630415441468358\n",
      "361 0.0015714284963905811\n",
      "362 0.0015122200129553676\n",
      "363 0.0014577176189050078\n",
      "364 0.0014051604084670544\n",
      "365 0.0013537703780457377\n",
      "366 0.0013052194844931364\n",
      "367 0.0012576173758134246\n",
      "368 0.0012136257719248533\n",
      "369 0.0011707840021699667\n",
      "370 0.0011310752015560865\n",
      "371 0.0010916029568761587\n",
      "372 0.0010522421216592193\n",
      "373 0.00101744313724339\n",
      "374 0.000982200726866722\n",
      "375 0.0009469771757721901\n",
      "376 0.0009150910773314536\n",
      "377 0.0008862115209922194\n",
      "378 0.0008567849290557206\n",
      "379 0.0008258918533101678\n",
      "380 0.0008006892167031765\n",
      "381 0.0007752370438538492\n",
      "382 0.0007507957052439451\n",
      "383 0.0007266864995472133\n",
      "384 0.0007031044224277139\n",
      "385 0.0006810096674598753\n",
      "386 0.0006595328450202942\n",
      "387 0.0006393390940502286\n",
      "388 0.0006196119938977063\n",
      "389 0.0006009501521475613\n",
      "390 0.000582729815505445\n",
      "391 0.0005650835228152573\n",
      "392 0.0005489973700605333\n",
      "393 0.0005327655235305429\n",
      "394 0.000517442764248699\n",
      "395 0.0005015606875531375\n",
      "396 0.0004856050363741815\n",
      "397 0.0004724894242826849\n",
      "398 0.0004580894310493022\n",
      "399 0.00044510432053357363\n",
      "400 0.0004325163026805967\n",
      "401 0.00042059237603098154\n",
      "402 0.00040849560173228383\n",
      "403 0.00039689382538199425\n",
      "404 0.00038530887104570866\n",
      "405 0.00037514965515583754\n",
      "406 0.0003653234743978828\n",
      "407 0.00035671793739311397\n",
      "408 0.0003471566888038069\n",
      "409 0.0003366142918821424\n",
      "410 0.0003284892882220447\n",
      "411 0.000319712795317173\n",
      "412 0.00031113342265598476\n",
      "413 0.0003034223918803036\n",
      "414 0.0002956021926365793\n",
      "415 0.0002882501867134124\n",
      "416 0.0002804916584864259\n",
      "417 0.00027315120678395033\n",
      "418 0.00026673043612390757\n",
      "419 0.0002603761968202889\n",
      "420 0.00025401037419214845\n",
      "421 0.00024831254268065095\n",
      "422 0.00024246636894531548\n",
      "423 0.00023652586969546974\n",
      "424 0.00023030518786981702\n",
      "425 0.00022564790560863912\n",
      "426 0.00022026860096957535\n",
      "427 0.00021486861805897206\n",
      "428 0.00020951253827661276\n",
      "429 0.00020531550399027765\n",
      "430 0.00020042952382937074\n",
      "431 0.0001954566250788048\n",
      "432 0.0001911994331749156\n",
      "433 0.0001868826220743358\n",
      "434 0.00018280689255334437\n",
      "435 0.00017845116963144392\n",
      "436 0.00017464986012782902\n",
      "437 0.0001707165065454319\n",
      "438 0.00016744951426517218\n",
      "439 0.0001643231080379337\n",
      "440 0.00016081877402029932\n",
      "441 0.00015763426199555397\n",
      "442 0.00015417265240103006\n",
      "443 0.00015092863759491593\n",
      "444 0.00014784828817937523\n",
      "445 0.0001449640840291977\n",
      "446 0.00014186804764904082\n",
      "447 0.0001388257514918223\n",
      "448 0.0001366826763842255\n",
      "449 0.0001342001196462661\n",
      "450 0.0001314403343712911\n",
      "451 0.00012885729665867984\n",
      "452 0.00012612546561285853\n",
      "453 0.00012400226842146367\n",
      "454 0.00012159006291767582\n",
      "455 0.00011932081542909145\n",
      "456 0.0001174739736597985\n",
      "457 0.00011504934082040563\n",
      "458 0.00011266070214333013\n",
      "459 0.0001107290736399591\n",
      "460 0.00010882718197535723\n",
      "461 0.00010662895510904491\n",
      "462 0.0001052985608112067\n",
      "463 0.00010329019278287888\n",
      "464 0.00010113834287039936\n",
      "465 9.982766641769558e-05\n",
      "466 9.789884643396363e-05\n",
      "467 9.588505054125562e-05\n",
      "468 9.439781570108607e-05\n",
      "469 9.287997090723366e-05\n",
      "470 9.091538959182799e-05\n",
      "471 8.930941112339497e-05\n",
      "472 8.782098302617669e-05\n",
      "473 8.652586257085204e-05\n",
      "474 8.502401760779321e-05\n",
      "475 8.364450332010165e-05\n",
      "476 8.211999374907464e-05\n",
      "477 8.079761028056964e-05\n",
      "478 7.948226266307756e-05\n",
      "479 7.833445852156729e-05\n",
      "480 7.679704867769033e-05\n",
      "481 7.577005453640595e-05\n",
      "482 7.439551700372249e-05\n",
      "483 7.331453525694087e-05\n",
      "484 7.215647929115221e-05\n",
      "485 7.089351856848225e-05\n",
      "486 6.9910500315018e-05\n",
      "487 6.876773841213435e-05\n",
      "488 6.738224328728393e-05\n",
      "489 6.659729842795059e-05\n",
      "490 6.558634049724787e-05\n",
      "491 6.448361091315746e-05\n",
      "492 6.365230365190655e-05\n",
      "493 6.250960723264143e-05\n",
      "494 6.155420851428062e-05\n",
      "495 6.063862019800581e-05\n",
      "496 6.003919770591892e-05\n",
      "497 5.9291367506375536e-05\n",
      "498 5.850107845617458e-05\n",
      "499 5.733182479161769e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D: Advanced PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later assignments we will use PyTorch as a higher level library which provides functions for automatic differentiation, so we do not need to perform backpropagation by ourselves. \n",
    "\n",
    "Additionally, for complicated neural networks, we do not need to write the entire architecture from scratch, as PyTorch provides many fundamental models which we can combine and modify. \n",
    "\n",
    "Please refer to [this tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for more details.\n",
    "\n",
    "An example of such a model is the Long Short-Term Memory Network (LSTM), which you can read more about [here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
    "\n",
    "LSTMs will be covered later in the class, but for now you just need to know that they are used for sequence modeling by keeping track of a hidden state which is dependent upon previous inputs in the sequence. \n",
    "\n",
    "Sequence modeling is essential for NLP, as this defines some kind of temporal dependence in the input sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Automatic differentiation functions is provided by torch.optim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words and labels must be converted to indices in order to be fed into the network. \n",
    "\n",
    "These indices are then converted to tensors by using the `prepare_sequence` function. \n",
    "\n",
    "In this example we will look at **part of speech training**; for each word we want to find its labeled part of speech. \n",
    "\n",
    "We will use some toy data to train a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
     ]
    }
   ],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "# convert toy training data to indices\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "ix_to_tag = {val: key for key, val in tag_to_ix.items()}\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "# NOTE: it is also possible to use pretrained embeddings like word2vec or glove, but this tutorial learns \n",
    "# an embedding layer along with the rest of the network\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All PyTorch models inherit from `nn.Module` and have a `forward()` function. \n",
    "\n",
    "Assume we have a class `MyModel` which inherits from `nn.Module` and has a forward function which takes in one parameter called \"input\". Then, to call the model, we can just run:\n",
    "\n",
    "`output = MyModel(input)`. \n",
    "\n",
    "The model below has the following high-level structure:\n",
    "\n",
    "word indices -> embedding -> LSTM hidden state -> conversion to tag space (dimension = number of tags) -> softmax probability over the tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the model and the loss function. \n",
    "\n",
    "We will learn more about loss functions, but **negative log likelihood** is a common loss function when you output probabilities over a set of classes (in this case part of speech tags). \n",
    "\n",
    "We then define an optimizer to perform stochastic gradient descent. \n",
    "\n",
    "You can read more about the optimizers available [here](https://pytorch.org/docs/stable/optim.html). \n",
    "\n",
    "We then loop over the training data 300 times and update the parameters based on the loss on our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2279, -1.2197, -0.8873],\n",
      "        [-1.1959, -1.2777, -0.8701],\n",
      "        [-1.2242, -1.2044, -0.9011],\n",
      "        [-1.2788, -1.0888, -0.9545],\n",
      "        [-1.2699, -1.1586, -0.9033]])\n",
      "tensor([[-0.0822, -3.4438, -3.0580],\n",
      "        [-5.2884, -0.0283, -3.7805],\n",
      "        [-3.4603, -3.4406, -0.0656],\n",
      "        [-0.0457, -4.5218, -3.3867],\n",
      "        [-5.6415, -0.0229, -3.9603]])\n",
      "['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)\n",
    "    predlabels = [ix_to_tag[int(idx)] for idx in tag_scores.argmax(dim=-1)]\n",
    "    print(predlabels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
