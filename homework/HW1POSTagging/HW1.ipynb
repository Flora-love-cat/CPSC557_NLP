{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b03ae1fc",
   "metadata": {},
   "source": [
    "# HW1 Language Modeling and Part of Speech Tagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cce45f2c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we will:\n",
    "\n",
    "1. Go through the basics of NLTK, the most popular NLP library for Python;\n",
    "\n",
    "2. Develop and evaluate several language models;\n",
    "\n",
    "3. Develop and evaluate a full part-of-speech tagger using the Viterbi algorithm;\n",
    "\n",
    "4. Compare that part-of-speech tagger with a neural network-based model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a419811",
   "metadata": {},
   "source": [
    "### Provided Files and Report\n",
    "\n",
    "Now let’s go back to the assignment folder you copied to your home directory (should be ~/hidden/<Your\n",
    "PIN>/Homework1). \n",
    "\n",
    "In this assignment we will be using the Brown Corpus, which is a dataset of English\n",
    "sentences compiled in the 1960s. \n",
    "\n",
    "We have provided this dataset to you so you don’t have to load it yourself from NLTK.\n",
    "\n",
    "Besides data, we are also providing code for evaluating your language models and POS tagger, and a skeleton code for the assignment. \n",
    "\n",
    "In this assignment you should not create any new code files, but rather just fill in the\n",
    "functions in the skeleton code.\n",
    "\n",
    "The only files that you should modify throughout the whole assignment are `ngramPOS.py` and `HMMPOS.py`.\n",
    "\n",
    "We have provided the following files:\n",
    "\n",
    "| File                          | Description                              |\n",
    "|-------------------------------|------------------------------------------|\n",
    "| data/Brown_train.txt          | Untagged Brown English training data       (32491 sentences)      |\n",
    "| data/Brown_tagged_train.txt   | Tagged Brown English training data               |\n",
    "| data/Brown_dev.txt            | Untagged Brown English development data          |\n",
    "| data/Brown_tagged_dev.txt     | Tagged Brown English development data            |\n",
    "| data/Sample1.txt              | Additional sentences for part A          |\n",
    "| data/Sample2.txt              | More additional sentences for part A      |\n",
    "| data/wikicorpus_tagged_train.txt | Tagged Wikicorpus Spanish training data       |\n",
    "| data/wikicorpus_dev.txt       | Untagged Wikicorpus Spanish development data     |\n",
    "| data/wikicorpus_tagged_dev.txt | Tagged Wikicorpus Spanish development data     |\n",
    "|                               |                                          |\n",
    "| perplexity.py                 | A script to analyze perplexity for part A|\n",
    "| accuracy.py                        | A script to analyze POS tagging accuracy for part B |\n",
    "| neuralPOStagger.py        | A neural POS tagger model                |\n",
    "| tagger.pt.model               | Model with trained parameters for the neural tagger |\n",
    "|                               |                                          |\n",
    "| ngramPOS.py                 | Skeleton code for part A                 |\n",
    "| HMMPOS.py                 | Skeleton code for part B                 |\n",
    "| solutionsC.py                 | A script to train and evaluate a Spanish tagger for part C |\n",
    "| solutionsD.py                 | A script to evaluate the neural tagger from for part D |\n",
    "|                               |                                          |\n",
    "| output/                       | Directory where answers to part A, B, and C are stored |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bccf0e",
   "metadata": {},
   "source": [
    "**Data Files Format**\n",
    "\n",
    "The untagged data files have one sentence per line, and the tokens are separated by spaces. \n",
    "\n",
    "The tagged data files are in the same format, except that, instead of tokens separated by spaces, those files have TOKEN/TAG pairs separated by spaces.\n",
    "\n",
    "**Report**\n",
    "\n",
    "Before starting the assignment, create a `README.txt` file on the homework folder. \n",
    "\n",
    "At the top, include a header that contains your netID and name. \n",
    "\n",
    "Throughout the assignment, you will be asked to include specific output or comment on specific aspects of your work. \n",
    "\n",
    "We recommend filling the README file as you go through the assignment, as opposed to starting the report afterwards.\n",
    "\n",
    "In this report it is not necessary to include introductions and/or explanations, other than the ones explicitly requested throughout the assignment.\n",
    "\n",
    "The first thing to report in the README is the affix patterns you came up with for your `regexp_tagger` and some situations (with brief explanation) where the tagger didn’t work.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5c091be",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "Once you have SSHed into the Zoo cluster, copy the homework files to your hidden directory under Homework1 folder.\n",
    "\n",
    "`cp -r /home/classes/cs477/assignments/ 2021 - Homework1 ~/hidden/<YOUR_PIN>`\n",
    "\n",
    "For the virtualenv, all you need to do is include the following line in your environmental setup section:\n",
    "\n",
    "Source the following environment to ensure that you are using the same packages that will be used for grading:\n",
    "\n",
    "`source /home/classes/cs477/venvs/hw1/bin/activate`\n",
    "\n",
    "If you wish to reproduce the virtualenv on your own computer, do the following:\n",
    "While in the sourced virtualenv:\n",
    "\n",
    "`pip freeze > requirements.txt`\n",
    "\n",
    "Copy the requirements file to your computer\n",
    "\n",
    "Then, create a virtualenv on your computer\n",
    "\n",
    "```\n",
    "virtualenv --python=python3 hw\n",
    "source hw1/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f4f060a",
   "metadata": {},
   "source": [
    "### Basic NLTK Tutorial\n",
    "\n",
    "The Natural Language Toolkit (NLTK) is the most popular NLP library for python. \n",
    "\n",
    "NLTK has already been installed on the Zoo, so you do not have to install it yourself. \n",
    "\n",
    "If you followed the “Environment Setup” section, you should be good to start using NLTK now.\n",
    "\n",
    "Now let’s walk through some simple NLTK use cases that concern this assignment. \n",
    "\n",
    "For that, you will need to first open a python interactive shell. \n",
    "\n",
    "Just type python or python3 on the command line and you should see the interactive shell start.\n",
    "\n",
    "**Import the NLTK package**\n",
    "\n",
    "To use the NLTK package, include the following line at the beginning of your code (or in this case just type directly into the interactive shell):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce7ed9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5cb3967",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "\n",
    "To tokenize means to break a continuous string into tokens (usually words, but a token could also be a symbol, punctuation, or other meaningful unit). \n",
    "\n",
    "In NLTK, text can be tokenized using the `word_tokenize()` method.\n",
    "\n",
    "It returns a list of tokens that will be the input for many methods in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af30a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"At eight o'clock on Thursday morning on Thursday morning on Thursday morning.\"\n",
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "550986b6",
   "metadata": {},
   "source": [
    "**N-grams Generation**\n",
    "\n",
    "An n-gram is a contiguous sequence of n tokens in a sentence. \n",
    "\n",
    "The following code returns a list of bigrams and a list of trigrams. \n",
    "\n",
    "Each n-gram is represented as a tuple in python (if you are not familiar with python tuples read the python tuple doc page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0ca0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tuples = list(nltk.bigrams(tokens))\n",
    "trigram_tuples = list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ace379db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'eight'),\n",
       " ('eight', \"o'clock\"),\n",
       " (\"o'clock\", 'on'),\n",
       " ('on', 'Thursday'),\n",
       " ('Thursday', 'morning'),\n",
       " ('morning', 'on'),\n",
       " ('on', 'Thursday'),\n",
       " ('Thursday', 'morning'),\n",
       " ('morning', 'on'),\n",
       " ('on', 'Thursday'),\n",
       " ('Thursday', 'morning'),\n",
       " ('morning', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tuples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47762322",
   "metadata": {},
   "source": [
    "We can calculate the count of each n-gram using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "587068b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('morning', 'on'): 2,\n",
       " ('on', 'Thursday'): 3,\n",
       " ('morning', '.'): 1,\n",
       " ('At', 'eight'): 1,\n",
       " ('eight', \"o'clock\"): 1,\n",
       " ('Thursday', 'morning'): 3,\n",
       " (\"o'clock\", 'on'): 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = {item: bigram_tuples.count(item) for item in set(bigram_tuples)}\n",
    "count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8467f8a4",
   "metadata": {},
   "source": [
    "Or we can find all the distinct n-grams that contain the word “on”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39d94c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('morning', 'on'), ('on', 'Thursday'), (\"o'clock\", 'on')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams = [item for item in set(bigram_tuples) if \"on\" in item]\n",
    "ngrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa9d0563",
   "metadata": {},
   "source": [
    "if you want to calculate the counts in a faster way, you can use a Counter object from python’s built-in collections library.\n",
    "\n",
    "The dictionary comprehension code can take in the worst-case $O(n^2)$ time, whereas the Counter object runs in $O(n)$. [check out](https://docs.python.org/3.6/library/collections.html#collections.Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e59fff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('At', 'eight'): 1,\n",
       "         ('eight', \"o'clock\"): 1,\n",
       "         (\"o'clock\", 'on'): 1,\n",
       "         ('on', 'Thursday'): 3,\n",
       "         ('Thursday', 'morning'): 3,\n",
       "         ('morning', 'on'): 2,\n",
       "         ('morning', '.'): 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter(bigram_tuples)\n",
    "count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f07d668d",
   "metadata": {},
   "source": [
    "**Default POS Tagger (Non-statistical)**\n",
    "\n",
    "The most naïve way of tagging parts-of-speech is to assign the same tag to all the tokens. This is exactly what the NLTK default tagger does. \n",
    "\n",
    "Although inaccurate and arbitrary, it sets a baseline for taggers, and can be used as a default tagger when more sophisticated methods fail.\n",
    "\n",
    "In NLTK, it’s easy to create a default tagger by indicating the default tag in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8762e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'NN'),\n",
       " ('eight', 'NN'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'NN'),\n",
       " ('Thursday', 'NN'),\n",
       " ('morning', 'NN'),\n",
       " ('on', 'NN'),\n",
       " ('Thursday', 'NN'),\n",
       " ('morning', 'NN'),\n",
       " ('on', 'NN'),\n",
       " ('Thursday', 'NN'),\n",
       " ('morning', 'NN'),\n",
       " ('.', 'NN')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a default tagger with default tag NN\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "tagged_sentence = default_tagger.tag(tokens)\n",
    "tagged_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6f4cb7b",
   "metadata": {},
   "source": [
    "Now we have our first tagger. NLTK can help if you need to understand the meaning of a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "398bd715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/wenxinxu/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')  # Downloading package `tagsets`\n",
    "nltk.help.upenn_tagset('NN') # Show the description of the tag 'NN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c29754b5",
   "metadata": {},
   "source": [
    "**Regular Expression POS Tagger (Non-statistical)**\n",
    "\n",
    "A regular expression tagger maintains a list of regular expressions paired with a tag.\n",
    "\n",
    "see the Wikipedia article for more information about [regular expressions](http://en.wikipedia.org/wiki/Regular_expression). \n",
    "\n",
    "The tagger tries to match each token to one of the regular expressions in its list such that the token receives the tag that is paired with the first matching regular expression. \n",
    "\n",
    "“None” is given to a token that does not match any regular expression.\n",
    "\n",
    "To create a Regular Expression Tagger in NLTK, we provide a list of pattern-tag pairs to the appropriate constructor. Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba8d867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', None),\n",
       " ('eight', None),\n",
       " (\"o'clock\", None),\n",
       " ('on', None),\n",
       " ('Thursday', None),\n",
       " ('morning', 'VBG'),\n",
       " ('on', None),\n",
       " ('Thursday', None),\n",
       " ('morning', 'VBG'),\n",
       " ('on', None),\n",
       " ('Thursday', None),\n",
       " ('morning', 'VBG'),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = [(r'.*ing$', 'VBG'),(r'.*ed$', 'VBD'),(r'.*es$', 'VBZ'),(r'.*ed$', 'VB')]\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "regexp_tagger.tag(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a657abc0",
   "metadata": {},
   "source": [
    "- See how many affix patterns you can come up with for your `regexp_tagger` up to a maximum of 10 rules. \n",
    "\n",
    "    `r'.*ness$'`: 'NN' - Nouns derived from adjectives (e.g., happiness, darkness)\n",
    "\n",
    "    `r'^[A-Z][a-z]*$'`: 'NNP' - Proper nouns 专有名词 (e.g., John, France)\n",
    "\n",
    "    `r'.*s$'`: 'NNS' - Plural nouns (e.g., cats, houses)\n",
    "\n",
    "    `r'.*\\'s$'`: 'POS' - Possessive nouns (e.g., John's, dog's)\n",
    "\n",
    "    `r'.*ly$'`: 'RB' - Adverbs (e.g., quickly, slowly)\n",
    "\n",
    "    `r'.*ing$'`: 'VBG' - Present participle verbs (e.g., running, singing)\n",
    "\n",
    "    `r'.*ed$'`: 'VBD' - Past tense verbs (e.g., walked, jumped)\n",
    "\n",
    "    `r'.*es$'`: 'VBZ' - Third-person singular present verbs (e.g., goes, tries)\n",
    "\n",
    "    `r'.*ould$'`: 'MD' - Modal verbs (e.g., could, would)\n",
    "\n",
    "    `r'^-?[0-9]+(\\.[0-9]+)?$'`: 'CD' - Cardinal numbers (e.g., 42, -3, 0.5)\n",
    "    \n",
    "\n",
    "- In what situations might this `regexp_tagger` report the wrong tags? \n",
    "\n",
    "    Ambiguity: Some words might have the same affix patterns but belong to different parts of speech, like \"axes\" can be a plural noun (NNS) or the third-person singular present verb (VBZ) for \"to ax\".\n",
    "\n",
    "    Irregular forms: Many English words have irregular forms that do not follow the common affix patterns (e.g., \"went\" as the past tense of \"go\").\n",
    "\n",
    "    Unusual capitalization: Proper nouns might not always start with a capital letter, or regular nouns might be capitalized in titles, leading to misclassification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bc55cc8",
   "metadata": {},
   "source": [
    "**N-gram HMM Tagger (Statistical)**\n",
    "\n",
    "Although there are many different kinds of statistical taggers, we will only work with Hidden Markov Model (HMM) taggers in this assignment.\n",
    "\n",
    "Like every statistical tagger, n-gram taggers use a set of tagged sentences, known as the training data, to create a model that is used to tag new sentences. \n",
    "\n",
    "In NLTK, a sentence of the training data must be formatted as a list of tuples, where each tuple is a pair in word-tag format (see example below).\n",
    "\n",
    "`[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL')]`\n",
    "\n",
    "NLTK already provides corpora formatted this way. \n",
    "\n",
    "In particular, we are going to use the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "774aca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/wenxinxu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "# import the corpus from NLTK and build the training set from sentences in “news”\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "training = brown.tagged_sents(categories='news')\n",
    "\n",
    "# Create Unigram, Bigram, Trigram taggers based on the training set.\n",
    "unigram_tagger = nltk.UnigramTagger(training)\n",
    "bigram_tagger = nltk.BigramTagger(training)\n",
    "trigram_tagger = nltk.TrigramTagger(training)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a18ca0d",
   "metadata": {},
   "source": [
    "Although we could also build 4-gram, 5-gram, etc. taggers, trigram taggers are the most popular model. \n",
    "\n",
    "This is because a trigram model is an excellent compromise between computational complexity and performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb363f12",
   "metadata": {},
   "source": [
    "**Combination of Taggers**\n",
    "\n",
    "A tagger fails when it cannot find a best tag sequence for a given sentence. \n",
    "\n",
    "For example, one situation when an n-gram tagger will fail is when it encounters an OOV (out of vocabulary) word not seen in the training data, the tagger will tag the word as ”NONE”. \n",
    "\n",
    "One way to handle tagger failure is to fall back to an alternative tagger\n",
    "if the primary one fails. This is called “using back off.” \n",
    "\n",
    "One can easily set a hierarchy of taggers in NLTK as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a4ae093",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "bigram_tagger = nltk.BigramTagger(training, backoff=default_tagger)\n",
    "trigram_tagger = nltk.TrigramTagger(training, backoff=bigram_tagger)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "886a8fb0",
   "metadata": {},
   "source": [
    "**Tagging Low Frequency Words**\n",
    "\n",
    "Low frequency words are another common source of tagger failure, because an n-gram that contains a low frequency word and is found in the test data might not be found in the training data. \n",
    "\n",
    "One method to resolve this tagger failure is to group low frequency words. \n",
    "\n",
    "For example, we could substitute the token “_RARE_” for all words with frequency lower than 0.05% in the training data. \n",
    "\n",
    "Any words in the development data that were not found in the training data could then be treated instead as the token “_RARE_”, thereby allowing the algorithm to assign a tag. \n",
    "\n",
    "If we wanted to add another group, we could substitute the string “_NUMBER_” for\n",
    "those rare words that represent a numeral. \n",
    "\n",
    "When tagging the test data, we could substitute “_NUMBER_” for all tokens that were unseen in the training data and represent a numeral. \n",
    "\n",
    "We will use this technique later in this assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "360ac3de",
   "metadata": {},
   "source": [
    "## Part A – Language Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d67363e",
   "metadata": {},
   "source": [
    "In this part of the assignment, you will be filling the **ngramPOS.py** file. \n",
    "\n",
    "Open the file and notice there are several functions with a #TODO comment; \n",
    "\n",
    "you will have to complete those functions. \n",
    "\n",
    "To understand the general workflow of the script, read the `main()` function **_but do not modify it_**. \n",
    "\n",
    "You also shouldn’t import any additional libraries/functions beyond what is already provided, but you also don’t have to use everything that’s provided if you can find a solution that works without something.\n",
    "\n",
    "If you are having issues with saving your edits in the file, check the permissions using `ls -la` in the terminal.\n",
    "\n",
    "If the third character is `-`, you won’t be able to edit: it should be `w`. \n",
    "\n",
    "To fix this, you can write the command `chmod u+w ngramPOS.py`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "640bb8cd",
   "metadata": {},
   "source": [
    "### 1. calculate n-gram probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a258157",
   "metadata": {},
   "source": [
    "For a trigram model, the probability of a word given its preceding word is calculated as count of trigram divided by count of preceding 2 words:\n",
    "\n",
    "$$\n",
    "\\hat p(w_3 | w_1, w_2) =\\frac{p(w_1,w_2,w_3)}{p(w_1,w_2)}= \\frac{\\frac{\\text{Count}(w_1,w_2,w_3)}{ \\text{Count}trigram}}{\\frac{\\text{Count}(w_1,w_2)}{\\text{Count} bigram}} \\approx\\frac{\\text{Count}(w_1,w_2,w_3)}{\\text{Count}(w_1,w_2)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7356273",
   "metadata": {},
   "source": [
    "Calculate the uni-, bi-, and trigram log-probabilities of the data in `data/Brown_train.txt`. \n",
    "\n",
    "This corresponds to implementing the `calc_probabilities()` function. \n",
    "\n",
    "In this assignment we will always use log base 2. \n",
    "\n",
    "Remember that n-gram probabilities are calculated as the probability of the nth tag conditioned on the first n-1 tags. It’s not the joint probability of the entire n-gram."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51eb5b47",
   "metadata": {},
   "source": [
    "Don’t forget to add the appropriate sentence start and end symbols; \n",
    "\n",
    "- Sentence boundaries: The start and end symbols help define the boundaries of a sentence in the n-gram models. This is important because words at the beginning or end of a sentence may have different probabilities than those in the middle of a sentence. \n",
    "\n",
    "- Conditioning probabilities: \n",
    "\n",
    "    - start symbol is inserted **once and twice** at the beginning of each sentence for bigram, trigram model, respectively. start symbol is used to condition the probability of the first word in a sentence (in bigrams) or the first and second words (in trigrams).\n",
    "\n",
    "    - The end symbol is appended to the end of each sentence for unigram, bigram, and trigram models.\n",
    "\n",
    "use the provided constants `START_SYMBOL` and `STOP_SYMBOL` in the skeleton code. \n",
    "\n",
    "You may or may not use NLTK to help you here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88d04852",
   "metadata": {},
   "source": [
    "The code will automatically output the log probabilities in a file `output/A1.txt`\n",
    "\n",
    "Here’s a few examples of log probabilities of uni-, bi-, and trigrams for you to check your results:\n",
    "\n",
    "```\n",
    "UNIGRAM captain -14.2810 \n",
    "UNIGRAM captain's -17.0883 \n",
    "UNIGRAM captaincy -19.4102 \n",
    " \n",
    "BIGRAM and religion -12.9316 \n",
    "BIGRAM and religious -11.3466 \n",
    "BIGRAM and religiously -13.9316 \n",
    " \n",
    "TRIGRAM and not a -4.0297 \n",
    "TRIGRAM and not by -4.6147 \n",
    "TRIGRAM and not come -5.6147\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "491a3165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAM captain -14.280981989874268\n",
      "UNIGRAM captain's -17.088336911931872\n",
      "UNIGRAM captaincy -19.410265006819234\n",
      "BIGRAM and religion -12.931660898852277\n",
      "BIGRAM and religious -11.346698398131123\n",
      "BIGRAM and religiously -13.931660898852277\n",
      "TRIGRAM and not a -4.029747343394052\n",
      "TRIGRAM and not by -4.614709844115208\n",
      "TRIGRAM and not come -5.614709844115209\n"
     ]
    }
   ],
   "source": [
    "! grep -E \"UNIGRAM captain|UNIGRAM captain's|UNIGRAM captaincy |BIGRAM and religion |BIGRAM and religious |BIGRAM and religiously |TRIGRAM and not a |TRIGRAM and not by |TRIGRAM and not come \" output/A1.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1451d55a",
   "metadata": {},
   "source": [
    "Make sure your results match the examples above up to four decimal places. \n",
    "\n",
    "Note that even though the code produces many decimal digits, common sense tells us to consider only the most significant ones. \n",
    "\n",
    "If they do match, include in your README the log probabilities to four decimal places of the\n",
    "following n-grams ( note the n-grams are case-sensitive ):\n",
    "\n",
    "```\n",
    "UNIGRAM natural\n",
    "BIGRAM natural that\n",
    "TRIGRAM natural that he\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23267ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAM natural -13.766408817044509\n",
      "BIGRAM natural that -4.058893689053568\n",
      "TRIGRAM natural that he -1.5849625007211563\n"
     ]
    }
   ],
   "source": [
    "! grep -E \"UNIGRAM natural |BIGRAM natural that |TRIGRAM natural that he \" output/A1.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91d3f851",
   "metadata": {},
   "source": [
    "### 2. score sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99db22c8",
   "metadata": {},
   "source": [
    "Use your models to find the log-probability, or score, of each sentence in the Brown training data with each n-gram model. \n",
    "\n",
    "This corresponds to implementing the `score()` function.\n",
    "\n",
    "Make sure to accommodate the possibility that you may encounter in the sentences an n-gram that doesn’t exist in the training corpus. This will not happen now, because we are computing the log-probabilities of the training sentences, but it will be necessary for question 5. \n",
    "\n",
    "If you find any n-gram that was not in the training sentences, set the whole sentence log-probability to the constant `MINUS_INFINITY_SENTENCE_LOG_PROB`.\n",
    "\n",
    "The code will output scores in three files: `output/A2.uni.txt`, `output/A2.bi.txt`, `output/A2.tri.txt`. \n",
    "\n",
    "These files simply list the log-probabilities of each sentence for each different model. \n",
    "\n",
    "Here’s what the first few lines of each file look like (truncated at four decimal places):\n",
    "\n",
    "```\n",
    "A2.uni.txt \n",
    "-178.7268 \n",
    "-259.8586 \n",
    "-143.3304 \n",
    " \n",
    "A2.bi.txt \n",
    "-92.1039 \n",
    "-132.0966 \n",
    "-90.1859 \n",
    " \n",
    "A2.tri.txt \n",
    "-26.1800 \n",
    "-59.8531 \n",
    "-42.8392\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "593a0a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> ./output/A2.uni.txt <==\n",
      "-178.7268354828613\n",
      "-259.8586443200647\n",
      "-143.330429890216\n",
      "==> ./output/A2.bi.txt <==\n",
      "-92.10399842763805\n",
      "-132.09662640739958\n",
      "-90.1859108420144\n",
      "==> ./output/A2.tri.txt <==\n",
      "-26.180045341283773\n",
      "-59.85310080740878\n",
      "-42.83924489495398\n"
     ]
    }
   ],
   "source": [
    "! for file in output/A2.uni.txt output/A2.bi.txt output/A2.tri.txt; do echo \"==> $file <==\"; head -n 3 \"$file\"; done\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba2fb038",
   "metadata": {},
   "source": [
    "### 3. perplexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6661c7de",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Perplexity}(\\theta) = 2^{-\\frac{1}{N}\\sum_{i=1}^N \\log_2 p_{\\theta} (w_i | w_{i-n+1:i-1})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notations:\n",
    "\n",
    "$n$: context length\n",
    "\n",
    "$N$: sequence length\n",
    "\n",
    "$\\theta$: parameter of n-gram model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46f0004e",
   "metadata": {},
   "source": [
    "Now, you need to run our perplexity script, `perplexity.py`, on each of these files. \n",
    "\n",
    "This script will count the words of the corpus and use the log-probabilities computed by you to calculate the total perplexity of the corpus. \n",
    "\n",
    "To run the script, the command is:\n",
    "\n",
    "`python 3 perplexity.py <file of scores> <file of sentences that were scored>`\n",
    "\n",
    "where `<file of scores>` is one of the A2 output files and `<file of sentences that were scored>` is `data/Brown_train.txt`. \n",
    "\n",
    "**Include the perplexity of the corpus for the three different models in your README**. \n",
    "\n",
    "Here’s what our script printed when <file> was “A2.uni.txt” (truncated at four decimal\n",
    "places).\n",
    "\n",
    "```\n",
    "python3 perplexity.py output/A2.uni.txt data/Brown_train.txt\n",
    ">> The perplexity is 1052.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0cf02412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity is 1052.4865859021186\n",
      "The perplexity is 53.89847611982476\n",
      "The perplexity is 5.710679308201471\n"
     ]
    }
   ],
   "source": [
    "! for file in output/A2.uni.txt output/A2.bi.txt output/A2.tri.txt; do python3 perplexity.py \"$file\" data/Brown_train.txt; done\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3852ac0d",
   "metadata": {},
   "source": [
    "### 4. linear interpolation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "382e1681",
   "metadata": {},
   "source": [
    "a mixture model combining the probabilities of different order n-grams to estimate the probability of the next word.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(w_3 | w_1, w_2) \n",
    "&=\\text{trigram model + bigram model + unigram model} \\\\[1em]\n",
    "&= \\lambda _1 \\hat p(w_3 | w_1, w_2)  + \\lambda _2 \\hat p(w_3 | w_2) +\\lambda _3 \\hat p(w_3 ) \\\\[1em]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\lambda_1, \\lambda_2, \\lambda_3$ are the interpolation weights that sum to 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca2dba26",
   "metadata": {},
   "source": [
    "Implement linear interpolation among the three n-gram models you have created. \n",
    "\n",
    "This corresponds to implementing the `linearscore()` function.\n",
    "\n",
    "Linear interpolation is a method that aims to derive a better tagger by using all three uni-, bi-, and trigram taggers at once. \n",
    "\n",
    "Each tagger is given a weight described by a parameter lambda. \n",
    "\n",
    "There are some excellent methods for approximating the best set of lambdas, but for now, set the value of all three lambdas to be equal (i.e., 1/3). \n",
    "\n",
    "You can read more about linear interpolation on page 44 of the Jurafsky & Martin textbook.\n",
    "\n",
    "The code outputs scores to `output/A3.txt`. \n",
    "\n",
    "The first few lines of this file look like (truncated at four decimal places):\n",
    "\n",
    "```\n",
    "-46.5892 \n",
    "-85.7742 \n",
    "-58.5442 \n",
    "-47.5165 \n",
    "-52.7387\n",
    "```\n",
    "\n",
    "Run the perplexity script on the output file and include the perplexity in your README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed9f80dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-46.58916389729883\n",
      "-85.77421558996585\n",
      "-58.54420241627359\n",
      "-47.516505194800494\n",
      "-52.73873608147342\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 output/A3.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ae2179f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity is 12.551609488576808\n"
     ]
    }
   ],
   "source": [
    "!python3 perplexity.py output/A3.txt data/Brown_train.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83594604",
   "metadata": {},
   "source": [
    "### 5. different lambdas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc5b3bc0",
   "metadata": {},
   "source": [
    "In the `linearscore_newlambdas()` function, duplicate your `linearscore()` function but\n",
    "experiment with different values of lambda to see if you can improve the perplexity score. \n",
    "\n",
    "You don’t have to use the methods from Jurafsky & Martin but if you want to try implementing them, go for it!\n",
    "\n",
    "I’d recommend just toying around with them manually, though, as you will need to spend a decent amount of time on part B.\n",
    "\n",
    "In the README file, include the values of your new lambdas and explain why you think they might have improved the performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8649a35",
   "metadata": {},
   "source": [
    "> increasing the weight for trigrams and setting the weights in descending order (trigram > bigram > unigram) could decrease perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "933dc674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity is 5.914408253728796\n"
     ]
    }
   ],
   "source": [
    "# weights for unigram, bigram, trigram are 0.01, 0.04, 0.95\n",
    "!python3 perplexity.py output/A3.txt data/Brown_train.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc4dc3ef",
   "metadata": {},
   "source": [
    "### 6. compare performance of n-gram models and interpolation model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e4fa776",
   "metadata": {},
   "source": [
    "When you compare the performance (perplexity) between the best model without interpolation and the best model with linear interpolation, is the result you got expected? \n",
    "\n",
    "\n",
    "perplexity: trigram < interpolated < bigram < unigram\n",
    "\n",
    "I expect interpolated model perform best, but actually trigram model did. \n",
    "\n",
    "reason: \n",
    "\n",
    "- insufficient tuning of interpolation weights: A more systematic approach to finding the best interpolation weights, such as grid search or a more advanced optimization technique, could lead to better performance.\n",
    "\n",
    "- dataset characteristics with specific repetitive patterns. trigram model is enough, nigram and bigram models add noise.\n",
    "\n",
    "- dataset being small or homogeneous. training set has 32491 sentences, moderate size for n-gram language model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f629096",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "6. Both `data/Sample1.txt` and `data/Sample2.txt` contain sets of sentences; one of the files is an excerpt节选 of the Brown training dataset. \n",
    "\n",
    "    Use your model to score the sentences in both files. Our code outputs the scores of each into `Sample1_scored.txt` and `Sample2_scored.txt`. \n",
    "\n",
    "    Run the perplexity script on both output files and include the perplexity output of both samples in your README. \n",
    "\n",
    "    Use these results to make an argument for which sample belongs to the Brown dataset and which does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "999431fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity is 1.547619618012146\n",
      "The perplexity is 7.331421325067439\n"
     ]
    }
   ],
   "source": [
    "!python3 perplexity.py output/Sample1_scored.txt data/Brown_train.txt\n",
    "\n",
    "!python3 perplexity.py output/Sample2_scored.txt data/Brown_train.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dee07a94",
   "metadata": {},
   "source": [
    "## Part B – Part-of-Speech Tagging for English"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "545bdb60",
   "metadata": {},
   "source": [
    "In this part of the assignment, you will be filling the `HMMPOS.py` file. \n",
    "\n",
    "Open the file and notice there are several functions with a #TODO comment; you will have to complete those functions. \n",
    "\n",
    "To understand the general workflow of the script, read the `main()` function, **but do not modify it**. \n",
    "\n",
    "If you have issues with the writing permissions, do the same thing as you did for part A.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c18f76f",
   "metadata": {},
   "source": [
    "### 1. split tags and words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4314641",
   "metadata": {},
   "source": [
    "implementing the `split_wordtags()` function\n",
    "\n",
    "separate the tags and words in “Brown_tagged_train.txt”. \n",
    "\n",
    "store the sentences without tags in one data structure, and the tags alone in another. \n",
    "\n",
    "it's a trigram model, so add sentence start and stop symbols to both lists (of words and tags). Use the constants `START_SYMBOL` and `STOP_SYMBOL` already provided. \n",
    "\n",
    "make sure you accommodate words that themselves contain backslashes, e.g.., “1/2” is encoded as “1/2/NUM” in tagged form; make sure that the token you extract is “1/2” and not “1”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91ab83a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1/2', 'NUM')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"1/2/NUM\"\n",
    "word, tag = text.rsplit('/', 1)\n",
    "word, tag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3771a10b",
   "metadata": {},
   "source": [
    "### 2. calculate the trigram probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f5ccefb",
   "metadata": {},
   "source": [
    "implementing the `calc_trigrams()` function.\n",
    "\n",
    "calculate the trigram probabilities for the tags. \n",
    "\n",
    "The code outputs your results to a file `output/B2.txt`. \n",
    "\n",
    "Here are a few lines for you to check:\n",
    "\n",
    "```\n",
    "TRIGRAM * * ADJ -5.2055 \n",
    "TRIGRAM ADJ . X -9.9961 \n",
    "TRIGRAM NOUN DET NOUN -1.2645 \n",
    "TRIGRAM X . STOP -1.9292\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "235de118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIGRAM * * ADJ -5.205575150818023\n",
      "TRIGRAM ADJ . X -9.996120363033047\n",
      "TRIGRAM NOUN DET NOUN -1.264527106474862\n",
      "TRIGRAM X . STOP -1.9292269255866192\n"
     ]
    }
   ],
   "source": [
    "! grep -E \"TRIGRAM \\* \\* ADJ |TRIGRAM ADJ \\. X |TRIGRAM NOUN DET NOUN |TRIGRAM X \\. STOP \" output/B2.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9c3d9e8",
   "metadata": {},
   "source": [
    "After you checked your algorithm is giving the correct output, add to your README the log probabilities of the following trigrams:\n",
    "\n",
    "```\n",
    "TRIGRAM CONJ ADV ADP \n",
    "TRIGRAM DET NOUN NUM \n",
    "TRIGRAM NOUN PRT PRON\n",
    "```\n",
    "Note: you might wish to reuse a function you wrote in part A to make your life easier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3dbbdd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIGRAM CONJ ADV ADP -2.9755173148006566\n",
      "TRIGRAM DET NOUN NUM -8.970052616298892\n",
      "TRIGRAM NOUN PRT PRON -11.085472459181283\n"
     ]
    }
   ],
   "source": [
    "! grep -E \"TRIGRAM CONJ ADV ADP |TRIGRAM DET NOUN NUM |TRIGRAM NOUN PRT PRON \" output/B2.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d830f2e6",
   "metadata": {},
   "source": [
    "### 3. smoothing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "017c9045",
   "metadata": {},
   "source": [
    "The next step is to implement a smoothing method. \n",
    "\n",
    "To prepare to add smoothing, replace every word that occurs five or fewer times with the token specified in the constant `RARE_SYMBOL`. \n",
    "\n",
    "This corresponds to implementing the `calc_known()` and `replace_rare()` functions.\n",
    "\n",
    "First, you will create a list of words that occur more than five times in the training data. \n",
    "\n",
    "When tagging, any word that does not appear in this list should be replaced with the token in RARE_SYMBOL. \n",
    "\n",
    "You don’t need to write anything on README about this question. \n",
    "\n",
    "The code outputs the new version of the training data to “output/B3.txt”. \n",
    "\n",
    "Here are the first two lines of this file:\n",
    "\n",
    "```\n",
    "At that time highway engineers traveled rough and dirty roads to accomplish their duties.\n",
    "_RARE_ _RARE_ vehicles was a personal _RARE_ for such employees , and the matter of providing state transportation was felt perfectly _RARE_.\n",
    "```\n",
    "\n",
    "Hint: if you use a set instead of a list to store frequently occurring words, this operation will be faster. \n",
    "\n",
    "A set is python’s implementation of a hash table and has constant time membership checking, as opposed to a list, which has linear time checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91a3ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At that time highway engineers traveled rough and dirty roads to accomplish their duties .\n",
      "_RARE_ _RARE_ vehicles was a personal _RARE_ for such employees , and the matter of providing state transportation was felt perfectly _RARE_ .\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 output/B3.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "878ad721",
   "metadata": {},
   "source": [
    "### 4. calculate emission probability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8649d223",
   "metadata": {},
   "source": [
    "implementing the `calc_emission()` function. \n",
    "\n",
    "calculate the emission probabilities on the modified dataset, output in `B4.txt`.\n",
    "\n",
    "Here are a few lines (not contiguous) from this file for you to check your work:\n",
    "\n",
    "```\n",
    "America NOUN -10.9992 \n",
    "Columbia NOUN -13.5599 \n",
    "New ADJ -8.1884 \n",
    "York NOUN -10.7119 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b4c9c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "America NOUN -10.999259550049267\n",
      "Columbia NOUN -13.559974504523746\n",
      "New ADJ -8.18848005226213\n",
      "York NOUN -10.711977597968795\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"America NOUN |Columbia NOUN |New ADJ |York NOUN \" output/B4.txt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "082eb410",
   "metadata": {},
   "source": [
    "After  you  check  that  your  algorithm  is  giving  the  correct  output,  add  to  your  README  the  log probabilities of the following emissions (note words are case-sensitive):\n",
    "\n",
    "```\n",
    "* *\n",
    "Night NOUN\n",
    "Place VERB\n",
    "prime ADJ\n",
    "STOP STOP\n",
    "_RARE_ VERB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5548e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* * 0.0\n",
      "Night NOUN -13.881902599411106\n",
      "Place VERB -15.453881489107426\n",
      "STOP STOP 0.0\n",
      "_RARE_ VERB -3.177320850889013\n",
      "prime ADJ -10.69483271828692\n"
     ]
    }
   ],
   "source": [
    "! grep -E \"\\* \\* |Night NOUN |Place VERB |prime ADJ |STOP STOP |_RARE_ VERB \" output/B4.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c17a0a3d",
   "metadata": {},
   "source": [
    "### 5. Viterbi algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a57b6f3",
   "metadata": {},
   "source": [
    "Viterbi Algorithm\n",
    "\n",
    "1. Initialization $1 \\leq j \\leq N$:\n",
    "\n",
    "    $$\n",
    "    v_1(j) = \\pi_j b_j(o_1)\\\\[1em]\n",
    "    b_1(j)=0\n",
    "    $$\n",
    "\n",
    "2. Recursion $1 \\leq j \\leq N, 1 < t \\leq T$:\n",
    "   \n",
    "   $$\n",
    "   v_t(j) = \\max_{1 \\leq i \\leq N} v_{t-1}(i) a_{ij} b_j(o_t) \\\\[1em]\n",
    "   b_t(j) = \\arg\\max_{1 \\leq i \\leq N} v_{t-1}(i) a_{ij}b_j(o_t)\n",
    "   $$\n",
    "\n",
    "3. Termination:\n",
    "\n",
    "   Best score: $P^* = \\max_{1 \\leq i \\leq N} v_T(i)$\n",
    "\n",
    "   Start of backtrace: $q_T^* = \\arg\\max_{1 \\leq i \\leq N} v_T(i)$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "612dc069",
   "metadata": {},
   "source": [
    "implement the Viterbi algorithm for HMM taggers in `viterbi()` function.\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm that has many applications. \n",
    "\n",
    "For our purposes, the Viterbi algorithm is a comparatively efficient method for finding the highest scoring tag sequence for a given sentence. \n",
    "\n",
    "Please read about the specifics about this algorithm in section 8.4 of the book.\n",
    "\n",
    "your book uses the term “state observation likelihood” for “emission probability” and the term “transition probability” for “trigram probability.”\n",
    "\n",
    "Note that, while the output doesn’t have the “RARE” token, you still have to count unknown words as a “RARE” symbol to compute probabilities inside the Viterbi Algorithm.\n",
    "\n",
    "When exploring the space of possibilities for the tags of a given word, make sure to only consider tags with emission probability greater than zero for that given word. \n",
    "\n",
    "Also, when accessing the transition probabilities of tag trigrams, use -1000 (constant `LOG_PROB_OF_ZERO` in the code) to represent the log-probability of an unseen transition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12061f",
   "metadata": {},
   "source": [
    "Do what you can to make your algorithm as efficient as possible! \n",
    "\n",
    "While we won’t give you specifics, you should think about the order in which you check the words of the trigrams, which words and/or tags you can ignore, etc. \n",
    "\n",
    "For reference, our solution runs in 11-12 seconds. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca7bfe26",
   "metadata": {},
   "source": [
    "Using your emission and trigram probabilities, calculate the most likely tag sequence for each sentence in `Brown_dev.txt`. \n",
    "\n",
    "Your tagged sentences will be outputted to `B5.txt`. \n",
    "\n",
    "The first two tagged sentences should look like this:\n",
    "```\n",
    "He/PRON had/VERB obtained/VERB and/CONJ provisioned/VERB a/DET veteran/ADJ ship/NOUN called/VERB the/DET Discovery/NOUN and/CONJ had/VERB recruited/VERB a/DET crew/NOUN of/ADP twenty-one/NOUN ,/. the/DET largest/ADJ he/PRON had/VERB ever/ADV commanded/VERB ./.\n",
    "The/DET purpose/NOUN of/ADP this/DET fourth/ADJ voyage/NOUN was/VERB clear/ADJ ./. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf110dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He/PRON had/VERB obtained/VERB and/CONJ provisioned/VERB a/DET veteran/ADJ ship/NOUN called/VERB the/DET Discovery/NOUN and/CONJ had/VERB recruited/VERB a/DET crew/NOUN of/ADP twenty-one/NOUN ,/. the/DET largest/ADJ he/PRON had/VERB ever/ADV commanded/VERB ./. \n",
      "The/DET purpose/NOUN of/ADP this/DET fourth/ADJ voyage/NOUN was/VERB clear/ADJ ./. \n"
     ]
    }
   ],
   "source": [
    "!head -n 2 output/B5.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a9535c0",
   "metadata": {},
   "source": [
    "Once you run your implementation, use the part of speech evaluation script `accuracy.py` to compare the output file with `Brown_tagged_dev.txt`. \n",
    "\n",
    "Include the accuracy of your tagger in the README file. \n",
    "\n",
    "To use the script, run the following command:\n",
    "\n",
    "`python3 accuracy.py output/B5.txt data/Brown_tagged_dev.txt`\n",
    "\n",
    "This is the result we got with our implementation of the Viterbi algorithm:\n",
    "\n",
    "```\n",
    "Percent correct tags: 93.325\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5b0e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct tags: 93.32499462544219\n"
     ]
    }
   ],
   "source": [
    "!python3 accuracy.py output/B5.txt data/Brown_tagged_dev.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "681d1990",
   "metadata": {},
   "source": [
    "### 6. NLTK tagger"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ca5b389",
   "metadata": {},
   "source": [
    "Implement `nltk_tagger()` function. \n",
    "\n",
    "create an instance of NLTK’s trigram tagger set to back off to NLTK’s bigram tagger. Let the bigram tagger itself back off to NLTK’s default tagger using the tag “NOUN”. \n",
    "\n",
    "The code outputs your results to a file `B6.txt`, and this is how the first two lines of this file should look:\n",
    "\n",
    "```\n",
    "He/NOUN had/VERB obtained/VERB and/CONJ provisioned/NOUN a/DET veteran/NOUN ship/NOUN called/VERB the/DET Discovery/NOUN and/CONJ had/VERB recruited/NOUN a/DET crew/NOUN of/ADP twenty-one/NUM ,/. the/DET largest/ADJ he/PRON had/VERB ever/ADV commanded/VERB ./.\n",
    "The/NOUN purpose/NOUN of/ADP this/DET fourth/ADJ voyage/NOUN was/VERB clear/ADJ ./.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7001d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He/NOUN had/VERB obtained/VERB and/CONJ provisioned/NOUN a/DET veteran/NOUN ship/NOUN called/VERB the/DET Discovery/NOUN and/CONJ had/VERB recruited/NOUN a/DET crew/NOUN of/ADP twenty-one/NUM ,/. the/DET largest/ADJ he/PRON had/VERB ever/ADV commanded/VERB ./.\n",
      "The/NOUN purpose/NOUN of/ADP this/DET fourth/ADJ voyage/NOUN was/VERB clear/ADJ ./.\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 output/B6.txt "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0254a5c7",
   "metadata": {},
   "source": [
    "Use `accuracy.py` to evaluate the NLTK’s tagger accuracy and put the result in your README. This is the accuracy that we got with our implementation:\n",
    "\n",
    "`Percent correct tags: 88.0399`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a2673872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct tags: 88.03994762249106\n"
     ]
    }
   ],
   "source": [
    "!python3 accuracy.py output/B6.txt data/Brown_tagged_dev.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4daabb6f",
   "metadata": {},
   "source": [
    "## Part C – Part-of-Speech Tagging for Spanish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb03cd87",
   "metadata": {},
   "source": [
    "In this part of the assignment, you will be using `solutionsC.py`. \n",
    "\n",
    "To understand the general workflow of the script, read the `main()· function, **_but do not modify it_**. \n",
    "\n",
    "Note that `solutionsC.py` uses functions from `HMMPOS.py`. It is recommended that you complete Part B before you begin Part C.\n",
    "\n",
    "Part C uses a corpus other than the Brown corpus. \n",
    "\n",
    "The training data comes from Wikicorpus, a trilingual corpus using passages from Wikipedia. We will be using the Spanish portion to train and evaluate a part-of-speech tagger.\n",
    "\n",
    "As you may suspect, different languages may use different tagsets. \n",
    "\n",
    "The data from Wikicorpus uses the Parole tagset, which is similar to the Brown corpus’s tagset, but also has tags that are not used in English (such as DE for determiner, exclamatory; Fia for punctuation, ¿; VS* for semi-auxiliary verbs). \n",
    "\n",
    "For instance, the Brown corpus distinguishes between adverbs, comparative adverbs, and superlative adverbs (I run fast, but Miles runs faster, and Drago runs fastest.) \n",
    "\n",
    "The Parole tagset does not differentiate between such adverbs, but instead distinguishes between regular adverbs and negative adverbs (nunca - never). \n",
    "\n",
    "This particular tag may be helpful in Spanish since some sentences use double negatives for emphasis (“No conozco a nadie” word-for-word translates to “I do not know nobody” and actually means “I don’t know anybody”.)\n",
    "\n",
    "You can use your `solutionsC.py` to train and evaluate a part-of-speech tagger in Spanish, with normal trigrams. \n",
    "\n",
    "To do this, run the following:\n",
    "\n",
    "`python 3 solutionsC.py`\n",
    "\n",
    "This program will likely take a lot longer to run than `HMMPOS.py`.\n",
    "\n",
    "Run the following command to find the accuracy:\n",
    "\n",
    "`python 3 accuracy.py output/C5.txt data/wikicorpus_tagged_dev.txt`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3d8f9c1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In your README, record the accuracy of your tagger for Spanish. \n",
    "\n",
    "Our implementation of the Spanish tagger achieved the following performance (to three decimal places):\n",
    "\n",
    "`Percent correct tags: 84.472`\n",
    "\n",
    "In your README, answer the following questions:\n",
    "\n",
    "- The Spanish dataset takes longer to evaluate. Why do you think this is the case?\n",
    "\n",
    "- What are aspects or features of a language that may improve tagging accuracy that are\n",
    "not captured by the tagged training sets?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efd5737e",
   "metadata": {},
   "source": [
    "## Part D – Comparison with Neural Methods\n",
    "\n",
    "In this part of the assignment, you will compare your HMM based tagger with an existing neural network-based model for POS tagging, by Jonathan K. Kummerfeld from the University of Michigan. \n",
    "\n",
    "A version of the code that uses a library called PyTorch is available in `neuralPOStagger.py`. You do not have to implement or modify it\n",
    "\n",
    "Their implementation uses the GloVe word embeddings, along with a neural network called a bidirectional LSTM \n",
    "\n",
    "For this assignment, we have already trained the model on the Brown corpus in English. \n",
    "\n",
    "You simply have to evaluate this neural model.\n",
    "\n",
    "You will evaluate `tagger.pt.model`, the trained model by running:\n",
    "\n",
    "`python 3 solutionsD.py`\n",
    "\n",
    "You should get something like this:\n",
    "\n",
    "`Test accuracy: 96.691`\n",
    "\n",
    "In your README, answer the following question:\n",
    "\n",
    "- How does this result compare with your HMM based tagger?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1014380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9655823672419682\n",
      "0 0.9672237083345857\n",
      "Test Accuracy: 96.691\n"
     ]
    }
   ],
   "source": [
    "!python3 solutionsD.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2f65d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
