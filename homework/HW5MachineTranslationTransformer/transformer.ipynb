{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcWQsDm3u31c"
      },
      "source": [
        "# Homework 5 Transformer\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRDq1l8K12NO"
      },
      "source": [
        "Transformer models took the NLP community by storm by achieving state-of-the-art results in machine translation in the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf). A very good tutorial on this architecture can be found [here](http://jalammar.github.io/illustrated-transformer/). As you can tell from the name, this model is based on attention! It replaces standard recurrent neural networks used for translation with a self-attention-based network. \n",
        "\n",
        "The model is an encoder-decoder model, as shown below:\n",
        "\n",
        "\n",
        "<center><img src=\"http://nlp.seas.harvard.edu/images/the-annotated-transformer_14_0.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "The encoder and decoder each consists of modules which are repeated N times (N=6 in the original paper). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg_kKg-E7GJf"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QIwuVRWHdyf"
      },
      "source": [
        "This is basically the same as before, but we need batch first in `collate_fn`, simply transpose `src_batch` and `trg_batch` before return them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-S0rAnQWWEK",
        "outputId": "c0744b6e-e571-4027-b8db-1f524e9c9321",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UTF-8\n",
            "2023-04-22 08:09:47.294498: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-22 08:09:48.240846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-22 08:09:49.529907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-22 08:09:49.530362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-22 08:09:49.530547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-04-22 08:10:03.863614: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-22 08:10:04.828954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-22 08:10:06.085852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-22 08:10:06.086314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-22 08:10:06.086498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return 'UTF-8'\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "print(locale.getpreferredencoding())\n",
        "\n",
        "! pip install -U spacy -q\n",
        "! python -m spacy download en_core_web_sm -q\n",
        "! python -m spacy download de_core_news_sm -q\n",
        "! pip install torch==1.13.1 torchtext==0.14.1 torchdata==0.5.1 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrCzmt5xWWEM",
        "outputId": "ce9f97c8-365e-4a0e-ed76-3ae1120871aa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pytorch version is:  1.13.1+cu117\n",
            "You are using:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Pytorch version is: \", torch.__version__)\n",
        "print(\"You are using: \", device)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F \n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import spacy\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "from typing import List, Iterable, Tuple \n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.ticker as ticker\n",
        "# We'll set the random seeds for deterministic results.\n",
        "SEED = 1\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.enabled = False \n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR_n1_WEWWEN",
        "outputId": "b61c6dd4-49e9-4f9f-c5a7-ce9c7d8b9cdd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 8014\n",
            "Unique tokens in target (en) vocabulary: 6191\n"
          ]
        }
      ],
      "source": [
        "# multi30k original link broke Issue: https://github.com/pytorch/text/issues/1756\n",
        "# Update URLs to point to data stored by user\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "multi30k.URL[\"test\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
        "\n",
        "# Update hash since there is a discrepancy between user hosted test split and that of the test split in the original dataset \n",
        "multi30k.MD5[\"test\"] = \"6d1ca1dba99e2c5dd54cae1226ff11c2551e6ce63527ebb072a1f70f72a5cd36\"\n",
        "\n",
        "\n",
        "# Build vocab\n",
        "SRC = 'de'\n",
        "TRG = 'en'\n",
        "LANG = {SRC: 0, TRG: 1}\n",
        "models = ['de_core_news_sm', 'en_core_web_sm']\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid', 'test'), \n",
        "                                             language_pair=(SRC, TRG))\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[LANG[language]])\n",
        "\n",
        "for L, model in zip(LANG, models):\n",
        "  token_transform[L] = get_tokenizer('spacy', language=model)\n",
        "  vocab_transform[L] = build_vocab_from_iterator(yield_tokens(train_iter, L),\n",
        "                                                    min_freq=2,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "  vocab_transform[L].set_default_index(UNK_IDX)\n",
        "\n",
        "print(f\"Unique tokens in source (de) vocabulary: {len(vocab_transform[SRC])}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(vocab_transform[TRG])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_0rvGaYWWEO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def tensor_transform(token_ids: List[int]):\n",
        "  \"\"\"\n",
        "  function to add BOS/EOS and create tensor for input sequence indices\n",
        "  \"\"\"\n",
        "  return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                    torch.tensor(token_ids),\n",
        "                    torch.tensor([EOS_IDX])))\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[Tuple[str, str]]):\n",
        "    \"\"\"\n",
        "    Tokenization, Numericalization, and Add BOS/EOS to create tensor for source/target language\n",
        "\n",
        "    sort the sequences based on their lengths first, we can ensure that shorter sequences are processed together, \n",
        "    and then pad them to match the length of the longest sequence in the batch,\n",
        "    reducing the number of unnecessary padding tokens that the model has to process, \n",
        "    lead to a more efficient computation and potentially better results, \n",
        "\n",
        "    batch: a list of tuples, where each tuple contains a pair of source and target samples. \n",
        "            The source and target samples are usually strings. e.g.\n",
        "            [\n",
        "              (\"This is a source sentence.\", \"This is a target sentence.\"),\n",
        "              (\"Another source sentence.\", \"Another target sentence.\"),\n",
        "              ...\n",
        "            ]\n",
        "\n",
        "    \"\"\"\n",
        "    src_batch, trg_batch = [], []\n",
        "    \n",
        "    # Sort batch based on the source length before tokenization and numericalization\n",
        "    sorted_batch = sorted(batch, key=lambda x: len(x[0].rstrip(\"\\n\").split()), reverse=True)\n",
        "    \n",
        "    # tokenization and numericalization\n",
        "    for src_sample, trg_sample in sorted_batch:\n",
        "        src_batch.append(tensor_transform(vocab_transform[SRC](token_transform[SRC](src_sample.rstrip(\"\\n\")))).long())\n",
        "        trg_batch.append(tensor_transform(vocab_transform[TRG](token_transform[TRG](trg_sample.rstrip(\"\\n\")))).long())\n",
        "    \n",
        "    # Pad sequences \n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
        "\n",
        "    src_len = (src_batch != PAD_IDX).sum(dim=0)\n",
        "    \n",
        "    return src_batch.T, src_len, trg_batch.T "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efH6Js_zWWEP",
        "outputId": "961bc9e5-c9e2-478b-c8d3-5054e3da2028",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 29001\n",
            "Number of validation examples: 1015\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_iter, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "valid_loader = DataLoader(valid_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "print(f\"Number of training examples: {len(list(train_iter))}\")\n",
        "print(f\"Number of validation examples: {len(list(valid_iter))}\")\n",
        "# print(f\"Number of testing examples: {len(list(test_iter))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRa_E7vTGtt4",
        "outputId": "c24bfa5d-b880-451d-98b0-3b9cb7ad7882",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([28, 128]) torch.Size([27, 128])\n"
          ]
        }
      ],
      "source": [
        "for src, trg in train_loader:\n",
        "  print(src.shape, trg.shape)\n",
        "  break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcsAial3WWEQ"
      },
      "source": [
        "## self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQrZuTGbWWEQ"
      },
      "source": [
        "The encoder consists of a self-attention layer (multi-head attention in the first figure) which is then followed by a feed-forward network. \n",
        "<center><img src=\"http://jalammar.github.io/images/t/encoder_with_tensors_2.png\" alt=\"mlp\" align=\"middle\"></center>\n",
        "\n",
        "\n",
        "The self-attention matrix calculation : \n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}} )V$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$Q$ represents the query matrix, you want to use every word in the sentence as a query for that keyword and check its relevance for representing that key.\n",
        "\n",
        "$K$ represents the key matrix, refers to each word in the sentence\n",
        "\n",
        "$V$ represents the value matrix\n",
        "\n",
        "$d_k$ is the dimensionality of the key and query vectors\n",
        "\n",
        "dot product of $Q$ and $K$ is divided by a constant and then softmax'ed before being multiplied by a matrix $V$, attention vectors can be thought of as the *weighted* from the previous notebooks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBXFI1vGWWER"
      },
      "source": [
        "**TODO** explain what the role of the denominator in the self-attention equation is (check the original paper). \n",
        "\n",
        "**Answer:** \n",
        "\n",
        "1. mitigating the vanishing or exploding gradient problem during training\n",
        "\n",
        "2. prevent 'hard' attention, maintain the numerical stability.\n",
        "\n",
        "  When the dimensionality of the key vectors is larger, the dot products between the query and key matrices can become very large, causing the softmax function to squash the input values into the extreme ends of the range (very small or very large values), leading to a \"hard\" attention mechanism, model only pay attention to single word. \n",
        "    \n",
        "  By dividing the dot product by $\\sqrt{d_k}$, the self-attention mechanism becomes more \"soft\", allowing the model to pay attention to multiple words in the input sequence.\n",
        "\n",
        "\n",
        "\n",
        "**TODO** explain what the motivation is behind using multiple heads attention (8 in the original paper). \n",
        "\n",
        "**Answer:** 1. Diversity of features 2. larger receptive field 3. increased capacity 4. Parallel computation and faster training.\n",
        "\n",
        "\n",
        "**TODO**  explain what the benefits of using residual connections (an extra input arrow pointing to the *Add & Norm* module) are here and in neural networks in general. \n",
        "\n",
        "**Answer:** Residual connections allow gradients to flow through the network directly, without passing through non-linear activation functions. This prevents the gradients from exploding or vanishing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ViyHOkd747Q"
      },
      "source": [
        "**TODO**\n",
        "\n",
        "1. Modify the size of the Q, K and V matrices to be of size (batch size, n heads, sent len, hid dim // n heads). \n",
        "\n",
        "  use `view()` and `permute()` functions.\n",
        "  \n",
        "  This line will be the same for each of the three matrices. \n",
        "\n",
        "2. Matrix multiply Q and K and scale the output following the equation above. \n",
        "\n",
        "3. Matrix multiple attention and V\n",
        "\n",
        "4. Change the shape of x to match the desired output shape.\n",
        "\n",
        "  after using operations like transpose, permute, or slicing, use `contiguous()` method to rearranges its elements in memory, creating a new contiguous tensor with the same data, ensure that a tensor's data is stored in a contiguous block of memory. This makes certain tensor operations, such as slicing and reshaping, more efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIo_9JySu312",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        assert hid_dim % n_heads == 0\n",
        "\n",
        "        # Weight matrices for query, key, and value\n",
        "        self.w_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.w_v = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        # Final fully connected layer to project the concatenated attention matrix\n",
        "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        query = key = value: [batch size, sent len, hid dim] \n",
        "        \"\"\"\n",
        "        batch_size = query.shape[0]\n",
        "        # Compute Q, K, V using linear layers. \n",
        "        # Q, K, V = [batch size, sent len, hid dim]  \n",
        "        Q = self.w_q(query)\n",
        "        K = self.w_k(key)\n",
        "        V = self.w_v(value)\n",
        "        \n",
        "        # TODO 1: Reshape and permute Q, K, V.\n",
        "        # Q, K, V = [batch size, sent len, n_heads, hid dim // n heads] -> (batch size, n_heads, sent len, hid dim // n heads)\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.hid_dim // self.n_heads).permute(0, 2, 1, 3)\n",
        "        # Q, K, V = [batch size, n heads, sent len, hid dim // n heads]\n",
        "        \n",
        "        # TODO 2: compute attention scores (energy) by dot product of Q and K and scaling\n",
        "        # energy = [batch size, n heads, sent len, sent len]\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        \n",
        "        # apply masked self-attention if provided\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        # Compute the softmax-normalized attention scores. \n",
        "        # attention = [batch size, n heads, sent len, sent len]\n",
        "        attention = self.do(F.softmax(energy, dim=-1))\n",
        "\n",
        "        # TODO 3: compute attention matrix for each head by multiplying the attention scores with V\n",
        "        # x = [batch size, n heads, sent len, hid dim // n heads] -> [batch size, sent len, n heads, hid dim // n heads]\n",
        "        x = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\n",
        "        \n",
        "        # TODO 4 concatenate the attention matrices for all heads. \n",
        "        # x = [batch size,sent len, hid dim]\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        \n",
        "        # Project the concatenated attention matrix to the output shape. \n",
        "        # x = [batch size, sent len, hid dim]\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skxWEXkCWWES"
      },
      "source": [
        "## Positionwise Feedforward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFMYjYrdu314",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedforward(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.pf_dim = pf_dim\n",
        "        \n",
        "        # Two convolution layers (1x1)\n",
        "        self.fc_1 = nn.Conv1d(hid_dim, pf_dim, 1)\n",
        "        self.fc_2 = nn.Conv1d(pf_dim, hid_dim, 1)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x = [batch size, sent len, hid dim] -> [batch size, hid dim, sent len]\n",
        "        x = self.do(F.relu(self.fc_1(x.permute(0, 2, 1)))) # x = [batch size, ff dim, sent len]\n",
        "        \n",
        "        x = self.fc_2(x).permute(0, 2, 1) # x = [batch size, hid dim, sent len] -> [batch size, sent len, hid dim]\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM3xwRzoWWES"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVTlHaIsWWES"
      },
      "source": [
        "**TODO** \n",
        "\n",
        "1. Apply embeddings over the source, scale these embeddings, add positional embeddings and then at the end apply dropout to everything. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aWp6zMlWWES"
      },
      "source": [
        "### positional embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrtYzOLnWWET"
      },
      "source": [
        "Words are transformed into embeddings before being input into these modules. \n",
        "\n",
        "However, these embeddings are added with positional embeddings, which give the model a notion of relative input position (remember that there is no recurrence model which keeps track sequentially.). \n",
        "\n",
        "These positional embeddings consist of the sine and cosine functions of different frequencies: $$PE(pos, 2i) = sin(pos/10000^\\frac{2i}{d_{model}})$$   $$PE(pos, 2i+1) = cos(pos/10000^\\frac{2i}{d_{model}}).$$ \n",
        "\n",
        "Thus, each dimension of the positional encoding corresponds to a sinusoid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUv2lgnH2qnc"
      },
      "source": [
        "This implementation uses **learned** positional embeddings rather than the fixed sinusoidal positional embeddings proposed in the original Transformer paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i42xZMvCWWET",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device)\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        src = [batch size, src sent len, hid dim]\n",
        "        src_mask = [batch size, src sent len]\n",
        "        \"\"\"\n",
        "        # Apply self-attention and layer normalization\n",
        "        src = self.ln(src + self.do(self.sa(src, src, src, src_mask)))\n",
        "        # Apply positionwise feedforward and layer normalization\n",
        "        src = self.ln(src + self.do(self.pf(src)))\n",
        "        \n",
        "        return src"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc2f9p3FWWET",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, encoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.encoder_layer = encoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.device = device\n",
        "\n",
        "        # Token embedding and position embedding\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        # Create `n_layers` of encoder layers\n",
        "        self.layers = nn.ModuleList([encoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device) \n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, src, src_mask):\n",
        "        \"\"\"\n",
        "        params:\n",
        "        src = [batch size, src sent len]\n",
        "        src_mask = [batch size, src sent len]\n",
        "\n",
        "        return: src = [batch size, src sent len, hid dim]\n",
        "        \"\"\"\n",
        "        # initialize position index\n",
        "        pos = torch.arange(0, src.shape[1]).unsqueeze(0).repeat(src.shape[0], 1).to(self.device)\n",
        "        \n",
        "        # TODO create position-aware input (token embedding + position embedding). src = [batch size, src sent len, hid dim]\n",
        "        # scaling token embeddings for numerical stability\n",
        "        src = self.do((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        # Pass the input through each encoder layer\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        \n",
        "        # The final output of the encoder src = [batch size, src sent len, hid dim]\n",
        "        return src"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH6geRGWWET"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpJ-zqyq7-we"
      },
      "source": [
        "**TODO** \n",
        "\n",
        "1. (same as above) -- Apply embeddings over the source, scale these embeddings, add positional embeddings and then at the end apply dropout to everything. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue4mq_MGWWET",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.ln = nn.LayerNorm(hid_dim)                             # layer norm\n",
        "        self.sa = self_attention(hid_dim, n_heads, dropout, device) # self-attention\n",
        "        self.ea = self_attention(hid_dim, n_heads, dropout, device) # encoder-decoder attention\n",
        "        self.pf = positionwise_feedforward(hid_dim, pf_dim, dropout)  # postiionwise feedforward layer\n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \"\"\"\n",
        "        params:\n",
        "        trg: target sequence  [batch size, trg sent len, hid dim]\n",
        "        src: source sequence [batch size, src sent len, hid dim]\n",
        "        trg_mask: target mask [batch size, trg sent len]\n",
        "        src_mask: source mask [batch size, src sent len]\n",
        "\n",
        "        return: trg = [batch size, trg sent len, hid dim]\n",
        "        \"\"\"\n",
        "        # The output of each step is added to the input and then passed \n",
        "        \n",
        "        # first apply self-attention and layer normalization \n",
        "        trg = self.ln(trg + self.do(self.sa(trg, trg, trg, trg_mask)))\n",
        "\n",
        "        # next Apply encoder-decoder attention and layer normalization \n",
        "        trg = self.ln(trg + self.do(self.ea(trg, src, src, src_mask)))\n",
        "\n",
        "        # finally Apply positionwise feedforward layers and layer normalization \n",
        "        trg = self.ln(trg + self.do(self.pf(trg)))\n",
        "        \n",
        "        # trg = [batch size, trg sent len, hid dim]\n",
        "        return trg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ZzgTaku316",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, pf_dim, decoder_layer, self_attention, positionwise_feedforward, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.pf_dim = pf_dim\n",
        "        self.decoder_layer = decoder_layer\n",
        "        self.self_attention = self_attention\n",
        "        self.positionwise_feedforward = positionwise_feedforward\n",
        "        self.dropout = dropout\n",
        "        self.device = device\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(1000, hid_dim)\n",
        "        \n",
        "        self.layers = nn.ModuleList([decoder_layer(hid_dim, n_heads, pf_dim, self_attention, positionwise_feedforward, dropout, device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.do = nn.Dropout(dropout)\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, src, trg_mask, src_mask):\n",
        "        \"\"\"\n",
        "        params\n",
        "        trg = [batch_size, trg sent len]\n",
        "        src = [batch_size, src sent len]\n",
        "        trg_mask = [batch size, trg sent len]\n",
        "        src_mask = [batch size, src sent len]\n",
        "\n",
        "        return\n",
        "        logits = [batch size, trg sent len, output dim]\n",
        "        \"\"\"\n",
        "        # initialize position index \n",
        "        pos = torch.arange(0, trg.shape[1]).unsqueeze(0).repeat(trg.shape[0], 1).to(self.device)\n",
        "        \n",
        "        # TODO input is token embedding plus positional embedding\n",
        "        trg = self.do((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        # trg = [batch size, trg sent len, hid dim]\n",
        "        \n",
        "        # Pass the input through each decoder layer\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, src, trg_mask, src_mask)\n",
        "        \n",
        "        # Apply the output layer to produce logits\n",
        "        # logits [batch size, trg sent len, output dim]\n",
        "        logits = self.fc(trg)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CY9COjqWWEU"
      },
      "source": [
        "## Seq2seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz_hvERG8HeZ"
      },
      "source": [
        "The Seq2seq model itself doesn't change much. Yay modular code. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbYQVjgru319",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_masks(self, src, trg):\n",
        "      \"\"\"\n",
        "      creates masks for the source and target sequences.\n",
        "      The masks are used during self-attention in both the encoder and the decoder\n",
        "      to prevent the model from attending to padding tokens or future tokens in the target sequence.\n",
        "\n",
        "      Args:\n",
        "          src (torch.Tensor): Source sequence tensor of shape [batch size, src sent len]\n",
        "          trg (torch.Tensor): Target sequence tensor of shape [batch size, trg sent len]\n",
        "\n",
        "      Returns:\n",
        "          src_mask (torch.Tensor): Source mask tensor of shape [batch size, 1, 1, src sent len]\n",
        "          trg_mask (torch.Tensor): Target mask tensor of shape [batch size, 1, trg sent len, trg sent len]\n",
        "      \"\"\"\n",
        "      # Create source mask by checking which tokens are not padding tokens.\n",
        "      # This mask is used in the encoder self-attention to prevent attending to padding tokens.\n",
        "      src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "      # Create target padding mask by checking which tokens are not padding tokens.\n",
        "      trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "\n",
        "      # Create target subsequent mask to prevent attending to future tokens in the target sequence.\n",
        "      # This is used in the decoder self-attention mechanism.\n",
        "      # `torch.tril` returns the lower triangular matrix of a matrix \n",
        "      trg_len = trg.shape[1]\n",
        "      trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), dtype=torch.uint8, device=self.device))\n",
        "\n",
        "      # Combine target padding mask and target subsequent mask.\n",
        "      trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "      return src_mask, trg_mask\n",
        "\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        \"\"\"\n",
        "        takes source and target sequences as input and returns logits after passing through the encoder and decoder.\n",
        "\n",
        "        Args:\n",
        "            src (torch.Tensor): Source sequence tensor of shape [batch size, src sent len].\n",
        "            trg (torch.Tensor): Target sequence tensor of shape [batch size, trg sent len].\n",
        "\n",
        "        Returns:\n",
        "            out (torch.Tensor): Output logits tensor of shape [batch size, trg sent len, output dim].\n",
        "        \"\"\"\n",
        "\n",
        "        # Create masks for the source and target sequences.\n",
        "        src_mask, trg_mask = self.make_masks(src, trg)\n",
        "\n",
        "        # compute hidden states of the encoder by Pass the source sequence and source mask through the encoder.\n",
        "        # These hidden states capture the contextual information of the input source sequence and serve as the input for the decoder\n",
        "        # enc_src = [batch size, src sent len, hid dim] \n",
        "        enc_src = self.encoder(src, src_mask) \n",
        "           \n",
        "        # Pass the target sequence, encoded source sequence, target mask, and source mask through the decoder.\n",
        "        out = self.decoder(trg, enc_src, trg_mask, src_mask) \n",
        "        \n",
        "        # return logits. out = [batch size, trg sent len, output dim]\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx2ZeIgqWWEU"
      },
      "source": [
        "## Noam Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o728galWWEU"
      },
      "source": [
        "A new optimzer is introduced in this paper. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt-OOtV8WWEU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class NoamOpt:\n",
        "    \"\"\"\n",
        "    Noam Optimizer. a learning rate scheduler used in conjunction with the Adam optimizer, \n",
        "    specifically designed for training the Transformer model. \n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDnbMz8sWWEV"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMLd_NlYu31_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "input_dim = len(vocab_transform[SRC])\n",
        "output_dim = len(vocab_transform[TRG])\n",
        "hid_dim = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "pf_dim = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "enc = Encoder(input_dim, hid_dim, n_layers, n_heads, pf_dim, EncoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)\n",
        "dec = Decoder(output_dim, hid_dim, n_layers, n_heads, pf_dim, DecoderLayer, SelfAttention, PositionwiseFeedforward, dropout, device)\n",
        "model = Seq2Seq(enc, dec, PAD_IDX, device).to(device)\n",
        "optimizer = NoamOpt(hid_dim, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1nExktMu32H",
        "outputId": "061ed011-c10b-4abf-8bfe-0df25ecef90d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 55,593,007 trainable parameters\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(8014, 512)\n",
              "    (pos_embedding): Embedding(1000, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): EncoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (do): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(6191, 512)\n",
              "    (pos_embedding): Embedding(1000, 512)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ea): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ea): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ea): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): DecoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ea): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): DecoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ea): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): DecoderLayer(\n",
              "        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (sa): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ea): SelfAttention(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (fc): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (pf): PositionwiseFeedforward(\n",
              "          (fc_1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
              "          (fc_2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
              "          (do): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (do): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc): Linear(in_features=512, out_features=6191, bias=True)\n",
              "    (do): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "        \n",
        "model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkCfsy0eu32R",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for src, trg in loader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        \n",
        "        optimizer.optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "                \n",
        "        # output = [batch size, trg sent len - 1, output dim]\n",
        "        # trg = [batch size, trg sent len]\n",
        "        output = output[:,1:].contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        # output = [batch size * trg sent len - 1, output dim]\n",
        "        # trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(list(loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfZR7IWzu32T",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg)\n",
        "            # output = [batch size, trg sent len - 1, output dim]\n",
        "            # trg = [batch size, trg sent len]\n",
        "            output = output[:,1:].contiguous().view(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            # output = [batch size * trg sent len - 1, output dim]\n",
        "            # trg = [batch size * trg sent len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(list(loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47z_qLBsu32U",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8VoGPJj8VYh"
      },
      "source": [
        "**TODO** Train for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df_eR_B2u32W",
        "outputId": "559dbaf2-9d28-4c8d-a38d-87fd8f112402",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Epoch: 001 | Time: 2m 45s| Train Loss: 3.382 | Train PPL:  29.425 | Val. Loss: 0.759 | Val. PPL:   2.137 |\n",
            "| Epoch: 002 | Time: 2m 44s| Train Loss: 0.561 | Train PPL:   1.752 | Val. Loss: 0.184 | Val. PPL:   1.202 |\n",
            "| Epoch: 003 | Time: 2m 46s| Train Loss: 0.182 | Train PPL:   1.199 | Val. Loss: 0.046 | Val. PPL:   1.047 |\n",
            "| Epoch: 004 | Time: 2m 44s| Train Loss: 0.057 | Train PPL:   1.058 | Val. Loss: 0.007 | Val. PPL:   1.007 |\n",
            "| Epoch: 005 | Time: 2m 43s| Train Loss: 0.011 | Train PPL:   1.011 | Val. Loss: 0.000 | Val. PPL:   1.000 |\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "CLIP = 1\n",
        "SAVE_DIR = 'models'\n",
        "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'transformer-seq2seq.pt')\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "if not os.path.isdir(f'{SAVE_DIR}'):\n",
        "    os.makedirs(f'{SAVE_DIR}')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.perf_counter()\n",
        "    \n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader, criterion)\n",
        "    \n",
        "    end_time = time.perf_counter()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    \n",
        "    print(f'| Epoch: {epoch+1:03} | Time: {epoch_mins}m {epoch_secs}s| Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "M4UNQFNru32Z",
        "outputId": "562d7e7e-0f11-4de8-adb8-dd613a430d40",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Test Loss: 2.392 | Test PPL:  10.939 |\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "test_loss = evaluate(model, test_loader, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
